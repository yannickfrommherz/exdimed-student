{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9b2abe-9dbc-4b0c-bd97-088470e5eca4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Web Scraping Teil 1 (Lösungen)\n",
    "\n",
    "☝️ Beachte: Es gibt beim Programmieren fast immer verschiedene Lösungswege. Deine Lösung mag anders aussehen, aber dennoch zum gewünschten Resultat führen. Das richtige Resultat ist das Wichtigste. \n",
    "\n",
    "⚠️ Führ folgenden Code aus, bevor Du einzelne Lösungen ausführst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001fff4-aee5-4103-a632-55331948d234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63429c5-bf09-47ba-84ad-f9230de9e187",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "✏️ **Lösung 1:** Besuch eine beliebige Webseite in einem Browser Deiner Wahl. Die Webseite sollte mehrere Links zu anderen Seiten beinhalten, was auf die meisten Webseiten zutrifft. Lass Dir wie oben beschrieben den Quelltext der Seite anzeigen. Geh nun wie folgt vor und verwend dabei die angegebenen Tastenkombinationen.\n",
    "\n",
    "1. Markier den gesamten Quelltext (`Strg` + `a` (Windows und Linux) bzw. `⌘` + `a` (macOS)), kopier ihn (`Strg`/`⌘` + `c`) und füg ihn in ein leeres Dokument bei Sublime Text ein (`Strg`/`⌘` + `v`). \n",
    "2. Formulier einen regulären Ausdruck, der sämtliche Links in Deinem Quelltext matcht. Manche Webseiten enthalten komplette Links in ihrem Quelltext (z.&nbsp;B. \"ht<span>tps://www.</span>domain<span>.de/</span>subdomain/page\"), andere wiederum bloß abgekürzte Links (\"subdomain/page\"), die, wenn man im Browser draufklickt, um den gemeinsamen \"Stammlink\" ergänzt werden. Dein regulärer Ausdruck soll sich nach dem Linkformat im Quelltext Deiner Webseite richten und möglichst viele Links matchen.\n",
    "3. Aktivier die Suche bei Sublime Text über `Strg`/`⌘` + `f`). Gib den regulären Ausdruck ein und such bzw. markier alle Matches über die Tastenkombination `Alt`/`option` + `Enter` (dies entspricht der Schaltfläche \"Find All\"). Ggf. musst Du reguläre Ausdrücke für die Suche aktivieren (links neben dem Suchfeld über `.*`).\n",
    "5. Kopier alle Links. Öffne ein weiteres Dokument bei Sublime Text und füg die Links dort ein. Führ auch diese Schritte nach Möglichkeit mithilfe von Tastenkombinationen aus.\n",
    "\n",
    "*Keine Lösung, da jede Webseite unterschiedlich aufgebaut ist. Der reguläre Ausdruck im Tipp bietet aber einen Anhaltspunkt, den Du vermutlich im Hinblick auf Deine Webseite anpassen musst.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d015d2a-af2f-4e54-9578-73f622777e79",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "<!-- Da es sich um in Markdown eingebetteten HTML-Code handelt, ist die grundlegende HTML-Struktur mit Dokumenttypdeklaration, html- und body-Element nicht nötig. Markdown-Code kommt ohne diese Struktur aus. Dieser Text hier ist übrigens ein Kommentar, der nicht gerendert wird.-->\n",
    "\n",
    "<table style=\"font-size: 16px; font-family: sans-serif; letter-spacing: 0.2px;\">\n",
    "      <tr>\n",
    "        <td>\n",
    "            <p>✏️ <b>Übung 2:</b> Um die Struktur von HTML-Code weiter zu verinnerlichen, wollen wir trotzdem eine (sehr basale) Webseite schreiben. \n",
    "                Deine Aufgabe ist es, die im Screenshot gezeigte Rezeptwebseite nachzuprogrammieren. \n",
    "                Viel mehr als die oben gegebene HTML-Grundstruktur benötigst Du dafür nicht. Bloß zwei Dinge fehlen Dir:</p>\n",
    "            <ol>\n",
    "                <li>Um den Link zum <a href=\"https://www.eat-this.org/baba-ganoush-orientalischer-auberginensalat/#recipe\">Originalrezept</a> einzubetten, \n",
    "                    kannst Du folgende Syntax verwenden, wobei Du den Link bei \"link\" innerhalb der Anführungszeichen einsetzt.<br><br>\n",
    "                    <code>&lt;a href=\"link\"&gt;Eat this&lt;/a&gt;</code></li><br>\n",
    "                <li>Um eine Liste zu nummerieren, verwendest Du das Tag <code>&lt;ol&gt;</code> für <u>o</u>rdered <u>l</u>ist \n",
    "                    statt <code>&lt;ul&gt;</code> (<u>u</u>nordered <u>l</u>ist).</li>\n",
    "            </ol>\n",
    "            <p>Schreib den Code in einem Sublime Text-Dokument und speichere es anschließend mit der Endung \".html\". \n",
    "               Wenn Du es nun in einem beliebigen Browser öffnest, sollte Dir Deine Rezeptwebseite angezeigt werden.</p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../../3_Dateien/Grafiken_und_Videos/lieblingsrezept.png\" width=\"1000\">\n",
    "        </td> \n",
    "      </tr>\n",
    "</table>\n",
    "    \n",
    "*Die Lösung findet sich im Ordner \"3_Dateien/HTML/lieblingsrezept.html\". Öffne das Dokument mit Sublime Text, um den Code zu inspizieren, bzw. mit einem Browser, um die Webseite gerendert zu sehen.*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead7b7e0-867f-47c5-b939-f21f7e13ecd2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "✏️ **Lösung 3:** In folgenden HTML-Code haben sich diverse Fehler eingeschlichen. Wieviele findest Du? Beheb sie alle.  \n",
    "\n",
    "*Es befinden sich acht Fehler im Code, und zwar:*\n",
    "\n",
    "- *Das erste Kind des `<html>`-Elements heißt `<head>` und nicht `<kopf>`.*\n",
    "- *Textinhalte werden **nicht** von Anführungszeichen umrahmt, es sei denn diese sollen auch gerendert werden. Dieser Fehler betrifft alle Textinhalte.*\n",
    "- *Das Encoding \"UTF-9\" gibt es nicht, korrekt ist \"UTF-8\".*\n",
    "- *Das erste `<p>`-Element endet mit dem falschen Tag (`</q>`).*\n",
    "- *Dem `<div>`-Element fehlt das schließende Tag (`</div>`).*\n",
    "- *Dem zweiten `<p>`-Element fehlt ebenfalls das schließende Tag (`</p>`).*\n",
    "- *Der Wert im `href`-Attribut, also der Link, muss in Anführungszeichen stehen. Außerdem können nur Links beginnend mit \"http(s)://\" angeklickt werden.*\n",
    "- *Im schließenden `<html>`-Tag wird der falsche Schrägstrich verwendet.*\n",
    "\n",
    "*Keine Fehler sind hingegen:*\n",
    "\n",
    "- *die fehlende Einrückung der `<head>`- und `<body>`-Elemente unter dem `<html>`-Element. Einrückung spielt bei HTML keine Rolle. Einzig die korrekte Verschachtelung der Elemente ist relevant. HTML-Code wird meist eingerückt angezeigt, um die Beziehungen zwischen den Elementen zu verdeutlichen.*\n",
    "- *das vermeintlich fehlende schließende Tag im `<img>`-Element. `<img>`-Elemente benötigen kein schließendes Tag, da sie über keinen Inhalt verfügen (außer natürlich den Bildinhalt, der aber über ein Attribut geladen wird).*\n",
    "\n",
    "*Korrekt sollte der Code so aussehen (mit optionalen Einrückungen):*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b36dc93-e0f1-4c8a-ae5a-77b325fe74ec",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Finde den Fehler!</title>\n",
    "        <meta charset=\"UTF-8\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <p>Es wimmelt nur so vor <b>Fehlern</b> in diesem HTML-Code</p>\n",
    "        <div>\n",
    "            <p>Eine gute Ressource zum Weiterlernen ist <a href=\"https://www.w3schools.com/html\">w3schools</a>.</p>\n",
    "            <img src=\"picture.png\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a64422-d49f-4eae-836c-13c5d44567aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Lösung 4:** Reproduzier folgenden Text mithilfe von HTML. Achte auf die korrekte Hierarchie der Elemente sowie darauf, alle Tags zu schließen. Handelt es sich um formatierende Inline-Elemente oder um strukturierende Blockelemente?\n",
    "\n",
    "<img src=\"../../3_Dateien/Grafiken_und_Videos/HTML_Formatierung.png\"> \n",
    "\n",
    "*Doppelklick in die folgende Markdown-Zelle, um den zugrundeliegenden HTML-Code zu sehen. Sämtliche Elemente sind formatierende Inline-Elemente.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae1eb7-379f-4fe8-85b7-de481adfea62",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Sog. <i><a href=\"https://irgendeinlink.com\">Definitionen</a></i> kursiv zu setzen, ist <u>strengstens <b>verboten</b></u>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016d47f-4f2f-44ea-b773-b4b1b89ab1f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Lösung 5:** Besuch die Webseite der [Tagesschau](https://www.tagesschau.de) (und überprüf gleich mal, ob die oben extrahierte Schlagzeile wirklich die aktuelle ist!) und ergänz die in `categories` bereits angefangene Liste mit Ressorts, die Nachrichten zu spezifischen Themen bieten. Besuch außerdem die Webseiten einiger Ressorts und analysier, wie die jeweiligen Links aufgebaut sind. Nutz dieses Wissen und iterier über `categories`, um den Quelltext jedes Ressorts herunterzuladen. Benutz denselben Code wie oben, um die oberste Schlagzeile für jedes Ressort zu extrahieren. Lass Dir Ressort und aktuelle oberste Schlagzeile nacheinander ausgeben. \n",
    "\n",
    "Schreib den Code für den Abruf- bzw. Extraktionsschritt in separaten Zellen. In der ersten Zelle rufst Du alle Quelltexte ab und hängst sie einer Liste an. In der zweiten Zelle extrahierst Du aus jedem Quelltext die gewünschten Informationen. So führst Du den Abrufschritt nicht mehr aus, wenn Du am Extraktionsschritt arbeitest, wodurch der Server der Tagesschau nicht unnötig belastet wird. Dadurch sinkt das Risiko, dass Du blockiert wirst.\n",
    "\n",
    "<details><summary>📌 Herausforderung </summary>\n",
    "<br>Extrahier zusätzlich zur obersten Schlagzeile den Link, der zum entsprechenden Artikel führt und lass ihn Dir ebenfalls ausgeben. Klick auf die ausgegebenen Links, um zu schauen, ob sie wie erwartet funktionieren.\n",
    "<br><br>\n",
    "Das entsprechende Element findest Du auf die gleiche Weise wie für die oberste Schlagzeile beschrieben. Da sich der Link aber im <code>href</code>-Attribut (und nicht im Elementinhalt) verbirgt, hängst Du anstatt <code>text</code> die Methode <code>get(\"href\")</code> an das im Quelltext gefundene Element. \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b90dd4-8270-4d91-8a92-14b727da6233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Abrufschritt\n",
    "categories = [\"Inland\", \"Ausland\", \"Wirtschaft\", \"Wissen\", \"Faktenfinder\"]\n",
    "\n",
    "base_link = \"https://www.tagesschau.de\"\n",
    "\n",
    "source_codes = []\n",
    "\n",
    "for category in categories:\n",
    "    \n",
    "    #Abrufen des Quelltexts zum konkatenierten Link und Anhängen an 'source_codes'\n",
    "    source_codes.append(requests.get(base_link + \"/\" + category, timeout=5, headers=headers).text)\n",
    "    \n",
    "    print(f'Fertig mit der Kategorie \"{category}\"') #Manuelle Fortschrittsausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71052948-1633-4e81-aee4-43a2856a5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraktionsschritt\n",
    "for i in range(len(source_codes)):\n",
    "    \n",
    "    soup = BeautifulSoup(source_codes[i])\n",
    "    \n",
    "    headline = soup.find(\"span\", class_=\"teaser__headline\")\n",
    "    \n",
    "    #Herausforderung\n",
    "    link = soup.find(\"a\", class_=\"teaser__link\").get(\"href\")\n",
    "    \n",
    "    print(f\"Aktuelle oberste Schlagzeile aus dem Ressort: '{categories[i]}': '{headline.text}'\")\n",
    "    \n",
    "    #Herausforderung\n",
    "    print(f\"Artikellink: {base_link + link}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2991c4e8-65cc-424d-9053-9cc0798d10d7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Lösung 6:** Extrahier die oberste Überschrift (\"Schlecht vernetzt? Psychische Störungen im Gehirn\") sowie die erste Überschrift auf der nächstkleineren Ebene (\"Hirnforschung: Damals und heute\") aus dem Quelltext. Extrahier ebenfalls die erste Bildunterschrift (\"Die Phrenologie (Bildquelle)\"). Als Erstes musst Du dafür herausfinden, mit welchen Tags diese Elemente versehen sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ca9a3-224a-4568-9e0f-09594974a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapen bzw. Einlesen des Quelltexts sowie Parsen (nur im Lösungsnotebook notwendig)\n",
    "article = requests.get(\"https://scilogs.spektrum.de/hirn-und-weg/schlecht-vernetzt-psychische-stoerungen-im-gehirn/\", timeout=5, headers=headers).text\n",
    "#with open(\"../../3_Dateien/HTML/artikel.html\") as f:\n",
    "    #article = f.read()\n",
    "    \n",
    "soup = BeautifulSoup(article)\n",
    "\n",
    "#Eigentliche Lösung (Wichtig: 'text'-Attribut am Ende)\n",
    "print(soup.find(\"h1\").text) #Oberste Überschrift\n",
    "print(soup.find(\"h3\").text) #Nächstniedrigere Überschrift, im Quelltext wird die von HTML vorgegebene Ebene h2 übersprungen\n",
    "print(soup.figcaption.text) #Bildunterschrift über dot-Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558f6b4-6972-4a93-bdf6-89a9765006b5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Lösung 7:** In dieser Übung sollst Du die Kommentare unter dem Artikel extrahieren. Identifizier dazu ein oder mehrere Suchkriterien im Quelltext, anhand derer Du alle Kommentare extrahieren kannst. Konzentrier Dich vorerst auf die Kommentare auf der ersten Ebene (ignorier also Kommentare zu Kommentaren). \n",
    "\n",
    "Lass Dir für jeden Kommentar folgende Informationen ausgeben:\n",
    "\n",
    "1. Wer hat den Kommentar verfasst?\n",
    "2. Wann wurde der Kommentar verfasst?\n",
    "3. Wie lautet der Text des Kommentars?\n",
    "\n",
    "<details><summary>📌 Herausforderung </summary>\n",
    "<br>Extrahier zusätzlich die Kommentare zu den Kommentaren und lass sie Dir unterhalb des zugehörigen Hauptkommentars, wiederum mit Verfasser:in, Datum und Text, ausgeben. Benutz dafür einen regulären Ausdruck.\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b3c23-494b-4934-aeb4-b4313dcbb3e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Achtung: Code funktioniert nur, wenn der Code zur vorangehenden Übung ausgeführt wurde!\n",
    "\n",
    "comments = soup.find(\"ol\", class_=\"commentlist\") #Extrahieren eines maßgeschneiderten Elements, das nur den Bereich mit den Kommentaren umfasst\n",
    "\n",
    "\"\"\"Zwei Alternativen, um die gewünschten Werte des 'class'-Attributs zu spezifizieren: Erstens: Eine Liste an Werten, die im Quelltext \n",
    "für Kommentare auf erster Ebene verwendet werden (Achtung: Liste muss ggf. erweitert werden, sollten seit 03/24 weitere Kommentare gepostet \n",
    "worden sein, vgl. Übung 8 unten; die hier verwendete Liste funktioniert in jedem Fall für den Quelltext, wie er in \"3_Dateien/HTML/artikel.html\" \n",
    "gespeichert wurde). Zweitens: mit regulärem Ausdruck.\"\"\"\n",
    "\n",
    "#Erste Alternative\n",
    "values = [\"comment even thread-even depth-1\", \"comment odd alt thread-odd thread-alt depth-1\"]\n",
    "\n",
    "#Zweite Alternative\n",
    "#import re\n",
    "#values = re.compile(\"comment[\\w\\s-]+depth-1\")\n",
    "\n",
    "#Suchen nach allen Elementen mit bestimmten Werten beim 'class'-Attribut\n",
    "for comment in comments.find_all(class_=values):\n",
    "  \n",
    "    #Suchen nach relevanter Tag-Attribut-Kombination für alle drei zu extrahierenden Informationen\n",
    "    author = comment.find(class_=\"comment-headline\").text #Extraktion des Textinhalts aus diesem Element (keine Unterelemente)\n",
    "    date = comment.find(\"li\", class_=\"comment-meta\").text #Extraktion des Textinhalts aus dem Unterelement\n",
    "    text = comment.find(class_=\"comment-text\").text #Extraktion sämtlichen Textinhalts aus den Unterelementen\n",
    "    print(f\"Kommentar von {author}, gepostet am {date.strip()}: {text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032dfe8c-cb89-42e9-95b9-c2b09bed9556",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Herausforderung\n",
    "import re\n",
    "#Regulärer Ausdruck matcht Kommentare auf Ebene eins bis max. fünf\n",
    "values = re.compile(\"comment[\\w\\s-]+depth-[1-5]\")\n",
    "\n",
    "for comment in comments.find_all(class_=values):\n",
    "  \n",
    "    #Suchen nach relevanter Tag-Attribut-Kombination für alle drei zu extrahierenden Informationen\n",
    "    author = comment.find(class_=\"comment-headline\").text #Extraktion des Textinhalts aus diesem Element (keine Unterelemente)\n",
    "    date = comment.find(\"li\", class_=\"comment-meta\").text #Extraktion des Textinhalts aus dem Unterelement\n",
    "    text = comment.find(class_=\"comment-text\").text #Extraktion sämtlichen Textinhalts aus den Unterelementen\n",
    "    \n",
    "    #Optionale Extraktion der Ebene, auf der sich der Kommentar befindet (für Ausgabe unten)\n",
    "    depth = int(comment.get(\"class\")[-1][-1])\n",
    "    \n",
    "    #Ausgabe der Informationen inkl. Einrückung bei Kommentaren auf zweiter, dritter etc. Ebene\n",
    "    print(f\"{' '*depth*2 if depth > 1 else ''}Kommentar auf Ebene {depth} von {author}, veröffentlicht am {date.strip()}: {text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386372ae-a8ce-4eb7-86ad-5506a49eccda",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Lösung 8:** Begib Dich auf die [SciLogs-Startseite](https://scilogs.spektrum.de) und trag einige Links zu Blogeinträgen manuell in `links` zusammen. Ruf in derselben Zelle die Quelltexte zu allen Seiten ab und speichere sie in einer weiteren Liste. \n",
    "\n",
    "Parse anschließend in der zweiten Zelle iterativ einen Quelltext nach dem anderen und extrahier jeweils die Kommentare. Hierfür kannst Du grundsätzlich den gleichen Code wie in Übung 7 verwenden. Pass ihn aber (sofern nötig) so an, dass nun alle Kommentare extrahiert werden, auch jene auf zweiter, dritter etc. Ebene. Stell sicher, dass Du auch wirklich alle Kommentare pro Blogeintrag extrahierst.\n",
    "\n",
    "Lass Dir zusätzlich zu den Informationen aus Übung 7 vor den Kommentaren jeweils den Titel des Blogeintrags ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7594db-3863-4d91-b701-325dbcac7918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Abrufschritt\n",
    "links = [\"https://scilogs.spektrum.de/hirn-und-weg/das-raetsel-der-genitalien/\",\n",
    "         \"https://scilogs.spektrum.de/natur-des-glaubens/der-atheistische-evolutionsbiologe-richard-dawkins-und-das-kulturchristentum/\",\n",
    "         \"https://scilogs.spektrum.de/menschen-bilder/showdown-im-bundesrat-cannabis-ist-entkriminalisiert/\",\n",
    "         \"https://scilogs.spektrum.de/natur-des-glaubens/ki-experiment-kann-deep-ai-die-ressourcenfluch-und-rentierstaatstheorie-unterscheiden/\"]\n",
    "\n",
    "articles = []\n",
    "\n",
    "for link in links:\n",
    "    articles.append(requests.get(link, timeout=5, headers=headers).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f52ed-95ce-4229-8727-3490d5e856fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Extraktionsschritt\n",
    "for article in articles:\n",
    "        \n",
    "    soup = BeautifulSoup(article)\n",
    "    \n",
    "    #Ausgabe Titel des Blogeintrags\n",
    "    print(soup.find(\"h1\").text, \"\\n\")\n",
    "    \n",
    "    comments = soup.find(\"ol\", class_=\"commentlist\") #Extrahieren eines maßgeschneiderten Elements, das nur den Bereich mit den Kommentaren umfasst\n",
    "\n",
    "    i=0 #Zähler\n",
    "    \n",
    "    #Regulärer Ausdruck matcht Kommentare auf Ebene eins bis max. fünf\n",
    "    values = re.compile(\"comment[\\w\\s-]+depth-[1-5]\")\n",
    "\n",
    "    for comment in comments.find_all(class_=values):\n",
    "\n",
    "        #Suchen nach relevanter Tag-Attribut-Kombination für alle drei zu extrahierenden Informationen\n",
    "        author = comment.find(class_=\"comment-headline\").text #Extraktion des Textinhalts aus diesem Element (keine Unterelemente)\n",
    "        date = comment.find(\"li\", class_=\"comment-meta\").text #Extraktion des Textinhalts aus dem Unterelement\n",
    "        text = comment.find(class_=\"comment-text\").text #Extraktion sämtlichen Textinhalts aus den Unterelementen\n",
    "\n",
    "        #Optionale Extraktion der Ebene, auf der sich der Kommentar befindet (für Ausgabe unten)\n",
    "        depth = int(comment.get(\"class\")[-1][-1])\n",
    "\n",
    "        #Ausgabe der Informationen inkl. Einrückung bei Kommentaren auf zweiter, dritter etc. Ebene\n",
    "        print(f\"{' '*depth*2 if depth > 1 else ''}Kommentar auf Ebene {depth} von {author}, veröffentlicht am {date.strip()}: {text.strip()}\\n\")\n",
    "        i+=1      \n",
    "            \n",
    "    \"\"\"Überprüfung, ob alle Kommentare gescrapt wurden, indem der Zähler mit der aus dem Element mit id='comments-title'\n",
    "    herausgesclicten Zahl übereinstimmt\"\"\"\n",
    "    if not soup.find(id=\"comments-title\").text.strip().split(\" \")[0] == str(i):\n",
    "        print(\"⚠️ NICHT ALLE KOMMENTARE WURDEN GESCRAPT!\")\n",
    "    \n",
    "    print(\"----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a87f95-d390-43d3-942a-cba555868d57",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Lösung 9:** Benutz `trafilatura`, um Dir die Artikeltexte sämtlicher Blogeinträge ausgeben zu lassen, deren URLs Du in Übung 8 in `links` zusammengetragen hast. Recherchier in der [Dokumentation](https://trafilatura.readthedocs.io/en/latest/corefunctions.html#trafilatura.bare_extraction) von `trafilatura`, wie Du die Kommentare dabei herausfiltern kannst. Lass Dir bei jedem Blogeintrag den Link, die ersten 300 Zeichen des Texts, das Auslassungszeichen \"[...]\" sowie die finalen 300 Zeichen ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a110115-ad84-492b-a88b-5910e7b7f100",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Achtung: Code funktioniert nur, wenn der Code zur vorangehenden Übung ausgeführt wurde!\n",
    "\n",
    "import trafilatura\n",
    "\n",
    "for link in links:\n",
    "    downloaded = trafilatura.fetch_url(link) #Abrufschritt\n",
    "    extract = trafilatura.extract(downloaded, include_comments=False) #Extraktionsschritt mit Parameter zum Ausschließen von Kommentaren\n",
    "    \n",
    "    #Ausgabe von Link und gewünschtem Textausschnitt\n",
    "    print(f\"{link}: {extract[0:300]}\\n[...]\\n{extract[-300:]}\\n\")\n",
    "    \n",
    "    \"\"\"Wie sich zeigt, funktioniert das Herausfiltern von Kommentaren gut, allerdings erkennt 'trafilatura'\n",
    "    die Quellen als Teil des Haupttexts (was natürlich am HTML-Aufbau der Seiten liegt). Je nach Anwendungsfall \n",
    "    muss 'trafilatura'-Output nachbearbeitet werden bzw. stattdessen der Weg über das zielgerichtete\n",
    "    Extrahieren mit 'BeautifulSoup' gewählt werden.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da003c3-16ff-4575-a559-2cbf0e0402c2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "***\n",
    "<table>\n",
    "      <tr>\n",
    "        <td>\n",
    "            <img src=\"../../3_Dateien/Lizenz/CC-BY-SA.png\" width=\"400\">\n",
    "        </td> \n",
    "        <td>\n",
    "            <p>Dieses Notebook sowie sämtliche weiteren <a href=\"https://github.com/yannickfrommherz/exdimed-student/tree/main\">Materialien zum Programmierenlernen für Geistes- und Sozialwissenschaftler:innen</a> sind im Rahmen des Projekts <i>Experimentierraum Digitale Medienkompetenz</i> als Teil von <a href=\"https://tu-dresden.de/gsw/virtuos/\">virTUos</a> entstanden. Erstellt wurden sie von Yannick Frommherz unter Mitarbeit von Anne Josephine Matz. Sie stehen als Open Educational Resource nach <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY SA</a> zur freien Verfügung. Für Feedback und bei Fragen nutz bitte das <a href=\"https://forms.gle/VsYJgy4bZTSqKioA7\">Kontaktformular</a>.\n",
    "        </td>\n",
    "      </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}