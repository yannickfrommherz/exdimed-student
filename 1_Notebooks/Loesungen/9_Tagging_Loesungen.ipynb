{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f4dd3b-c3fe-453c-928c-a5e77f880d54",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Tagging (Lösungen)\n",
    "\n",
    "☝️ Beachte: Es gibt beim Programmieren fast immer verschiedene Lösungswege. Deine Lösung mag anders aussehen, aber dennoch zum gewünschten Resultat führen. Das richtige Resultat ist das Wichtigste. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445b783-1bc8-448b-a633-0ede06005ec4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Übung 1:** Lemmatisier den Koalitionsvertrag von 2018, der sich im Ordner \"3_Dateien/Koalitionsvertraege\" befindet. Find dann erstens heraus, welches Lemma am häufigsten darin vorkommt sowie wie oft. Ermittle zweitens, welchen Wortformen dieses häufigste Lemma wie oft entspricht. Da der Koalitionsvertrag recht lang ist, dauert die Ausführung des Codes vielleicht etwas länger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d120a-9269-44c7-9a64-c16c3c7bec26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "import nltk, pandas as pd\n",
    "F\n",
    "ht_tagger = ht.HanoverTagger('morphmodel_ger.pgz') \n",
    "\n",
    "#Einlesen des Koalitionsvertrags\n",
    "#Achtung: anderer Pfad als im Notebook, da das Lösungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/koalitionsvertrag_2018.txt\", encoding=\"utf8\") as f:\n",
    "    kv18 = f.read()\n",
    "\n",
    "kv18_tokenized = nltk.word_tokenize(kv18) #Tokenisierung\n",
    "\n",
    "kv18_output = ht_tagger.tag_sent(kv18_tokenized) #Tagging\n",
    "\n",
    "#Überführung in DataFrame zwecks Datenauswertung\n",
    "kv18_df = pd.DataFrame(kv18_output, columns=['Wortform', 'Lemma', 'POS'])\n",
    "\n",
    "\"\"\"Häufigstes Lemma mithilfe von 'value_counts' sowie 'idxmax' herausfinden ('idxmax' gibt den Namen der Zeile \n",
    "mit dem höchsten Wert zurück, also das häufigste Lemma; Gegenstück dazu ist 'max', das den höchsten Wert zurückgibt, s. u.)\"\"\"\n",
    "most_frequent_lemma = kv18_df.Lemma.value_counts().idxmax() \n",
    "\n",
    "#Erster Teil der Aufgabe\n",
    "print(f\"Das häufigste Lemma, {most_frequent_lemma}, kommt {len(kv18_df[kv18_df.Lemma == most_frequent_lemma])} vor.\") #Mit Filter und 'len'\n",
    "#print(f\"Das häufigste Lemma, {most_frequent_lemma}, kommt {kv18_df.Lemma.value_counts().max()} vor.\") #Alternativer Code mit 'max'\n",
    "\n",
    "#Zweiter Teil der Aufgabe\n",
    "#Filtern von 'kv18_df' nach 'most_frequent_lemma' in der Spalte \"Lemma\", danach Zugriff auf Spalte \"Wortform\" und Auszählen der Werte darin\n",
    "word_forms_of_most_frequent_lemma = kv18_df[kv18_df.Lemma == most_frequent_lemma].Wortform.value_counts()\n",
    "word_forms_of_most_frequent_lemma #Ausgabe der ausgezählten Wortformen zum häufigsten Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8d265-4dd8-4783-9271-4df587103609",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Übung 2:** Bring in Erfahrung, welche fünf Adjektive am häufigsten im ersten Kapitel von Niels Holgersen vorkommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c0276-d5d8-47ed-b4aa-632cbe2e9e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Einlesen, Tokenisieren und Taggen des Texts (nur im Lösungsnotebook notwendig)\n",
    "#Achtung: anderer Pfad als im Notebook, da das Lösungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Niels_Holgersen/Kapitel_1.txt\", encoding=\"utf8\") as f:\n",
    "    niels_holgersen = f.read()\n",
    "    \n",
    "niels_holgersen_tokenized = nltk.word_tokenize(niels_holgersen) #Tokenisierung\n",
    "\n",
    "niels_holgersen_output = ht_tagger.tag_sent(niels_holgersen_tokenized) #Tagging\n",
    "\n",
    "#Überführung in DataFrame zwecks Datenauswertung\n",
    "niels_holgersen_df = pd.DataFrame(niels_holgersen_output, columns=['Wortform', 'Lemma', 'POS']) \n",
    "\n",
    "#Eigentliche Lösung\n",
    "\n",
    "\"\"\"Filtern von 'niels_holgersen_df' nach den Werten \"ADJ(A)\" und \"ADJ(D)\" in der Spalte \"POS\"\n",
    "mithilfe der 'isin'-Methode. Beachte, dass die vom 'HanoverTagger' verwendeten Tags minimal\n",
    "vom Standardtagset abweichen\"\"\"\n",
    "adjectives_df = niels_holgersen_df[niels_holgersen_df.POS.isin([\"ADJ(A)\", \"ADJ(D)\"])]\n",
    "\n",
    "\"\"\"Ausgabe der fünf häufigsten Lemmata im gefilterten DataFrame (denn Lemmata eignen sich besser \n",
    "als Wortformen zur Beantwortung der Fragestellung)\"\"\"\n",
    "adjectives_df.Lemma.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645cb62-60a2-4883-a1fe-dcb38746afdc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Übung 3:** Reicher den Koalitionsvertrag von 2021 nicht nur mit morphologischen Tags, sondern auch mit Lemmata und POS-Tags an. Wähl selbst, welchen Tagger Du dazu verwendest und informier Dich ggf. in der entsprechenden Dokumentation zu den Taggingmöglichkeiten. Überführ die getaggten Daten abschließend in ein DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221d3e5-3e5f-4552-8292-811024bc8313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Einlesen des Texts\n",
    "#Achtung: anderer Pfad als im Notebook, da das Lösungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/koalitionsvertrag_2021.txt\", encoding=\"utf8\") as f:\n",
    "    kv21 = f.read()\n",
    "\n",
    "\"\"\"Wir benutzen hier 'spacy', da es sich um einen langen Text handelt. Sollte jedoch die Tagqualität \n",
    "entscheidend für Dich sein, lohnt es sich u. U. 'stanza' zu benutzen.\"\"\"\n",
    "import spacy\n",
    "\n",
    "spacy_tagger = spacy.load(\"de_core_news_sm\") #Initialisieren von 'spacy_tagger' mit dem deutschsprachigen Modell\n",
    "\n",
    "spacy_output = spacy_tagger(kv21) #Tagging\n",
    "\n",
    "#Überführen des Outputs in ein DataFrame inkl. POS-Tags und Lemmata (Kommentare zur DataFrame-Erstellung s. Notebook)\n",
    "list_of_all_dicts = []\n",
    "\n",
    "for word in spacy_output:\n",
    "    dict_per_word = {\n",
    "        \"Wortform\": word.text,\n",
    "        \"Morphologie\": str(word.morph).strip(\"()\"), #Casting zwecks Bereinigung und um string-Objekt statt 'spacy'-Objekt zu überführen\n",
    "        \"POS\": word.tag_, #Das 'tag_'-Attribut gibt ein feingliedrigeres POS-Tag zurück, als das 'pos_'-Attribut\n",
    "        \"Lemma\": word.lemma_}\n",
    "    list_of_all_dicts.append(dict_per_word)\n",
    "    \n",
    "kv21_df = pd.DataFrame(list_of_all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967a5f1-681c-47e6-91e5-3858a0fe6234",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv21_df #Ausgabe des DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b02d7-254f-4674-b867-190db31ebc58",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Übung 4:** Ermittle die 20 häufigsten femininen Nomina im Koalitionsvertrag von 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b6d3e-dd4d-4c13-8c1c-8fbc4cdb417f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Der Übersichtlichkeit zuliebe definieren wir die beiden benötigten Filter vorab und zwar in \n",
    "runde Klammern gesetzt, was bei Verwendung des '&'-Operators zwingend nötig ist. Nur nach Genus\n",
    "zu filtern reicht nicht, da auch Artikel und Adjektive feminines Genus haben können. Nur nach\n",
    "Nomina zu filtern reicht nicht, da Nomina auch maskulines oder neutrales Genus haben können.\"\"\"\n",
    "filter_noun = (kv21_df.POS == \"NN\") #\"NOUN\" statt \"NN\", falls 'pos_'-Attribut oben benutzt wurde\n",
    "filter_gender = (kv21_df.Morphologie.str.contains(\"Gender=Fem\"))\n",
    "\n",
    "\"\"\"Anwenden der Filter auf 'kv21_df', Zugriff auf Spalte \"Lemma\", denn wir sind nicht an Wortformen\n",
    "interessiert, sondern an Wörtern in ihrer Grundform. Anschließendes Auszählen der Werte in dieser Spalte\n",
    "und Ausgabe der obersten 20.\"\"\"\n",
    "kv21_df[filter_noun & filter_gender].Lemma.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4423ab9-3e59-42de-a9eb-75b7f16a2b9c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Übung 5:** Find heraus, welche Wörter besonders häufig als Subjekt im Koalitionsvertrag von 2021 stehen und lass Dir die zehn Spitzenreiter ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23afd1fb-7415-4d47-a3a1-0c64fe52a63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Koalitionsvertrag muss nicht noch einmal getaggt werden, da 'spacy' syntaktische Informationen standardmäßig mittaggt.\n",
    "Voraussetzung ist natürlich, dass 'spacy_output' noch im Arbeitsspeicher ist. Wir müssen allerdings das DataFrame \n",
    "neu erstellen, um eine weitere Spalte zur Syntax zu erhalten – Kommentare zur DataFrame-Erstellung s. o.\"\"\"\n",
    "list_of_all_dicts = []\n",
    "\n",
    "for word in spacy_output:\n",
    "    dict_per_word = {\n",
    "        \"Wortform\": word.text,\n",
    "        \"Morphologie\": str(word.morph).strip(\"()\"), \n",
    "        \"POS\": word.tag_, \n",
    "        \"Lemma\": word.lemma_,\n",
    "        \"Syntax\": word.dep_} #Neue Spalte\n",
    "    list_of_all_dicts.append(dict_per_word)\n",
    "    \n",
    "kv21_df_syntax = pd.DataFrame(list_of_all_dicts) \n",
    "\n",
    "#Filtern des DataFrame nach dem Wert \"sb\" in der Spalte \"Syntax\", Zugriff auf Spalte \"Lemma\", Auszählen der Werte und Ausgabe der obersten zehn \n",
    "kv21_df_syntax[kv21_df_syntax.Syntax == \"sb\"].Lemma.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1a61c-1d8a-40c6-9237-ff84c2175c11",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Übung 6:** Lad Dir mithilfe der [API für Wikipedia](https://pypi.org/project/Wikipedia-API/) `wikipediaapi` (vgl. Zusatzübungen zum Notebook \"Reguläre Ausdrücke\") den *englischsprachigen* Artikel zu einer prominenten Person Deiner Wahl herunter und extrahier alle darin erwähnten Personen mithilfe von `spacy` und `stanza`. Welche Unterschiede zeigen sich beim Vergleich der zehn häufigsten Personen in den beiden Outputs? Verwend `pandas` für diesen Vergleich.\n",
    "\n",
    "Lass Dir außerdem sämtliche von `spacy` gefundenen Named Entities visualisieren (also nicht nur die Personen). Erkennst Du einen Unterschied im Vergleich zum NER-Tagging deutschsprachiger Texte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4b55e-f262-44c8-8d10-39b8d7e82ecf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Herunterladen des Artikels\n",
    "mail_address = \"\" #Trag hier Deine Mailadresse ein.\n",
    "\n",
    "import wikipediaapi #Kein Bindestrich beim Import\n",
    "\n",
    "name = \"Kamala Harris\" #Angabe der Person, deren Artikel heruntergeladen werden soll\n",
    "\n",
    "#Initialisieren der Schnittstelle mittels Angabe von 'user_agent', Sprache und Extraktionsformat \n",
    "#Wichtig: Spezifiziere die richtige Sprache im 'language'-Parameter\n",
    "Wiki_API = wikipediaapi.Wikipedia(user_agent=f\"Programmierenlernen, {mail_address}\", language=\"en\", extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "article = Wiki_API.page(name).text\n",
    "\n",
    "#Initialisieren eines Taggers mit 'spacy' für englischsprachige Texte\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\") #Download eines englischsprachigen Modells\n",
    "spacy_tagger_en = spacy.load(\"en_core_web_sm\") #Initialisieren von 'spacy_tagger_en' mit dem englischsprachigen Modell\n",
    "spacy_output = spacy_tagger_en(article) #Tagging\n",
    "\n",
    "#Initialisieren eines Taggers mit 'stanza' für englischsprachige Texte\n",
    "import stanza \n",
    "stanza_tagger_en = stanza.Pipeline(lang=\"en\", processors=\"tokenize,ner\") #Spezifikation von 'lang' als \"en\"\n",
    "stanza_output = stanza_tagger_en(article) #Tagging\n",
    "\n",
    "#Erstellen je einer 'pandas'-Series mit nur den als Person getaggten Entitäten mithilfe von List Comprehensions\n",
    "#Achtung: Das Tag für Personen ist bei beiden Taggern \"PERSON\" und nicht \"PER\" wie bei deutschsprachigen Texten.\n",
    "spacy_PER = pd.Series([person.text for person in spacy_output.ents if person.label_ == \"PERSON\"])\n",
    "stanza_PER = pd.Series([person.text for person in stanza_output.ents if person.type == \"PERSON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4998dcd-6313-4b38-ac83-993151d2ee66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe der häufigsten zehn Personen gemäß 'spacy' mithilfe von 'value_counts'\n",
    "spacy_PER.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625d826-ff38-4516-a7db-8b38f9be315c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe der häufigsten zehn Personen gemäß 'stanza' mithilfe von 'value_counts'\n",
    "stanza_PER.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129083c-a484-4e7a-9c35-5eff05025377",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Unterschied im Output der beiden Tagger: Beim Artikel von Kamala Harris zum Zeitpunkt des Verfassens dieses Notebooks zeigt sich, dass `stanza` zuverlässiger zu taggen scheint. So taggt `spacy` etwa Wörter, die keine Personen bezeichnen. Bei anderen Artikeln oder anderen Versionen desselben Artikels mag dieses Bild jedoch anders aussehen.*\n",
    "\n",
    "*Es zeigt sich außerdem, dass bei beiden Taggern ein Normalisierungsschritt (z.&nbsp;B. Reduktion von \"Biden\" und \"Joe Biden\" auf einen Namen) nachgelagert werden müsste, um die Auszählung der erwähnten Personen zu verbessern.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f9898-0f01-4b04-a3b8-89ded29345f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Visualisierung sämtlicher Tags im Output von 'spacy'\n",
    "from spacy import displacy\n",
    "displacy.render(spacy_output, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61356fa3-10e1-4c76-9f4d-c29bc95ef31b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Unterschied zwischen dem NER-Tagging deutschsprachiger und englischsprachiger Texte: Das Tagset für englischsprachige Texte umfasst weitaus mehr Kategorien, etwa für Daten (\"DATE\") oder Gesetze (\"LAW\").*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00896078-ab2b-49c4-bd83-5080fdfae014",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "✏️ **Übung 7:** Find heraus, ob es sich beim Koalitionsvertrag von 2021 um einen positiven, neutralen oder negativen Text handelt. Überleg Dir genau, was für Input `predict_sentiment` erwartet, d.&nbsp;h. wie Du den Koalitionsvertrag sinnvollerweise taggst.\n",
    "\n",
    "<details><summary>🦊 Herausforderung </summary>\n",
    "<br>Find zusätzlich heraus, bei welchen Sätzen des Koalitionsvertrags ein negatives bzw. positives Sentiment mit einer Wahrscheinlichkeit von über 50% getaggt wurde.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefb5ae-de04-46a0-b049-860e6f8add69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from germansentiment import SentimentModel\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_tagger = SentimentModel()\n",
    "\n",
    "#Einlesen des Texts\n",
    "#Achtung: anderer Pfad als im Notebook, da das Lösungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/koalitionsvertrag_2021.txt\", encoding=\"utf8\") as f:\n",
    "    kv21 = f.read()\n",
    "    \n",
    "sentences = sent_tokenize(kv21) #Aufsplitten in Sätze mithilfe von 'nltk'\n",
    "\n",
    "\"\"\"Auskommentierte Reduktion von 'sentences' auf die ersten 100 Sätze, um Code damit erst zu schreiben, um \n",
    "ihn anschließend, wenn er fehlerfrei funktioniert, auf alle Sätze anzuwenden (s. Tipp 2)\"\"\"\n",
    "#sentences = sentences[0:100] \n",
    "\n",
    "tags = [] #Leere Liste für Tags initialisieren\n",
    "\n",
    "#Erstellen einer Liste mit Tags und 'tdqm' zur Fortschrittsanzeige\n",
    "for sentence in tqdm(sentences):\n",
    "    \"\"\"Übergabe von 'sentence' als Element einer Liste, Speichern des Tags zum ersten Satz (Index null), \n",
    "    da es ja nur einen Satz gibt. Analysier Input- und Outputformat von 'germansentiment', um dies zu verstehen.\"\"\"   \n",
    "    tags.append(sentiment_tagger.predict_sentiment([sentence])[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6eb1e3-9295-4718-8f11-ecfab84a9cd9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Häufigstes Tag beantwortet die Fragestellung (zumindest in der Tendenz)\n",
    "pd.Series(tags).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0356f45-f035-4e60-b9f2-18240ccc66d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Herausforderung\n",
    "from germansentiment import SentimentModel\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_tagger = SentimentModel()\n",
    "\n",
    "#Einlesen des Texts\n",
    "#Achtung: anderer Pfad als im Notebook, da das Lösungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/koalitionsvertrag_2021.txt\", encoding=\"utf8\") as f:\n",
    "    kv21 = f.read()\n",
    "    \n",
    "sentences = sent_tokenize(kv21) #Aufsplitten in Sätze mithilfe von 'nltk'\n",
    "\n",
    "\"\"\"Auskommentierte Reduktion von 'sentences' auf die ersten 100 Sätze, um Code damit erst zu schreiben, um \n",
    "ihn anschließend, wenn er fehlerfrei funktioniert, auf alle Sätze anzuwenden (s. Tipp 2)\"\"\"\n",
    "#sentences = sentences[0:100] \n",
    "\n",
    "#Wahrscheinlichkeiten je Tag werden auch benötigt, daher zweite Liste initialisieren...\n",
    "tags, probabilities = [], []\n",
    "\n",
    "#...und beim Taggen 'output_probabilities' auf 'True' setzen\n",
    "for sentence in tqdm(sentences):\n",
    "    tag, probability = sentiment_tagger.predict_sentiment([sentence], output_probabilities=True)\n",
    "    tags.append(tag[0]); probabilities.append(probability) #Befehle auf einer Zeile mithilfe von Semikolon\n",
    "\n",
    "#Kommentare zur DataFrame-Erstellung s. o.\n",
    "list_of_all_dicts = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    \n",
    "    \"\"\"Schreiben von jeweiligem Tag sowie relevanten Informationen aus dem jeweiligen dictionary auf 'probabilities'\n",
    "    in 'dict_per_sentence'. Das Outputformat der Wahrscheinlichkeiten ist wie gesagt stark verschachtelt, analysier\n",
    "    es genau, um die Indizierung der relevanten Werte zu verstehen.\"\"\"\n",
    "    dict_per_sentence = {\"sentence\": sentences[i],\n",
    "                         \"tag\": tags[i],\n",
    "                         \"positive_prob\": probabilities[i][0][0][1],\n",
    "                         \"negative_prob\": probabilities[i][0][1][1],\n",
    "                         \"neautral_prob\": probabilities[i][0][2][1]}\n",
    "    \n",
    "    list_of_all_dicts.append(dict_per_sentence)\n",
    "    \n",
    "kv21_sentiment = pd.DataFrame(list_of_all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a639340-0255-486d-ae70-f3d47ff3cce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe aller positiven Tags mit Wahrscheinlichkeit von über 50%\n",
    "kv21_sentiment[kv21_sentiment.positive_prob > 0.5].sentence.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a532b-8dab-44e7-897a-8a3ba7c39d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe aller negativen Tags mit Wahrscheinlichkeit von über 50%\n",
    "kv21_sentiment[kv21_sentiment.negative_prob > 0.5].sentence.values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c713960-b8f4-41cb-9522-8bb6371b6605",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Der Tagger beurteilt fast den gesamten Text als neutral formuliert. Neutrale Formulierungen könnten ein Charakteristikum von Koalitionsverträgen sein. Wahrscheinlicher ist jedoch, dass dieser Tagger bei dieser Art von Daten nicht zufriedenstellend funktioniert, zumal das Textgenre durchaus positive Sätze vermuten lässt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bdbaf-fcda-4088-99a0-e5fcf67de414",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "🔧 **Anwendungsfall:** \n",
    "\n",
    "1. Vorgelagerter Abruf- und Extraktionsschritt für den Musteranwendungsfall (Vergleich von Informationsseiten über Steuern des Bundesfinanzministeriums in Standard- und Leichter Sprache hinsichtlich der Verteilung von Wortarten):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d541e90-ef0e-4ba9-8cb7-792a50a78cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "\n",
    "simple_language = \"https://www.bundesfinanzministerium.de/Content/DE/Standardartikel/Service/Leichte_Sprache/steuern.html\"\n",
    "standard_language = \"https://www.bundesfinanzministerium.de/Content/DE/Standardartikel/Themen/Steuern/steuern.html\"\n",
    "\n",
    "#Abruf der Quelltexte zu den beiden Sprachversionen zur Seite über Steuern\n",
    "simple_source_code = requests.get(simple_language, timeout=5, headers=headers).text\n",
    "standard_source_code = requests.get(standard_language, timeout=5).text\n",
    "\n",
    "#Alternativ: Einlesen aus dem Ordner \"3_Dateien/Tagging\", falls Seitenzugriff blockiert wird (zeigt sich an Fehlermeldungen unten)\n",
    "#with open(\"../../3_Dateien/Tagging/simple_language.html\", encoding=\"utf8\") as f, open(\"../../3_Dateien/Tagging/standard_language.html\", encoding=\"utf8\") as g:\n",
    "    #simple_source_code, standard_source_code = f.read(), g.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4487a-0a1d-4a12-ad4a-e6269ec1a42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Definition einer Funktion, die relevante Textteile extrahiert und extern speichert;\n",
    "Für zwei Texte ist die Schaffung einer Funktion natürlich nicht nötig. Ist sie aber einmal \n",
    "geschrieben, kann sie jederzeit auf neue Texte angewendet werden.\"\"\"\n",
    "def extract_and_save_text(source_code, file_name):\n",
    "\n",
    "    #Extraktion des kleinsten HTML-Elements, das die relevanten Textteile beinhaltet\n",
    "    main_content = BeautifulSoup(source_code).find(class_=\"article-wrapper documentleaf basepage\")\n",
    "    \n",
    "    #Speichern aller bereinigten Textparagraphen in externer Datei\n",
    "    with open(f\"../../3_Dateien/Output/{file_name}_text_taxes.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\" \".join([p.text.strip() for p in main_content.find_all(\"p\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ca528-3ea9-4704-a88f-13cb076830cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Aufrufen der Funktion\n",
    "extract_and_save_text(simple_source_code, \"simple\")\n",
    "extract_and_save_text(standard_source_code, \"standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82bf0c-37bc-44ac-8e64-eee3c795e5a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. Tagging am Beispiel des Musteranwendungsfall, inkl. Speicherung der getaggten Daten in externer Datei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97479d77-98d6-49bf-8a21-0a08a35521e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "ht_tagger = ht.HanoverTagger('morphmodel_ger.pgz') \n",
    "\n",
    "#Definition einer Funktion zum Taggen von Texten.\n",
    "def tag_text(input_path, output_path):\n",
    "    \n",
    "    with open(input_path, encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    words = word_tokenize(text) #Tokenisieren\n",
    "\n",
    "    text_tagged = ht_tagger.tag_sent(words) #Tagging\n",
    "    \n",
    "    \"\"\"Überführen in DataFrame (csv-Dateien können auch ohne den Umweg über 'pandas' extern gespeichert werden\n",
    "    [vgl. Notebook \"Input und Output Teil 2\"], wie häufig bietet 'pandas' aber einen unkomplizierten Ansatz)\"\"\"\n",
    "    df_tagged = pd.DataFrame(text_tagged, columns=['Wortform', 'Lemma', 'POS']) \n",
    "    \n",
    "    #Speichern als externe Datei\n",
    "    df_tagged.to_csv(output_path, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d700f-b2ec-4530-95f4-95f5ce016d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Aufrufen der Funktion\n",
    "tag_text(\"../../3_Dateien/Output/simple_text_taxes.txt\", \"../../3_Dateien/Output/simple_text_taxes_output.csv\")\n",
    "tag_text(\"../../3_Dateien/Output/standard_text_taxes.txt\", \"../../3_Dateien/Output/standard_text_taxes_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8af9ba-1ec3-4da8-af4b-a9c8506475ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Visualisieren der Häufigkeitsverteilung von Wortarten mithilfe von Säulendiagramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b077d-53f8-47bb-8406-5a31dc53bf42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Einlesen der extern gepeicherten, getaggten Daten zur Weiterverarbeitung mit 'pandas'\n",
    "simple_df = pd.read_csv(\"../../3_Dateien/Output/simple_text_taxes_output.csv\", encoding=\"utf8\")\n",
    "standard_df = pd.read_csv(\"../../3_Dateien/Output/standard_text_taxes_output.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f87322-1d7f-43f5-8ce1-92b8cc268af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Konfigurieren von Plotgröße sowie Stil (beides optional, verschönert aber das Ergebnis)\n",
    "plt.figure(figsize=(12, 8)) \n",
    "plt.style.use(\"bmh\") #Ändere zwischen \"classic\", \"classic\", \"bmh\", \"tableau-colorblind10\", uvm. \n",
    "\n",
    "#Erstellen von Series mit den ausgezählten Werten in der Spalte \"POS\", normalisiert als relative Werte\n",
    "pos_frequencies_simple = simple_df.POS.value_counts(normalize=True)\n",
    "pos_frequencies_standard = standard_df.POS.value_counts(normalize=True)\n",
    "\n",
    "#Erstellen einer sortierten Liste aller einzigartigen POS-Tags, die in einem oder beiden Texten vorkommen (senkrechter Strich bedeutet \"or\")\n",
    "pos_tags = sorted(set(simple_df.POS.unique()) | set(standard_df.POS.unique()))\n",
    "\n",
    "bar_width = 0.4 #Definieren der Säulenbreite\n",
    "\n",
    "#Definieren von Koordinaten auf der x-Achse, an denen die einzelnen Säulen pro POS-Tag geplottet werden sollen\n",
    "x = np.arange(len(pos_tags))\n",
    "\n",
    "\"\"\"Plotten von je zwei Säulen pro POS-Tag, eine für die relative Häufigkeit des gegebenen Tags im Text\n",
    "in Leichter Sprache und eine für diejenige im Text in Standardsprache. Übergeben werden der 'bar'-Funktion\n",
    "als erstes Argument 'x', das eine Arte Liste mit den Koordinaten auf der x-Achse enthält (genau so viele, \n",
    "wie es Tags zu plotten gibt). Die Säule für Leichte Sprache wird um die halbe 'bar_width' nach links verschoben\n",
    "geplottet, die Säule für Standardsprache um die halbe 'bar_width' nach rechts verschoben. So werden die beiden\n",
    "Säulen direkt nebeneinander, aber nicht übereinader geplottet. Als zweites Argument werden die Koordinaten für\n",
    "die y-Achse in Form der oben erstellten Series mit relativen Häufigkeiten pro POS-Tag übergeben ('reindex(pos_tags)' \n",
    "sorgt dafür, dass die beiden Series gleich sortiert werden). Weiter werden die Säulenbreite und Labels definiert.\"\"\"\n",
    "plt.bar(x - bar_width/2, pos_frequencies_simple.reindex(pos_tags), width=bar_width, label=\"Leichte Sprache\")\n",
    "plt.bar(x + bar_width/2, pos_frequencies_standard.reindex(pos_tags), width=bar_width, label=\"Standardsprache\")\n",
    "\n",
    "#Abschließend werden die Achsen sowie der Plot beschriftet und die POS-Tags um 90 Grad rotiert sowie die Legende eingeblendet\n",
    "plt.xlabel(\"POS-Tags\")\n",
    "plt.ylabel(\"Relative Häufigkeit\")\n",
    "plt.title(\"Häufigkeitsverteilung von POS-Tags nach Sprachschwierigkeit\\n\", fontweight=\"bold\")\n",
    "plt.xticks(x, pos_tags, rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9baf6-8138-4327-889d-6c90c26b20cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Es zeigt sich u.&nbsp;a., dass in den beiden untersuchten Texten in Leichter Sprache im Vergleich zu Standardsprache viel weniger Artikel (\"ART\") und Präpositionen (\"APPR\"), dagegen wesentlich mehr finite Verben (\"VV(FIN)\", \"VM(FIN)\") und Infinitive (\"VV(INF)\") verwendet werden. Um Aussagen über die beiden Sprachschwierigkeitsstufen treffen zu können, müsste die Datenbasis natürlich erweitert werden.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46479c9c-c118-4379-bbb3-4493b4da4c25",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "<table>\n",
    "      <tr>\n",
    "        <td>\n",
    "            <img src=\"../../3_Dateien/Lizenz/CC-BY-SA.png\" width=\"400\">\n",
    "        </td> \n",
    "        <td>\n",
    "            <p>Dieses Notebook sowie sämtliche weiteren <a href=\"https://github.com/yannickfrommherz/exdimed-student/tree/main\">Materialien zum Programmierenlernen für Geistes- und Sozialwissenschaftler:innen</a> sind im Rahmen des Projekts <i>Experimentierraum Digitale Medienkompetenz</i> als Teil von <a href=\"https://tu-dresden.de/gsw/virtuos/\">virTUos</a> entstanden. Erstellt wurden sie von Yannick Frommherz unter Mitarbeit von Anne Josephine Matz. Sie stehen als Open Educational Resource nach <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY SA</a> zur freien Verfügung. Für Feedback und bei Fragen nutz bitte das <a href=\"https://forms.gle/VsYJgy4bZTSqKioA7\">Kontaktformular</a>.\n",
    "        </td>\n",
    "      </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}