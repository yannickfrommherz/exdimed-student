{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f4dd3b-c3fe-453c-928c-a5e77f880d54",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Tagging (L√∂sungen)\n",
    "\n",
    "‚òùÔ∏è Beachte: Es gibt beim Programmieren fast immer verschiedene L√∂sungswege. Deine L√∂sung mag anders aussehen, aber dennoch zum gew√ºnschten Resultat f√ºhren. Das richtige Resultat ist das Wichtigste. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445b783-1bc8-448b-a633-0ede06005ec4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 1:** Lemmatisier den Koalitionsvertrag von 2018, der sich im Ordner \"3_Dateien/Koalitionsvertraege\" befindet. Find dann erstens heraus, welches Lemma am h√§ufigsten darin vorkommt sowie wie oft. Ermittle zweitens, welchen Wortformen dieses h√§ufigste Lemma wie oft entspricht. Da der Koalitionsvertrag recht lang ist, dauert die Ausf√ºhrung des Codes vielleicht etwas l√§nger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d120a-9269-44c7-9a64-c16c3c7bec26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "import nltk, pandas as pd\n",
    "F\n",
    "ht_tagger = ht.HanoverTagger('morphmodel_ger.pgz') \n",
    "\n",
    "#Einlesen des Koalitionsvertrags\n",
    "#Achtung: anderer Pfad als im Notebook, da das L√∂sungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/koalitionsvertrag_2018.txt\", encoding=\"utf8\") as f:\n",
    "    kv18 = f.read()\n",
    "\n",
    "kv18_tokenized = nltk.word_tokenize(kv18) #Tokenisierung\n",
    "\n",
    "kv18_output = ht_tagger.tag_sent(kv18_tokenized) #Tagging\n",
    "\n",
    "#√úberf√ºhrung in DataFrame zwecks Datenauswertung\n",
    "kv18_df = pd.DataFrame(kv18_output, columns=['Wortform', 'Lemma', 'POS'])\n",
    "\n",
    "\"\"\"H√§ufigstes Lemma mithilfe von 'value_counts' sowie 'idxmax' herausfinden ('idxmax' gibt den Namen der Zeile \n",
    "mit dem h√∂chsten Wert zur√ºck, also das h√§ufigste Lemma; Gegenst√ºck dazu ist 'max', das den h√∂chsten Wert zur√ºckgibt, s. u.)\"\"\"\n",
    "most_frequent_lemma = kv18_df.Lemma.value_counts().idxmax() \n",
    "\n",
    "#Erster Teil der Aufgabe\n",
    "print(f\"Das h√§ufigste Lemma, {most_frequent_lemma}, kommt {len(kv18_df[kv18_df.Lemma == most_frequent_lemma])} vor.\") #Mit Filter und 'len'\n",
    "#print(f\"Das h√§ufigste Lemma, {most_frequent_lemma}, kommt {kv18_df.Lemma.value_counts().max()} vor.\") #Alternativer Code mit 'max'\n",
    "\n",
    "#Zweiter Teil der Aufgabe\n",
    "#Filtern von 'kv18_df' nach 'most_frequent_lemma' in der Spalte \"Lemma\", danach Zugriff auf Spalte \"Wortform\" und Ausz√§hlen der Werte darin\n",
    "word_forms_of_most_frequent_lemma = kv18_df[kv18_df.Lemma == most_frequent_lemma].Wortform.value_counts()\n",
    "word_forms_of_most_frequent_lemma #Ausgabe der ausgez√§hlten Wortformen zum h√§ufigsten Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8d265-4dd8-4783-9271-4df587103609",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 2:** Bring in Erfahrung, welche f√ºnf Adjektive am h√§ufigsten im ersten Kapitel von Niels Holgersen vorkommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c0276-d5d8-47ed-b4aa-632cbe2e9e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Einlesen, Tokenisieren und Taggen des Texts (nur im L√∂sungsnotebook notwendig)\n",
    "#Achtung: anderer Pfad als im Notebook, da das L√∂sungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Niels_Holgersen/Kapitel_1.txt\", encoding=\"utf8\") as f:\n",
    "    niels_holgersen = f.read()\n",
    "    \n",
    "niels_holgersen_tokenized = nltk.word_tokenize(niels_holgersen) #Tokenisierung\n",
    "\n",
    "niels_holgersen_output = ht_tagger.tag_sent(niels_holgersen_tokenized) #Tagging\n",
    "\n",
    "#√úberf√ºhrung in DataFrame zwecks Datenauswertung\n",
    "niels_holgersen_df = pd.DataFrame(niels_holgersen_output, columns=['Wortform', 'Lemma', 'POS']) \n",
    "\n",
    "#Eigentliche L√∂sung\n",
    "\n",
    "\"\"\"Filtern von 'niels_holgersen_df' nach den Werten \"ADJ(A)\" und \"ADJ(D)\" in der Spalte \"POS\"\n",
    "mithilfe der 'isin'-Methode. Beachte, dass die vom 'HanoverTagger' verwendeten Tags minimal\n",
    "vom Standardtagset abweichen\"\"\"\n",
    "adjectives_df = niels_holgersen_df[niels_holgersen_df.POS.isin([\"ADJ(A)\", \"ADJ(D)\"])]\n",
    "\n",
    "\"\"\"Ausgabe der f√ºnf h√§ufigsten Lemmata im gefilterten DataFrame (denn Lemmata eignen sich besser \n",
    "als Wortformen zur Beantwortung der Fragestellung)\"\"\"\n",
    "adjectives_df.Lemma.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645cb62-60a2-4883-a1fe-dcb38746afdc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 3:** Reicher den Koalitionsvertrag von 2021 nicht nur mit morphologischen Tags, sondern auch mit Lemmata und POS-Tags an. W√§hl selbst, welchen Tagger Du dazu verwendest und informier Dich ggf. in der entsprechenden Dokumentation zu den Taggingm√∂glichkeiten. √úberf√ºhr die getaggten Daten abschlie√üend in ein DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221d3e5-3e5f-4552-8292-811024bc8313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Einlesen des Texts\n",
    "#Achtung: anderer Pfad als im Notebook, da das L√∂sungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/koalitionsvertrag_2021.txt\", encoding=\"utf8\") as f:\n",
    "    kv21 = f.read()\n",
    "\n",
    "\"\"\"Wir benutzen hier 'spacy', da es sich um einen langen Text handelt. Sollte jedoch die Tagqualit√§t \n",
    "entscheidend f√ºr Dich sein, lohnt es sich u. U. 'stanza' zu benutzen.\"\"\"\n",
    "import spacy\n",
    "\n",
    "spacy_tagger = spacy.load(\"de_core_news_sm\") #Initialisieren von 'spacy_tagger' mit dem deutschsprachigen Modell\n",
    "\n",
    "spacy_output = spacy_tagger(kv21) #Tagging\n",
    "\n",
    "#√úberf√ºhren des Outputs in ein DataFrame inkl. POS-Tags und Lemmata (Kommentare zur DataFrame-Erstellung s. Notebook)\n",
    "list_of_all_dicts = []\n",
    "\n",
    "for word in spacy_output:\n",
    "    dict_per_word = {\n",
    "        \"Wortform\": word.text,\n",
    "        \"Morphologie\": str(word.morph).strip(\"()\"), #Casting zwecks Bereinigung und um string-Objekt statt 'spacy'-Objekt zu √ºberf√ºhren\n",
    "        \"POS\": word.tag_, #Das 'tag_'-Attribut gibt ein feingliedrigeres POS-Tag zur√ºck, als das 'pos_'-Attribut\n",
    "        \"Lemma\": word.lemma_}\n",
    "    list_of_all_dicts.append(dict_per_word)\n",
    "    \n",
    "kv21_df = pd.DataFrame(list_of_all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967a5f1-681c-47e6-91e5-3858a0fe6234",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv21_df #Ausgabe des DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b02d7-254f-4674-b867-190db31ebc58",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 4:** Ermittle die 20 h√§ufigsten femininen Nomina im Koalitionsvertrag von 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b6d3e-dd4d-4c13-8c1c-8fbc4cdb417f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Der √úbersichtlichkeit zuliebe definieren wir die beiden ben√∂tigten Filter vorab und zwar in \n",
    "runde Klammern gesetzt, was bei Verwendung des '&'-Operators zwingend n√∂tig ist. Nur nach Genus\n",
    "zu filtern reicht nicht, da auch Artikel und Adjektive feminines Genus haben k√∂nnen. Nur nach\n",
    "Nomina zu filtern reicht nicht, da Nomina auch maskulines oder neutrales Genus haben k√∂nnen.\"\"\"\n",
    "filter_noun = (kv21_df.POS == \"NN\") #\"NOUN\" statt \"NN\", falls 'pos_'-Attribut oben benutzt wurde\n",
    "filter_gender = (kv21_df.Morphologie.str.contains(\"Gender=Fem\"))\n",
    "\n",
    "\"\"\"Anwenden der Filter auf 'kv21_df', Zugriff auf Spalte \"Lemma\", denn wir sind nicht an Wortformen\n",
    "interessiert, sondern an W√∂rtern in ihrer Grundform. Anschlie√üendes Ausz√§hlen der Werte in dieser Spalte\n",
    "und Ausgabe der obersten 20.\"\"\"\n",
    "kv21_df[filter_noun & filter_gender].Lemma.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4423ab9-3e59-42de-a9eb-75b7f16a2b9c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 5:** Find heraus, welche W√∂rter besonders h√§ufig als Subjekt im Koalitionsvertrag von 2021 stehen und lass Dir die zehn Spitzenreiter ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23afd1fb-7415-4d47-a3a1-0c64fe52a63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Koalitionsvertrag muss nicht noch einmal getaggt werden, da 'spacy' syntaktische Informationen standardm√§√üig mittaggt.\n",
    "Voraussetzung ist nat√ºrlich, dass 'spacy_output' noch im Arbeitsspeicher ist. Wir m√ºssen allerdings das DataFrame \n",
    "neu erstellen, um eine weitere Spalte zur Syntax zu erhalten ‚Äì Kommentare zur DataFrame-Erstellung s. o.\"\"\"\n",
    "list_of_all_dicts = []\n",
    "\n",
    "for word in spacy_output:\n",
    "    dict_per_word = {\n",
    "        \"Wortform\": word.text,\n",
    "        \"Morphologie\": str(word.morph).strip(\"()\"), \n",
    "        \"POS\": word.tag_, \n",
    "        \"Lemma\": word.lemma_,\n",
    "        \"Syntax\": word.dep_} #Neue Spalte\n",
    "    list_of_all_dicts.append(dict_per_word)\n",
    "    \n",
    "kv21_df_syntax = pd.DataFrame(list_of_all_dicts) \n",
    "\n",
    "#Filtern des DataFrame nach dem Wert \"sb\" in der Spalte \"Syntax\", Zugriff auf Spalte \"Lemma\", Ausz√§hlen der Werte und Ausgabe der obersten zehn \n",
    "kv21_df_syntax[kv21_df_syntax.Syntax == \"sb\"].Lemma.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1a61c-1d8a-40c6-9237-ff84c2175c11",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 6:** Lad Dir mithilfe der [API f√ºr Wikipedia](https://pypi.org/project/Wikipedia-API/) `wikipediaapi` (vgl. Zusatz√ºbungen zum Notebook \"Regul√§re Ausdr√ºcke\") den *englischsprachigen* Artikel zu einer prominenten Person Deiner Wahl herunter und extrahier alle darin erw√§hnten Personen mithilfe von `spacy` und `stanza`. Welche Unterschiede zeigen sich beim Vergleich der zehn h√§ufigsten Personen in den beiden Outputs? Verwend `pandas` f√ºr diesen Vergleich.\n",
    "\n",
    "Lass Dir au√üerdem s√§mtliche von `spacy` gefundenen Named Entities visualisieren (also nicht nur die Personen). Erkennst Du einen Unterschied im Vergleich zum NER-Tagging deutschsprachiger Texte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4b55e-f262-44c8-8d10-39b8d7e82ecf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Herunterladen des Artikels\n",
    "mail_address = \"\" #Trag hier Deine Mailadresse ein.\n",
    "\n",
    "import wikipediaapi #Kein Bindestrich beim Import\n",
    "\n",
    "name = \"Kamala Harris\" #Angabe der Person, deren Artikel heruntergeladen werden soll\n",
    "\n",
    "#Initialisieren der Schnittstelle mittels Angabe von 'user_agent', Sprache und Extraktionsformat \n",
    "#Wichtig: Spezifiziere die richtige Sprache im 'language'-Parameter\n",
    "Wiki_API = wikipediaapi.Wikipedia(user_agent=f\"Programmierenlernen, {mail_address}\", language=\"en\", extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "article = Wiki_API.page(name).text\n",
    "\n",
    "#Initialisieren eines Taggers mit 'spacy' f√ºr englischsprachige Texte\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\") #Download eines englischsprachigen Modells\n",
    "spacy_tagger_en = spacy.load(\"en_core_web_sm\") #Initialisieren von 'spacy_tagger_en' mit dem englischsprachigen Modell\n",
    "spacy_output = spacy_tagger_en(article) #Tagging\n",
    "\n",
    "#Initialisieren eines Taggers mit 'stanza' f√ºr englischsprachige Texte\n",
    "import stanza \n",
    "stanza_tagger_en = stanza.Pipeline(lang=\"en\", processors=\"tokenize,ner\") #Spezifikation von 'lang' als \"en\"\n",
    "stanza_output = stanza_tagger_en(article) #Tagging\n",
    "\n",
    "#Erstellen je einer 'pandas'-Series mit nur den als Person getaggten Entit√§ten mithilfe von List Comprehensions\n",
    "#Achtung: Das Tag f√ºr Personen ist bei beiden Taggern \"PERSON\" und nicht \"PER\" wie bei deutschsprachigen Texten.\n",
    "spacy_PER = pd.Series([person.text for person in spacy_output.ents if person.label_ == \"PERSON\"])\n",
    "stanza_PER = pd.Series([person.text for person in stanza_output.ents if person.type == \"PERSON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4998dcd-6313-4b38-ac83-993151d2ee66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe der h√§ufigsten zehn Personen gem√§√ü 'spacy' mithilfe von 'value_counts'\n",
    "spacy_PER.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625d826-ff38-4516-a7db-8b38f9be315c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe der h√§ufigsten zehn Personen gem√§√ü 'stanza' mithilfe von 'value_counts'\n",
    "stanza_PER.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129083c-a484-4e7a-9c35-5eff05025377",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Unterschied im Output der beiden Tagger: Beim Artikel von Kamala Harris zum Zeitpunkt des Verfassens dieses Notebooks zeigt sich, dass `stanza` zuverl√§ssiger zu taggen scheint. So taggt `spacy` etwa W√∂rter, die keine Personen bezeichnen. Bei anderen Artikeln oder anderen Versionen desselben Artikels mag dieses Bild jedoch anders aussehen.*\n",
    "\n",
    "*Es zeigt sich au√üerdem, dass bei beiden Taggern ein Normalisierungsschritt (z.&nbsp;B. Reduktion von \"Biden\" und \"Joe Biden\" auf einen Namen) nachgelagert werden m√ºsste, um die Ausz√§hlung der erw√§hnten Personen zu verbessern.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f9898-0f01-4b04-a3b8-89ded29345f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Visualisierung s√§mtlicher Tags im Output von 'spacy'\n",
    "from spacy import displacy\n",
    "displacy.render(spacy_output, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61356fa3-10e1-4c76-9f4d-c29bc95ef31b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Unterschied zwischen dem NER-Tagging deutschsprachiger und englischsprachiger Texte: Das Tagset f√ºr englischsprachige Texte umfasst weitaus mehr Kategorien, etwa f√ºr Daten (\"DATE\") oder Gesetze (\"LAW\").*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00896078-ab2b-49c4-bd83-5080fdfae014",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 7:** Find heraus, ob es sich beim Koalitionsvertrag von 2021 um einen positiven, neutralen oder negativen Text handelt. √úberleg Dir genau, was f√ºr Input `predict_sentiment` erwartet, d.&nbsp;h. wie Du den Koalitionsvertrag sinnvollerweise taggst.\n",
    "\n",
    "<details><summary>ü¶ä Herausforderung </summary>\n",
    "<br>Find zus√§tzlich heraus, bei welchen S√§tzen des Koalitionsvertrags ein negatives bzw. positives Sentiment mit einer Wahrscheinlichkeit von √ºber 50% getaggt wurde.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefb5ae-de04-46a0-b049-860e6f8add69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from germansentiment import SentimentModel\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_tagger = SentimentModel()\n",
    "\n",
    "#Einlesen des Texts\n",
    "#Achtung: anderer Pfad als im Notebook, da das L√∂sungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/koalitionsvertrag_2021.txt\", encoding=\"utf8\") as f:\n",
    "    kv21 = f.read()\n",
    "    \n",
    "sentences = sent_tokenize(kv21) #Aufsplitten in S√§tze mithilfe von 'nltk'\n",
    "\n",
    "\"\"\"Auskommentierte Reduktion von 'sentences' auf die ersten 100 S√§tze, um Code damit erst zu schreiben, um \n",
    "ihn anschlie√üend, wenn er fehlerfrei funktioniert, auf alle S√§tze anzuwenden (s. Tipp 2)\"\"\"\n",
    "#sentences = sentences[0:100] \n",
    "\n",
    "tags = [] #Leere Liste f√ºr Tags initialisieren\n",
    "\n",
    "#Erstellen einer Liste mit Tags und 'tdqm' zur Fortschrittsanzeige\n",
    "for sentence in tqdm(sentences):\n",
    "    \"\"\"√úbergabe von 'sentence' als Element einer Liste, Speichern des Tags zum ersten Satz (Index null), \n",
    "    da es ja nur einen Satz gibt. Analysier Input- und Outputformat von 'germansentiment', um dies zu verstehen.\"\"\"   \n",
    "    tags.append(sentiment_tagger.predict_sentiment([sentence])[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6eb1e3-9295-4718-8f11-ecfab84a9cd9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#H√§ufigstes Tag beantwortet die Fragestellung (zumindest in der Tendenz)\n",
    "pd.Series(tags).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0356f45-f035-4e60-b9f2-18240ccc66d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Herausforderung\n",
    "from germansentiment import SentimentModel\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_tagger = SentimentModel()\n",
    "\n",
    "#Einlesen des Texts\n",
    "#Achtung: anderer Pfad als im Notebook, da das L√∂sungsnotebook in einem anderen Verzeichnis liegt \n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/koalitionsvertrag_2021.txt\", encoding=\"utf8\") as f:\n",
    "    kv21 = f.read()\n",
    "    \n",
    "sentences = sent_tokenize(kv21) #Aufsplitten in S√§tze mithilfe von 'nltk'\n",
    "\n",
    "\"\"\"Auskommentierte Reduktion von 'sentences' auf die ersten 100 S√§tze, um Code damit erst zu schreiben, um \n",
    "ihn anschlie√üend, wenn er fehlerfrei funktioniert, auf alle S√§tze anzuwenden (s. Tipp 2)\"\"\"\n",
    "#sentences = sentences[0:100] \n",
    "\n",
    "#Wahrscheinlichkeiten je Tag werden auch ben√∂tigt, daher zweite Liste initialisieren...\n",
    "tags, probabilities = [], []\n",
    "\n",
    "#...und beim Taggen 'output_probabilities' auf 'True' setzen\n",
    "for sentence in tqdm(sentences):\n",
    "    tag, probability = sentiment_tagger.predict_sentiment([sentence], output_probabilities=True)\n",
    "    tags.append(tag[0]); probabilities.append(probability) #Befehle auf einer Zeile mithilfe von Semikolon\n",
    "\n",
    "#Kommentare zur DataFrame-Erstellung s. o.\n",
    "list_of_all_dicts = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    \n",
    "    \"\"\"Schreiben von jeweiligem Tag sowie relevanten Informationen aus dem jeweiligen dictionary auf 'probabilities'\n",
    "    in 'dict_per_sentence'. Das Outputformat der Wahrscheinlichkeiten ist wie gesagt stark verschachtelt, analysier\n",
    "    es genau, um die Indizierung der relevanten Werte zu verstehen.\"\"\"\n",
    "    dict_per_sentence = {\"sentence\": sentences[i],\n",
    "                         \"tag\": tags[i],\n",
    "                         \"positive_prob\": probabilities[i][0][0][1],\n",
    "                         \"negative_prob\": probabilities[i][0][1][1],\n",
    "                         \"neautral_prob\": probabilities[i][0][2][1]}\n",
    "    \n",
    "    list_of_all_dicts.append(dict_per_sentence)\n",
    "    \n",
    "kv21_sentiment = pd.DataFrame(list_of_all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a639340-0255-486d-ae70-f3d47ff3cce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe aller positiven Tags mit Wahrscheinlichkeit von √ºber 50%\n",
    "kv21_sentiment[kv21_sentiment.positive_prob > 0.5].sentence.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a532b-8dab-44e7-897a-8a3ba7c39d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe aller negativen Tags mit Wahrscheinlichkeit von √ºber 50%\n",
    "kv21_sentiment[kv21_sentiment.negative_prob > 0.5].sentence.values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c713960-b8f4-41cb-9522-8bb6371b6605",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Der Tagger beurteilt fast den gesamten Text als neutral formuliert. Neutrale Formulierungen k√∂nnten ein Charakteristikum von Koalitionsvertr√§gen sein. Wahrscheinlicher ist jedoch, dass dieser Tagger bei dieser Art von Daten nicht zufriedenstellend funktioniert, zumal das Textgenre durchaus positive S√§tze vermuten l√§sst.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bdbaf-fcda-4088-99a0-e5fcf67de414",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "üîß **Anwendungsfall:** \n",
    "\n",
    "1. Vorgelagerter Abruf- und Extraktionsschritt f√ºr den Musteranwendungsfall (Vergleich von Informationsseiten √ºber Steuern des Bundesfinanzministeriums in Standard- und Leichter Sprache hinsichtlich der Verteilung von Wortarten):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d541e90-ef0e-4ba9-8cb7-792a50a78cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "\n",
    "simple_language = \"https://www.bundesfinanzministerium.de/Content/DE/Standardartikel/Service/Leichte_Sprache/steuern.html\"\n",
    "standard_language = \"https://www.bundesfinanzministerium.de/Content/DE/Standardartikel/Themen/Steuern/steuern.html\"\n",
    "\n",
    "#Abruf der Quelltexte zu den beiden Sprachversionen zur Seite √ºber Steuern\n",
    "simple_source_code = requests.get(simple_language, timeout=5, headers=headers).text\n",
    "standard_source_code = requests.get(standard_language, timeout=5).text\n",
    "\n",
    "#Alternativ: Einlesen aus dem Ordner \"3_Dateien/Tagging\", falls Seitenzugriff blockiert wird (zeigt sich an Fehlermeldungen unten)\n",
    "#with open(\"../../3_Dateien/Tagging/simple_language.html\", encoding=\"utf8\") as f, open(\"../../3_Dateien/Tagging/standard_language.html\", encoding=\"utf8\") as g:\n",
    "    #simple_source_code, standard_source_code = f.read(), g.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4487a-0a1d-4a12-ad4a-e6269ec1a42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Definition einer Funktion, die relevante Textteile extrahiert und extern speichert;\n",
    "F√ºr zwei Texte ist die Schaffung einer Funktion nat√ºrlich nicht n√∂tig. Ist sie aber einmal \n",
    "geschrieben, kann sie jederzeit auf neue Texte angewendet werden.\"\"\"\n",
    "def extract_and_save_text(source_code, file_name):\n",
    "\n",
    "    #Extraktion des kleinsten HTML-Elements, das die relevanten Textteile beinhaltet\n",
    "    main_content = BeautifulSoup(source_code).find(class_=\"article-wrapper documentleaf basepage\")\n",
    "    \n",
    "    #Speichern aller bereinigten Textparagraphen in externer Datei\n",
    "    with open(f\"../../3_Dateien/Output/{file_name}_text_taxes.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\" \".join([p.text.strip() for p in main_content.find_all(\"p\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ca528-3ea9-4704-a88f-13cb076830cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Aufrufen der Funktion\n",
    "extract_and_save_text(simple_source_code, \"simple\")\n",
    "extract_and_save_text(standard_source_code, \"standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82bf0c-37bc-44ac-8e64-eee3c795e5a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. Tagging am Beispiel des Musteranwendungsfall, inkl. Speicherung der getaggten Daten in externer Datei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97479d77-98d6-49bf-8a21-0a08a35521e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "ht_tagger = ht.HanoverTagger('morphmodel_ger.pgz') \n",
    "\n",
    "#Definition einer Funktion zum Taggen von Texten.\n",
    "def tag_text(input_path, output_path):\n",
    "    \n",
    "    with open(input_path, encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    words = word_tokenize(text) #Tokenisieren\n",
    "\n",
    "    text_tagged = ht_tagger.tag_sent(words) #Tagging\n",
    "    \n",
    "    \"\"\"√úberf√ºhren in DataFrame (csv-Dateien k√∂nnen auch ohne den Umweg √ºber 'pandas' extern gespeichert werden\n",
    "    [vgl. Notebook \"Input und Output Teil 2\"], wie h√§ufig bietet 'pandas' aber einen unkomplizierten Ansatz)\"\"\"\n",
    "    df_tagged = pd.DataFrame(text_tagged, columns=['Wortform', 'Lemma', 'POS']) \n",
    "    \n",
    "    #Speichern als externe Datei\n",
    "    df_tagged.to_csv(output_path, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d700f-b2ec-4530-95f4-95f5ce016d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Aufrufen der Funktion\n",
    "tag_text(\"../../3_Dateien/Output/simple_text_taxes.txt\", \"../../3_Dateien/Output/simple_text_taxes_output.csv\")\n",
    "tag_text(\"../../3_Dateien/Output/standard_text_taxes.txt\", \"../../3_Dateien/Output/standard_text_taxes_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8af9ba-1ec3-4da8-af4b-a9c8506475ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Visualisieren der H√§ufigkeitsverteilung von Wortarten mithilfe von S√§ulendiagramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b077d-53f8-47bb-8406-5a31dc53bf42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Einlesen der extern gepeicherten, getaggten Daten zur Weiterverarbeitung mit 'pandas'\n",
    "simple_df = pd.read_csv(\"../../3_Dateien/Output/simple_text_taxes_output.csv\", encoding=\"utf8\")\n",
    "standard_df = pd.read_csv(\"../../3_Dateien/Output/standard_text_taxes_output.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f87322-1d7f-43f5-8ce1-92b8cc268af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Konfigurieren von Plotgr√∂√üe sowie Stil (beides optional, versch√∂nert aber das Ergebnis)\n",
    "plt.figure(figsize=(12, 8)) \n",
    "plt.style.use(\"bmh\") #√Ñndere zwischen \"classic\", \"classic\", \"bmh\", \"tableau-colorblind10\", uvm. \n",
    "\n",
    "#Erstellen von Series mit den ausgez√§hlten Werten in der Spalte \"POS\", normalisiert als relative Werte\n",
    "pos_frequencies_simple = simple_df.POS.value_counts(normalize=True)\n",
    "pos_frequencies_standard = standard_df.POS.value_counts(normalize=True)\n",
    "\n",
    "#Erstellen einer sortierten Liste aller einzigartigen POS-Tags, die in einem oder beiden Texten vorkommen (senkrechter Strich bedeutet \"or\")\n",
    "pos_tags = sorted(set(simple_df.POS.unique()) | set(standard_df.POS.unique()))\n",
    "\n",
    "bar_width = 0.4 #Definieren der S√§ulenbreite\n",
    "\n",
    "#Definieren von Koordinaten auf der x-Achse, an denen die einzelnen S√§ulen pro POS-Tag geplottet werden sollen\n",
    "x = np.arange(len(pos_tags))\n",
    "\n",
    "\"\"\"Plotten von je zwei S√§ulen pro POS-Tag, eine f√ºr die relative H√§ufigkeit des gegebenen Tags im Text\n",
    "in Leichter Sprache und eine f√ºr diejenige im Text in Standardsprache. √úbergeben werden der 'bar'-Funktion\n",
    "als erstes Argument 'x', das eine Arte Liste mit den Koordinaten auf der x-Achse enth√§lt (genau so viele, \n",
    "wie es Tags zu plotten gibt). Die S√§ule f√ºr Leichte Sprache wird um die halbe 'bar_width' nach links verschoben\n",
    "geplottet, die S√§ule f√ºr Standardsprache um die halbe 'bar_width' nach rechts verschoben. So werden die beiden\n",
    "S√§ulen direkt nebeneinander, aber nicht √ºbereinader geplottet. Als zweites Argument werden die Koordinaten f√ºr\n",
    "die y-Achse in Form der oben erstellten Series mit relativen H√§ufigkeiten pro POS-Tag √ºbergeben ('reindex(pos_tags)' \n",
    "sorgt daf√ºr, dass die beiden Series gleich sortiert werden). Weiter werden die S√§ulenbreite und Labels definiert.\"\"\"\n",
    "plt.bar(x - bar_width/2, pos_frequencies_simple.reindex(pos_tags), width=bar_width, label=\"Leichte Sprache\")\n",
    "plt.bar(x + bar_width/2, pos_frequencies_standard.reindex(pos_tags), width=bar_width, label=\"Standardsprache\")\n",
    "\n",
    "#Abschlie√üend werden die Achsen sowie der Plot beschriftet und die POS-Tags um 90 Grad rotiert sowie die Legende eingeblendet\n",
    "plt.xlabel(\"POS-Tags\")\n",
    "plt.ylabel(\"Relative H√§ufigkeit\")\n",
    "plt.title(\"H√§ufigkeitsverteilung von POS-Tags nach Sprachschwierigkeit\\n\", fontweight=\"bold\")\n",
    "plt.xticks(x, pos_tags, rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9baf6-8138-4327-889d-6c90c26b20cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Es zeigt sich u.&nbsp;a., dass in den beiden untersuchten Texten in Leichter Sprache im Vergleich zu Standardsprache viel weniger Artikel (\"ART\") und Pr√§positionen (\"APPR\"), dagegen wesentlich mehr finite Verben (\"VV(FIN)\", \"VM(FIN)\") und Infinitive (\"VV(INF)\") verwendet werden. Um Aussagen √ºber die beiden Sprachschwierigkeitsstufen treffen zu k√∂nnen, m√ºsste die Datenbasis nat√ºrlich erweitert werden.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46479c9c-c118-4379-bbb3-4493b4da4c25",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "<table>\n",
    "      <tr>\n",
    "        <td>\n",
    "            <img src=\"../../3_Dateien/Lizenz/CC-BY-SA.png\" width=\"400\">\n",
    "        </td> \n",
    "        <td>\n",
    "            <p>Dieses Notebook sowie s√§mtliche weiteren <a href=\"https://github.com/yannickfrommherz/exdimed-student/tree/main\">Materialien zum Programmierenlernen f√ºr Geistes- und Sozialwissenschaftler:innen</a> sind im Rahmen des Projekts <i>Experimentierraum Digitale Medienkompetenz</i> als Teil von <a href=\"https://tu-dresden.de/gsw/virtuos/\">virTUos</a> entstanden. Erstellt wurden sie von Yannick Frommherz unter Mitarbeit von Anne Josephine Matz. Sie stehen als Open Educational Resource nach <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY SA</a> zur freien Verf√ºgung. F√ºr Feedback und bei Fragen nutz bitte das <a href=\"https://forms.gle/VsYJgy4bZTSqKioA7\">Kontaktformular</a>.\n",
    "        </td>\n",
    "      </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}