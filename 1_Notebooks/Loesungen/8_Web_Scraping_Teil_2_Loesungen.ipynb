{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9b2abe-9dbc-4b0c-bd97-088470e5eca4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Web Scraping Teil 2 (L√∂sungen)\n",
    "\n",
    "‚òùÔ∏è Beachte: Es gibt beim Programmieren fast immer verschiedene L√∂sungswege. Deine L√∂sung mag anders aussehen, aber dennoch zum gew√ºnschten Resultat f√ºhren. Das richtige Resultat ist das Wichtigste. \n",
    "\n",
    "‚ö†Ô∏è F√ºhr folgenden Code aus, bevor Du einzelne L√∂sungen ausf√ºhrst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001fff4-aee5-4103-a632-55331948d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "import xml.etree.ElementTree as ET "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723de51-a9a8-4574-aad4-4c797c193b19",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "üîß **Anwendungsfall (komplette L√∂sung):** \n",
    "\n",
    "Abrufschritt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac797b-7ecb-4e39-b4fb-467427736c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "#Definieren eines regul√§ren Ausdrucks, um den Link zur jeweils n√§chsten Seite zu extrahieren\n",
    "regex = r'href=\"(\\S+)\">weiter' #Runde Klammern umschlie√üen Gruppe mit eigentlichem Link\n",
    "\n",
    "#Definieren von Stammlink, erster Linkendung sowie des kompletten Links zur ersten zu scrapenden Seite\n",
    "base_link = \"https://www.projekt-gutenberg.org/goethe/faust1/\"\n",
    "link_ending = \"chap002.html\"\n",
    "link = base_link + link_ending\n",
    "\n",
    "all_pages = [] #Initialisieren einer leeren Liste, an die unten die einzelnen Quelltexte geh√§ngt werden\n",
    "\n",
    "\"\"\"'while'-Schleife scrapt eine paginierte Seite nach der anderen, bis kein Link mehr auf eine n√§chste Seite\n",
    "im Quelltext gefunden wird\"\"\"\n",
    "while True:\n",
    "    \n",
    "    print(f\"Aktuell wird gescrapt: {link}\") #Ausgabe des Fortschritts\n",
    "    \n",
    "    current_page = requests.get(link, timeout=5, headers=headers) #Abruf des Quelltexts zum aktuellen Link\n",
    "    \n",
    "    #Kontrolle des Statuscodes\n",
    "    if not current_page.status_code == requests.codes.ok:\n",
    "        print(\"Statuscode nicht ok!\")\n",
    "        break\n",
    "    \n",
    "    \"\"\"Definieren des Encodings des Quelltexts (requests geht unspezifiziert vom falschen Encoding aus, \n",
    "    was sich z. B. an Umlauten zeigt)\"\"\"\n",
    "    current_page.encoding = \"UTF-8\" \n",
    "    \n",
    "    #Anf√ºgen des eigentlichen Quelltexts der aktuellen Seite (Zugriff √ºber text-Attribut) an 'all_pages'\n",
    "    all_pages.append(current_page.text) \n",
    "    \n",
    "    #Quelltext nach regul√§rem Ausdruck absuchen, der Link zur n√§chsten Seite matcht\n",
    "    next_page = re.search(regex, current_page.text)\n",
    "    \n",
    "    #Wenn ein match gefunden wird...\n",
    "    if next_page:\n",
    "        \"\"\"Definieren des Links f√ºr die n√§chste Seite, indem wir mithilfe der \n",
    "        'group'-Methode auf die erste Gruppe zugreifen und die neue Linkendung an den Stammlink h√§ngen\"\"\"\n",
    "        link = base_link + next_page.group(1)\n",
    "        time.sleep(5) #Erst noch eine kleine Pause!\n",
    "    else: #...wenn nicht, ist das Werk komplett gescrapet und die 'while'-Schleife wird abgebrochen.\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c3e4f-f2b4-4d15-a39a-6f8870679984",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Extraktionsschritt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929fbdae-bf25-4375-984f-5e79a92e49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#√ñffnen der Datei, in die die Strophen geschrieben werden sollen\n",
    "with open(\"../../3_Dateien/Output/Faust_1.txt\" , \"w\") as write_file: \n",
    "    \n",
    "    for page in all_pages: #Iteration √ºber 'all_pages'\n",
    " \n",
    "        soup = BeautifulSoup(page, \"lxml\") #Konstruieren eines BeautifulSoup-Objekts\n",
    "        body = soup.find(\"body\") #Zugriff auf das body-Element √ºber 'find'-Methode\n",
    "        \n",
    "        \"\"\"Iteration √ºber Liste mit allen Elementen mit Tag <p>, die, wie ein Blick in die Quelltexte ergab,\n",
    "        die Strophen und Figuren (nebst weiteren Inhalten) enth√§lt\"\"\"\n",
    "        for paragraph in body.find_all(\"p\"): \n",
    "            \"\"\"Nun m√ºssen wir diejenigen Elemente aus allen <p>-Elementen extrahieren, die die Figur bzw. \n",
    "            die Strophen enthalten.\"\"\"\n",
    "            \n",
    "            \"\"\"Vor jeder Strophe steht die Figur, die spricht, und zwar in einem dem <p>-Element\n",
    "            untergeordneten Element mit <span>-Tag und Attribut class=\"speaker\". Wir √ºberpr√ºfen mittels \n",
    "            'if'-Bedingung, ob die 'find'-Methode ein solches Element findet, wenn ja...\"\"\"\n",
    "            if paragraph.find(\"span\", class_=\"speaker\"):\n",
    "                \"\"\"...extrahieren wir den darin enthaltenen Text... (Zugriff √ºber 'text'-Attribut w√ºrde Fehlermeldung\n",
    "                hervorrufen, wenn 'find' kein entsprechendes Element finden w√ºrde, daher if-Bedingung)\"\"\"\n",
    "                speaker = paragraph.find(\"span\", class_=\"speaker\").text\n",
    "                \"\"\"...und schreiben ihn von inkonsistent verwendeten Doppelpunkten bereinigt \n",
    "                und mit abschlie√üenden Zeilenumbr√ºchen in die Textdatei\"\"\"\n",
    "                write_file.write(speaker.strip(\":\") + \"\\n\\n\") \n",
    "           \n",
    "            elif paragraph.get(\"class\") == [\"vers\"]:  #Da <p>-Elemente nichts weiter enthalten, wenn sie die Figur enthalten, \n",
    "                                                      #k√∂nnen wir mittels 'elif' √ºberpr√ºfen, ob das Attribut \"class\" [\"vers\"] \n",
    "                                                      #entspricht, denn in <p>-Elementen mit diesem Attribut sind \n",
    "                                                      #die Strophen enthalten\n",
    "                \n",
    "                \"\"\"Stropheninterne Regieanweisungen befinden sich in einem <span>-Element, dessen Existenz\n",
    "                im aktuelle <p>-Element wir hier √ºberpr√ºfen\"\"\"\n",
    "                if paragraph.span:\n",
    "                    paragraph.span.decompose() #Wenn ja, l√∂schen wir es aus 'paragraph' mittels der 'decompose'-Methode\n",
    "            \n",
    "                \"\"\"Da Einr√ºckungen und whitespace generell in den Strophen inkonsistent verwendet werden,\n",
    "                splitten wir die Strophen in Verse, um im 'write'-Befehl unten einheitlich formatieren zu k√∂nnen.\"\"\"\n",
    "                lines = paragraph.text.split(\"\\n\")\n",
    "                \n",
    "                for line in lines: #Iteration √ºber die einzelnen Verse...\n",
    "                    write_file.write(\"\\t\" + line.strip() + \"\\n\") #...und schreiben in Textdokument (bereinigt und konsistent formatiert)\n",
    "                \n",
    "                write_file.write(\"\\n\") #Schreiben eines weiteren Zeilenumbruchs nach jeder Strophe (= zwei Zeilenumbr√ºche insgesamt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182326c3-7ded-4931-ae26-6d7ff9946d1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "üîß **Anwendungsfall (Schritt-f√ºr-Schritt-L√∂sung):**\n",
    "\n",
    "<u>Abruf</u>\n",
    "\n",
    "1. Schau Dir die [erste Seite](https://www.projekt-gutenberg.org/goethe/faust1/chap002.html) des Werks auf www.projekt-gutenberg.org im Browser an und untersuch den ihr zugrundeliegenden Quelltext. Find so heraus, wie Du an die Links zu den folgenden Seiten kommst, damit Du auch diese scrapen kannst.\n",
    "\n",
    "    *L√∂sung: Auf der ersten Seite, ebenso wie auf allen folgenden bis zur letzten, befindet sich eine Schaltfl√§che \"Weiter\". Im Quelltext siehst Du, dass sich dahinter der Link (bzw. die Linkendung) zur jeweils n√§chsten Seite verbirgt. Wir k√∂nnen folglich 1) die erste Seite scrapen, 2) deren Quelltext speichern, 3) darin nach dem Link zur n√§chsten Seite suchen, 4) die n√§chste Seite mithilfe des neuen Links ebenfalls scrapen und speichern usw., bis es keinen n√§chsten Link mehr gibt.*\n",
    "\n",
    "    *Es gibt zwei weitere Alternativen: Erstens gibt es auf [dieser Seite](https://www.projekt-gutenberg.org/goethe/faust1/index.html) ein Inhaltsverzeichnis mit Links zu allen Szenen des Werks. Wir k√∂nnten also auch 1) diese Seite scrapen, 2) alle relevanten Links aus deren Quelltext extrahieren und in eine Liste √ºberf√ºhren, 3) √ºber die Liste iterieren und alle entsprechenden Quelltexte scrapen. Zweitens k√∂nnten wir uns den Umstand zu Nutze machen, dass die Seiten von zwei bis 28 durchnummeriert sind (vgl. Linkendungen). Mit der `range`-Funktion k√∂nnten wir einfach von zwei bis 28 durchiterieren und \"on the fly\" jeweils einen Link daraus basteln.*\n",
    "\n",
    "    *In der Schritt-f√ºr-Schritt-L√∂sung verfolgen wir den ersten der drei Ans√§tze, da er sich am ehesten auf vergleichbare Anwendungsf√§lle mit paginierten Seiten √ºbertragen l√§sst.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49246856-63e2-45a3-86ba-ab1d690f8f22",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. Wie Du im Quelltext der ersten Seite erkennen kannst, verbirgt sich hinter der Schaltfl√§che \"Weiter\" nur jeweils die Linkendung, die um einen Stammlink erg√§nzt werden muss. \n",
    "    \n",
    "    Definier den Stammlink in `base_link` sowie die Linkendung f√ºr die erste Seite in `link_ending`. Konkatenier die beiden strings zu `link` und lass Dir `link` ausgeben. Wenn Du auf den ausgegebenen Link klickst, solltest Du auf der ersten Seite des Werks landen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3bb80-6710-4859-9331-9462f385b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definieren von Stammlink, erster Linkendung sowie des kompletten Links zur ersten zu scrapenden Seite\n",
    "base_link = \"https://www.projekt-gutenberg.org/goethe/faust1/\"\n",
    "link_ending = \"chap002.html\"\n",
    "link = base_link + link_ending\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6501d20-369e-4297-aff1-1edb6df7cc0d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. √úberleg Dir nun, mithilfe welcher Kontrollstruktur Du ausgehend von der ersten Seite nacheinander alle Seiten scrapen kannst, bis es keine weitere Seite mehr gibt. Welche weitere Kontrollstruktur kannst Du verwenden, um das Scraping in diesem Fall zu beenden? \n",
    "\n",
    "    *L√∂sung: Wir scrapen eine Seite nach der anderen mithilfe einer `while`-Schleife, konkret: `while True`. So bricht diese Schleife erst ab, wenn sie auf ein `break`-Statement trifft. Letzteres r√ºcken wir unter der zweiten Kontrollstruktur, einer `if`-Bedingung, ein, die nur dann `True` ergeben soll, wenn der zuletzt gescrapte Quelltext **keinen** Link auf eine n√§chste Seite mehr enth√§lt (also wenn wir auf der letzten Seite angelangt sind).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3d65e-c923-451a-be59-047cb274bca5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "4. Um von einer Seite zur n√§chsten zu gelangen, m√ºssen wir ja jeweils den Link darauf aus der aktuellen Seite extrahieren. Definier daf√ºr einen regul√§ren Ausdruck, der den Link hinter der Schaltfl√§che \"Weiter\" matcht. Falls Du noch nicht mit regul√§ren Ausdr√ºcken vertraut bist, dann kopier den Code f√ºr diesen Schritt aus der L√∂sung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e5364-4926-4265-afb7-0c3b19ed8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Definieren eines regul√§ren Ausdrucks, um den Link zur jeweils n√§chsten Seite zu extrahieren\n",
    "regex = r'href=\"(\\S+)\">weiter' #Runde Klammern umschlie√üen Gruppe mit eigentlichem Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05efbe1-3284-41fd-ac31-ebe2e8f30df7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "5. Setz nun Deine Erkenntnisse aus Schritt 3 in Code um: \n",
    "    \n",
    "    A. Schreib eine Schleife (erste Kontrollstruktur), die wiederholt wird, bis sie auf ein `break`-Statement trifft. \n",
    "    \n",
    "    B. Verwend das `requests`-Moduls, um die Seite zum aktuellen `link` abzurufen (`link` entspricht ja zumindest am Anfang der ersten Seite, s.&nbsp;o.). Speichere das Response-Objekt in `current_page`.\n",
    "    \n",
    "    C. Da `requests` automatisch von einem falschen Encoding ausgeht (weswegen etwa Umlaute falsch dekodiert w√ºrden), m√ºssen wir das Encoding korrigieren: `current_page.encoding = \"UTF-8\"`.\n",
    "    \n",
    "    D. H√§ng den eigentlichen Quelltext in `current_page` einer zuvor definierten Liste `all_pages` an, die s√§mtliche Quelltexte umfassen soll.\n",
    "    \n",
    "    E. Such den Quelltext nach `regex` ab. Verwend dazu die Funktion `search` des Moduls `re` (schau auch hier in der L√∂sung nach, wenn Du regul√§ren Ausdr√ºcken noch nicht vertraut bist).\n",
    "   \n",
    "    F. √úberpr√ºf nun mithilfe der zweiten Kontrollstruktur, ob ein match vorliegt. Wenn ja, √ºberschreib `link`, indem Du den match, also die neue Linkendung, an `base_link` anh√§ngst. Nun kann die n√§chste Seite gescrapt werden. Liegt kein match (mehr) vor, soll die Schleife abgebrochen werden.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7c6d5-9e24-4996-94eb-2070fde2d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "all_pages = [] #Initialisieren einer leeren Liste, an die unten die einzelnen Quelltexte geh√§ngt werden\n",
    "\n",
    "\"\"\"'while'-Schleife scrapt eine paginierte Seite nach der anderen, bis kein Link mehr auf eine n√§chste Seite\n",
    "im Quelltext gefunden wird\"\"\"\n",
    "while True:\n",
    "    \n",
    "    print(f\"Aktuell wird gescrapt: {link}\") #Ausgabe des Fortschritts\n",
    "    \n",
    "    current_page = requests.get(link, timeout=5, headers=headers) #Abruf des Quelltexts zum aktuellen Link\n",
    "    \n",
    "    #Kontrolle des Statuscodes\n",
    "    if not current_page.status_code == requests.codes.ok:\n",
    "        print(\"Statuscode nicht ok!\")\n",
    "        break\n",
    "    \n",
    "    \"\"\"Definieren des Encodings des Quelltexts (requests geht unspezifiziert vom falschen Encoding aus, \n",
    "    was sich z. B. an Umlauten zeigt)\"\"\"\n",
    "    current_page.encoding = \"UTF-8\" \n",
    "    \n",
    "    #Anf√ºgen des eigentlichen Quelltexts der aktuellen Seite (Zugriff √ºber text-Attribut) an 'all_pages'\n",
    "    all_pages.append(current_page.text) \n",
    "    \n",
    "    #Quelltext nach regul√§rem Ausdruck absuchen, der Link zur n√§chsten Seite matcht\n",
    "    next_page = re.search(regex, current_page.text)\n",
    "    \n",
    "    #Wenn ein match gefunden wird...\n",
    "    if next_page:\n",
    "        \"\"\"Definieren des Links f√ºr die n√§chste Seite, indem wir mithilfe der \n",
    "        'group'-Methode auf die erste Gruppe zugreifen und die neue Linkendung an den Stammlink h√§ngen\"\"\"\n",
    "        link = base_link + next_page.group(1)\n",
    "        time.sleep(5) #Erst noch eine kleine Pause!\n",
    "    else: #...wenn nicht, ist das Werk komplett gescrapet und die 'while'-Schleife wird abgebrochen.\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebefe201-5219-4e9e-ae42-3073ba2fd074",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<u>Extraktion</u>\n",
    "\n",
    "6. Nun haben wir alle Quelltexte gescrapt und es geht ans Extrahieren der relevanten Daten. Wir schreiben zun√§chst Code, um alle Strophen sowie die Figuren aus einem *einzelnen* Quelltext zu extrahieren. Anschlie√üend bauen wir diesen Code in eine Schleife, der √ºber *s√§mtliche* Quelltexte iteriert und nach und nach das gesamte Werk extrahiert.\n",
    "\n",
    "    Analysiere als Erstes die Quelltexte eingehend, entweder im Browser, in Sublime Text oder indem Du sie Dir mithilfe von `prettify` von BeautifulSoup ausgeben l√§sst. Welche Elemente mit welchen Tags und ggf. welchen Attributen beinhalten die Strophen und Figuren?\n",
    "    \n",
    "    *L√∂sung: Sowohl Strophen als auch Figuren sind in Elementen mit Tag `<p>` enthalten. Figuren sind in den `<p>`-Elementen **untergeordneten** `<span>`-Elementen mit Attribut `class=\"speaker\"` enthalten. Die Strophen befinden sich in `<p>`-Elementen mit dem Attribut `class=\"vers\"`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58694d-94ef-45cb-af13-bf578620a8b5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "7. Verwend BeautifulSoup, um den ersten Quelltext zu parsen. Schaffe ein Objekt `body`, das nur noch das `<body>`-Element beinh√§lt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ea5b3-30ce-4adc-bbb8-c2372b4c4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = all_pages[0] #Definieren des ersten Quelltexts als 'page'\n",
    "\n",
    "soup = BeautifulSoup(page, \"lxml\") #Konstruieren eines BeautifulSoup-Objekts\n",
    "body = soup.find(\"body\") #Zugriff auf das body-Element √ºber 'find'-Methode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c309b-dff7-40ae-ac5e-49df7f693127",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "8. Iterier mithilfe von `find_all` √ºber alle Elemente desjenigen Tags, das sowohl Strophen als auch Figuren umfasst. \n",
    "\n",
    "    √úberpr√ºf f√ºr jedes Element erstens, ob sich darin untergeordnet dasjenige Element befindet, das die Figuren enth√§lt. Wenn ja, extrahier es und weis es `speaker` zu. Bereinige `speaker` von √ºberfl√º√üigen Zeichen und lass es Dir ausgeben.\n",
    "\n",
    "    √úberpr√ºf zweitens, ob das Element andernfalls √ºber dasjenige Attribut verf√ºgt, das s√§mtliche Elemente, die Strophen beinhalten, miteinander teilen. Wenn ja, extrahier es, unterteil es in Verse und weis diese `lines` zu. Lass Dir `lines` sch√∂n formatiert ausgeben\n",
    "    \n",
    "    ‚ö†Ô∏è Achtung: Manche Strophen beinhalten auch Regienanweisungen in einem untergeordneten `<span>`-Element. Bau folgenden Code an der richtigen Stelle ein, um diese `<span>`-Elemente aus den Strophen zu entfernen:\n",
    "    \n",
    "    `if line.span:`\n",
    "     <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`line.span.decompose()`\n",
    "    \n",
    "    Wir √ºberpr√ºfen damit erst, ob die entsprechende Strophe ein `<span>`-Element enth√§lt und wenn ja, entfernen wir es aus ihr mithilfe der `decompose`-Methode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb719a-c4b6-46d7-b6b0-62101d5cd86f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Iteration √ºber Liste mit allen Elementen mit Tag <p>, die, wie ein Blick in die Quelltexte ergab,\n",
    "die Strophen und Figuren (nebst weiteren Inhalten) enth√§lt\"\"\"\n",
    "for paragraph in body.find_all(\"p\"): \n",
    "    \"\"\"Nun m√ºssen wir diejenigen Elemente aus allen <p>-Elementen extrahieren, die die Figur bzw. \n",
    "    die Strophen enthalten.\"\"\"\n",
    "\n",
    "    \"\"\"Vor jeder Strophe steht die Figur, die spricht, und zwar in einem dem <p>-Element\n",
    "    untergeordneten Element mit <span>-Tag und Attribut class=\"speaker\". Wir √ºberpr√ºfen mittels \n",
    "    'if'-Bedingung, ob die 'find'-Methode ein solches Element findet, wenn ja...\"\"\"\n",
    "    if paragraph.find(\"span\", class_=\"speaker\"):\n",
    "        \"\"\"...extrahieren wir den darin enthaltenen Text... (Zugriff √ºber 'text'-Attribut w√ºrde Fehlermeldung\n",
    "        hervorrufen, wenn 'find' kein entsprechendes Element finden w√ºrde, daher if-Bedingung)\"\"\"\n",
    "        speaker = paragraph.find(\"span\", class_=\"speaker\").text\n",
    "        \"\"\"...und schreiben ihn von inkonsistent verwendeten Doppelpunkten bereinigt \n",
    "        und mit abschlie√üenden Zeilenumbr√ºchen in die Textdatei\"\"\"\n",
    "        print(speaker.strip(\":\") + \"\\n\") \n",
    "\n",
    "    elif paragraph.get(\"class\") == [\"vers\"]:  #Da <p>-Elemente nichts weiter enthalten, wenn sie die Figur enthalten, \n",
    "                                              #k√∂nnen wir mittels 'elif' √ºberpr√ºfen, ob das Attribut \"class\" [\"vers\"] \n",
    "                                              #entspricht, denn in <p>-Elementen mit diesem Attribut sind \n",
    "                                              #die Strophen enthalten\n",
    "\n",
    "        \"\"\"Stropheninterne Regieanweisungen befinden sich in einem <span>-Element, dessen Existenz\n",
    "        im aktuelle <p>-Element wir hier √ºberpr√ºfen\"\"\"\n",
    "        if paragraph.span:\n",
    "            paragraph.span.decompose() #Wenn ja, l√∂schen wir es aus 'paragraph' mittels der 'decompose'-Methode\n",
    "\n",
    "        \"\"\"Da Einr√ºckungen und whitespace generell in den Strophen inkonsistent verwendet werden,\n",
    "        splitten wir die Strophen in Verse, um im 'write'-Befehl unten einheitlich formatieren zu k√∂nnen.\"\"\"\n",
    "        lines = paragraph.text.split(\"\\n\")\n",
    "        \n",
    "        for line in lines: #Iteration √ºber die einzelnen Verse...\n",
    "            print(\"\\t\" + line.strip())\n",
    "        \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ae3d8-704d-4e8b-b646-d561fc710948",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "9. Modifizier den Code aus Schritt 8 derart, dass Du die relevanten Daten nicht nur aus *einem* Quelltext extrahierst, sondern aus *allen* auf `all_pages`. Pass ihn au√üerdem so an, dass Dir Figuren und Strophen nicht ausgegeben werden, sondern dass diese in eine externe Datei geschrieben werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b8463-8fe7-4783-b9b7-51813900b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#√ñffnen der Datei, in die die Strophen geschrieben werden sollen\n",
    "with open(\"../../3_Dateien/Output/Faust_1.txt\" , \"w\") as write_file: \n",
    "    \n",
    "    for page in all_pages: #Iteration √ºber 'all_pages'\n",
    " \n",
    "        soup = BeautifulSoup(page, \"lxml\") #Konstruieren eines BeautifulSoup-Objekts\n",
    "        body = soup.find(\"body\") #Zugriff auf das body-Element √ºber 'find'-Methode\n",
    "        \n",
    "        \"\"\"Iteration √ºber Liste mit allen Elementen mit Tag <p>, die, wie ein Blick in die Quelltexte ergab,\n",
    "        die Strophen und Figuren (nebst weiteren Inhalten) enth√§lt\"\"\"\n",
    "        for paragraph in body.find_all(\"p\"): \n",
    "            \"\"\"Nun m√ºssen wir diejenigen Elemente aus allen <p>-Elementen extrahieren, die die Figur bzw. \n",
    "            die Strophen enthalten.\"\"\"\n",
    "            \n",
    "            \"\"\"Vor jeder Strophe steht die Figur, die spricht, und zwar in einem dem <p>-Element\n",
    "            untergeordneten Element mit <span>-Tag und Attribut class=\"speaker\". Wir √ºberpr√ºfen mittels \n",
    "            'if'-Bedingung, ob die 'find'-Methode ein solches Element findet, wenn ja...\"\"\"\n",
    "            if paragraph.find(\"span\", class_=\"speaker\"):\n",
    "                \"\"\"...extrahieren wir den darin enthaltenen Text... (Zugriff √ºber 'text'-Attribut w√ºrde Fehlermeldung\n",
    "                hervorrufen, wenn 'find' kein entsprechendes Element finden w√ºrde, daher if-Bedingung)\"\"\"\n",
    "                speaker = paragraph.find(\"span\", class_=\"speaker\").text\n",
    "                \"\"\"...und schreiben ihn von inkonsistent verwendeten Doppelpunkten bereinigt \n",
    "                und mit abschlie√üenden Zeilenumbr√ºchen in die Textdatei\"\"\"\n",
    "                write_file.write(speaker.strip(\":\") + \"\\n\\n\") \n",
    "           \n",
    "            elif paragraph.get(\"class\") == [\"vers\"]:  #Da <p>-Elemente nichts weiter enthalten, wenn sie die Figur enthalten, \n",
    "                                                      #k√∂nnen wir mittels 'elif' √ºberpr√ºfen, ob das Attribut \"class\" [\"vers\"] \n",
    "                                                      #entspricht, denn in <p>-Elementen mit diesem Attribut sind \n",
    "                                                      #die Strophen enthalten\n",
    "                \n",
    "                \"\"\"Stropheninterne Regieanweisungen befinden sich in einem <span>-Element, dessen Existenz\n",
    "                im aktuelle <p>-Element wir hier √ºberpr√ºfen\"\"\"\n",
    "                if paragraph.span:\n",
    "                    paragraph.span.decompose() #Wenn ja l√∂schen wir es aus 'paragraph' mittels der 'decompose'-Methode\n",
    "            \n",
    "                \"\"\"Da Einr√ºckungen und whitespace generell in den Strophen inkonsistent verwendet werden,\n",
    "                splitten wir die Strophen in Verse, um im 'write'-Befehl unten einheitlich formatieren zu k√∂nnen.\"\"\"\n",
    "                lines = paragraph.text.split(\"\\n\")\n",
    "                \n",
    "                for line in lines: #Iteration √ºber die einzelnen Verse...\n",
    "                    write_file.write(\"\\t\" + line.strip() + \"\\n\") #...und schreiben in Textdokument (bereinigt und konsistent formatiert)\n",
    "                \n",
    "                write_file.write(\"\\n\") #Schreiben eines weiteren Zeilenumbruchs nach jeder Strophe (= zwei Zeilenumbr√ºche insgesamt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b4fe2-0842-4101-ae7c-9fba956d426b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 1:** Pass obigen Extraktionscode so an, dass vor jeder √Ñu√üerung der Name des/der jeweiligen Abgeordneten ausgegeben wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbef67f-22dd-4405-b35a-716c6b2f56a6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Parsen des XML-Dokuments sowie Extrahieren von 'root' (nur im L√∂sungsnotebook notwendig)\n",
    "#Achtung: anderer Pfad als im Notebook, da das L√∂sungsnotebook in einem anderen Verzeichnis liegt \n",
    "tree = ET.parse(\"../../3_Dateien/XML/plenarprotokoll.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "for speech in root.iter(\"rede\"):\n",
    "    \n",
    "    \"\"\"Eigentliche L√∂sung (Anfang): Wir extrahieren Vor- und Nachnamen, indem wir 'speech' mithilfe von 'iter' \n",
    "    rekursiv nach den gew√ºnschten Tags absuchen, das Resultat in eine Liste casten, das erste Element davon indizieren \n",
    "    und auf dessen Textinhalt zugreifen. Anstatt Casting in Liste und Indizierung w√§re auch jeweils eine 'for'-Schleife \n",
    "    m√∂glich, was den Code aber l√§nger machen w√ºrde. Alternativ k√∂nnten wir auch die RegEx-√§hnliche Suchsprache XPath\n",
    "    verwenden, was f√ºr die Extraktion von 'name' so auss√§he: name = speech.find(\".//p[@klasse='redner']//vorname\").text\n",
    "    vgl. dazu die Dokumentation von XPath: https://www.w3.org/TR/xpath-31/\"\"\"\n",
    "    \n",
    "    name = list(speech.iter(\"vorname\"))[0].text\n",
    "    surname = list(speech.iter(\"nachname\"))[0].text\n",
    "    print(name, surname, sep=\" \") #Ausgabe des konkatenierten Namens\n",
    "    \n",
    "    \"\"\"Eigentliche L√∂sung (Ende)\"\"\"\n",
    "    \n",
    "    for element in speech:\n",
    "        if element.tag == \"name\":\n",
    "            break\n",
    "        elif element.tag == \"p\" and element.text:\n",
    "            print(element.text.strip(), end=\" \") #Inkl. Bereinigung von leading/trailing whitespace\n",
    "    print(\"\\n\") #Einf√ºgen eines Zeilenumbruchs nach jeder Rede"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea99f5-e107-43e9-9024-9890e78a507d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 2:** Neben den Reden bzw. √Ñu√üerungen der Abgeordneten, die gerade das Rederecht besitzen, enth√§lt das Protokoll auch verbale Zwischenrufe sowie Anmerkungen √ºber Beifall, Lachen etc. Diese Informationen sind in Elementen mit dem Tag `<kommentar>` enthalten. Extrahier sie f√ºr alle Reden, f√ºr die mindestens ein `<kommentar>`-Element protokolliert wurde. Lass sie Dir zusammen mit dem Namen des/der Abgeordneten ausgeben, der/die eigentlich gerade am sprechen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38aaeb8-429c-4e94-8bcb-5e8ae9a2936b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Achtung: Code funktioniert nur, wenn der Code zur vorangehenden √úbung ausgef√ºhrt wurde!\n",
    "\n",
    "for speech in root.iter(\"rede\"):\n",
    "    \n",
    "    #√úberpr√ºfung, ob es √ºberhaupt <kommentar>-Elemente gibt, wenn nein, √ºberspringen\n",
    "    if len(list(speech.iter(\"kommentar\"))) == 0:\n",
    "        continue \n",
    "\n",
    "    #Erl√§uterung vgl. L√∂sung zur ersten √úbung\n",
    "    name = list(speech.iter(\"vorname\"))[0].text\n",
    "    surname = list(speech.iter(\"nachname\"))[0].text\n",
    "    print(name, surname, sep=\" \")\n",
    "    \n",
    "    #Rekursives Iterieren √ºber 'speech' und Suche nach 'kommentar'-Elementen\n",
    "    for comment in speech.iter(\"kommentar\"):\n",
    "        print(comment.text) #Ausgabe des Textinhalts\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c559d3e-ac04-4696-beed-24a752cd67cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*** \n",
    "\n",
    "‚úèÔ∏è **√úbung 3:** Im Anwendungsfall haben wir s√§mtliche Strophen von Faust I in einer externen Textdatei gespeichert. Diese verf√ºgt (idealerweise) √ºber eine interne Struktur. In der Musterl√∂sung wurde etwa mit Tabstopps und Zeilenumbr√ºchen gearbeitet, um einzelne Strophen bzw. die zugeh√∂rigen Figuren voneinander abzugrenzen. Diese Struktur kommt uns sp√§testens dann zu Gute, wenn wir die Daten f√ºr irgendeine Form von Auswertung mit Python wieder einlesen. \n",
    "\n",
    "Um unseren Anwendungsfall zu \"professionalisieren\", ist es nun Deine Aufgabe, den Faust I in einem XML-Dokument speichern, also in einem Format, dessen \"Aufgabe\" es ist, Daten ordentlich zu strukturieren. Kopier dazu den Code vom Extraktionsschritt des Anwendungsfalls in die folgende Zelle. Pass ihn anschlie√üend so an, dass die Daten dynamisch in ein XML-Dokument statt in eine Textdatei geschrieben werden.\n",
    "\n",
    "Als kleiner Bonus, der √ºberpr√ºft, ob bei der Speicherung alles geklappt hat, vor allem aber die N√ºtzlichkeit strukturierter Daten aufzeigt, kannst Du Dir anschlie√üend √ºber die bereits gegebene Code-Zelle einfach alle Verse des ber√ºhmten Teufels aus dem Faust ausgeben lassen. Wenn n√∂tig, pass den Dateipfad bzw. die Tags und Attribute Deiner Namensgebung an.\n",
    "\n",
    "<details><summary>ü¶ä Herausforderung </summary>\n",
    "<br>Extrahier zus√§tzlich den Titel jeder Szene und speichere diese Information jeweils an der richtigen Position in der Hierarchie des zu schaffenden XML-Dokuments.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9666726-29eb-4f83-851c-a80854451515",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Alle Kommentare beziehen sich auf den neu hinzugekommenen L√∂sungscode; Kommentare zum Extraktionscode\n",
    "siehe Anwendungsfall oben; Code funktioniert nur, wenn 'all_pages' vom Abrufschritt noch im Arbeitsspeicher ist!\"\"\"\n",
    "\n",
    "from xml.dom import minidom\n",
    "\n",
    "\"\"\"Initialisieren des obersten Elements in der Hierarchie. Elemente k√∂nnen in XML ja frei benannt werden.\n",
    "Die Text Encoding Initiative (TEI; https://www.tei-c.org) strebt eine Standardisierung der Repr√§sentation\n",
    "und Speicherung von Texten an und bietet auch genrespezifische Empfehlungen (hier f√ºr Dramen: \n",
    "https://www.tei-c.org/release/doc/tei-p5-doc/en/html/DR.html). Im Folgenden wurden diese \n",
    "Empfehlungen nur lose umgesetzt.\"\"\"\n",
    "text = ET.Element(\"text\") \n",
    "\n",
    "#Optionales Hinzuf√ºgen von Attributen\n",
    "text.set(\"title\", \"Faust I\")\n",
    "text.set(\"source\", \"https://www.projekt-gutenberg.org/goethe/faust1/index.html\") \n",
    "\n",
    "for page in all_pages:\n",
    "\n",
    "    soup = BeautifulSoup(page, \"lxml\") \n",
    "    body = soup.find(\"body\") \n",
    "\n",
    "    for paragraph in body.find_all(\"p\"):\n",
    "\n",
    "        if paragraph.find(\"span\", class_=\"speaker\"):\n",
    "            \n",
    "            \"\"\"Initialisieren eines Kindelements von 'text', das s√§mtliche aufeinanderfolgende Strophen\n",
    "            einer Figur beinhalten soll\"\"\"\n",
    "            character = ET.SubElement(text, \"character\")\n",
    "            \n",
    "            speaker = paragraph.find(\"span\", class_=\"speaker\").text\n",
    "            \n",
    "            character.set(\"name\", speaker.strip(\":\")) #Setzen eines Attributs, das den Namen der Figur beinhaltet\n",
    "\n",
    "        elif paragraph.get(\"class\") == [\"vers\"]: \n",
    "            \n",
    "            \"\"\"Initialisieren eines Kindelements von 'character', das s√§mtliche Verse einer (!)\n",
    "            Strophe beinhalten soll\"\"\"\n",
    "            stanza = ET.SubElement(character, \"stanza\")\n",
    "\n",
    "            if paragraph.span:\n",
    "                paragraph.span.decompose() \n",
    "\n",
    "            lines = paragraph.text.split(\"\\n\")\n",
    "\n",
    "            for line in lines: \n",
    "                \n",
    "                \"\"\"Initialisieren eines Kindelements von 'stanza', das jeweils einen Vers\n",
    "                beinhalten soll. Achtung: die Variable 'line' (die 'Vers' bedeutet) ist bereits \n",
    "                in Benutzung. Daher weisen wir das Kindelement der Variablen 'line_xml' zu.\n",
    "                Das XML-Element hei√üt dennoch nur 'line'. Es zeigt sich, dass Variablen, die ein \n",
    "                XML-Element referenzieren, unabh√§ngig von dessen Tag (Name) benannt werden k√∂nnen.\"\"\"\n",
    "                line_xml = ET.SubElement(stanza, \"line\")\n",
    "                line_xml.text = line.strip()\n",
    "\n",
    "#Hinzuf√ºgen von Einr√ºckungen\n",
    "pretty_corpus = minidom.parseString(ET.tostring(text)).toprettyxml(indent=\"  \")\n",
    "\n",
    "#Schreiben von 'pretty_corpus' in externe Datei mit der Endung 'xml' (anderer Pfad als im Notebook!)\n",
    "with open(\"../../3_Dateien/Output/faust_1.xml\", \"w\", encoding=\"utf-8\") as write_file:\n",
    "    write_file.write(pretty_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73d905-4b65-40ec-9496-f7ca58a80f9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bonus: Ausgabe aller Verse von Mephisto(pheles)\n",
    "root = ET.parse(\"../../3_Dateien/Output/faust_1.xml\").getroot() #Anderer Pfad als im Notebook!\n",
    "\n",
    "for character in root.iter(\"character\"):\n",
    "    if not character.get(\"name\") == \"Mephistopheles\":\n",
    "        continue\n",
    "    for line in character.iter(\"line\"):\n",
    "        print(line.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e5b52-64b0-4a08-b1d0-6cf0f3c1fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Herausforderung\n",
    "\n",
    "\"\"\"Alle Kommentare beziehen sich auf den neu hinzugekommenen L√∂sungscode; Kommentare zum Extraktionscode\n",
    "siehe Anwendungsfall oben; Code funktioniert nur, wenn 'all_pages' vom Abrufschritt noch im Arbeitsspeicher ist!\"\"\"\n",
    "\n",
    "from xml.dom import minidom\n",
    "\n",
    "\"\"\"Initialisieren des obersten Elements in der Hierarchie. Elemente k√∂nnen in XML ja frei benannt werden.\n",
    "Die Text Encoding Initiative (TEI; https://www.tei-c.org) strebt eine Standardisierung der Repr√§sentation\n",
    "und Speicherung von Texten an und bietet auch genrespezifische Empfehlungen (hier f√ºr Dramen: \n",
    "https://www.tei-c.org/release/doc/tei-p5-doc/en/html/DR.html). Im Folgenden wurden diese \n",
    "Empfehlungen nur lose umgesetzt.\"\"\"\n",
    "text = ET.Element(\"text\") \n",
    "\n",
    "#Optionales Hinzuf√ºgen von Attributen\n",
    "text.set(\"title\", \"Faust I\")\n",
    "text.set(\"source\", \"https://www.projekt-gutenberg.org/goethe/faust1/index.html\")\n",
    "\n",
    "for page in all_pages:\n",
    "\n",
    "    soup = BeautifulSoup(page, \"lxml\") \n",
    "    body = soup.find(\"body\") \n",
    "    \n",
    "    scene = soup.find(\"h3\").text #Zus√§tzliche Extraktion des Szenentitels\n",
    "    \n",
    "    \"\"\"Um Faust I nach Szenen zu gliedern, dr√§ngt sich die Schaffung einer weiteren hierarchischen Ebene\n",
    "    zwischen 'faust' und 'character' auf. Deshalb initialisieren wir hier ein Kindelement von 'text', \n",
    "    das s√§mtliche Strophen einer Szene beinhalten soll. Der Szenentitel soll in einem Attribut gespeichert werden.\"\"\"\n",
    "    scene = ET.SubElement(text, \"scene\", {\"title\": scene})\n",
    "    \n",
    "    for paragraph in body.find_all(\"p\"):\n",
    "\n",
    "        if paragraph.find(\"span\", class_=\"speaker\"):\n",
    "            \n",
    "            \"\"\"Initialisieren eines Kindelements von 'scene', das s√§mtliche aufeinanderfolgende Strophen\n",
    "            einer Figur beinhalten soll\"\"\"\n",
    "            character = ET.SubElement(scene, \"character\")\n",
    "            \n",
    "            speaker = paragraph.find(\"span\", class_=\"speaker\").text\n",
    "            \n",
    "            character.set(\"name\", speaker.strip(\":\")) #Setzen eines Attributs, das den Namen der Figur beinhaltet\n",
    "            \n",
    "        elif paragraph.get(\"class\") == [\"vers\"]: \n",
    "            \n",
    "            \"\"\"Initialisieren eines Kindelements von 'character', das s√§mtliche Verse einer (!)\n",
    "            Strophe beinhalten soll\"\"\"\n",
    "            stanza = ET.SubElement(character, \"stanza\")\n",
    "\n",
    "            if paragraph.span:\n",
    "                paragraph.span.decompose() \n",
    "\n",
    "            lines = paragraph.text.split(\"\\n\")\n",
    "\n",
    "            for line in lines: \n",
    "                \n",
    "                \"\"\"Initialisieren eines Kindelements von 'stanza', das jeweils einen Vers\n",
    "                beinhalten soll. Achtung: die Variable 'line' (die 'Vers' bedeutet) ist bereits \n",
    "                in Benutzung. Daher weisen wir das Kindelement der Variablen 'line_xml' zu.\n",
    "                Das XML-Element hei√üt dennoch nur 'line'. Es zeigt sich, dass Variablen, die ein \n",
    "                XML-Element referenzieren, unabh√§ngig von dessen Tag (Name) benannt werden k√∂nnen.\"\"\"\n",
    "                line_xml = ET.SubElement(stanza, \"line\")\n",
    "                line_xml.text = line.strip()\n",
    "\n",
    "#Hinzuf√ºgen von Einr√ºckungen\n",
    "pretty_corpus = minidom.parseString(ET.tostring(text)).toprettyxml(indent=\"  \")\n",
    "\n",
    "#Schreiben von 'pretty_corpus' in externe Datei mit der Endung 'xml' (anderer Pfad als im Notebook!)\n",
    "with open(\"../../3_Dateien/Output/faust_1_mit_szenentitel.xml\", \"w\", encoding=\"utf-8\") as write_file:\n",
    "    write_file.write(pretty_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7564e88-16ea-48aa-a6b0-8ee271d4d024",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bonus: Ausgabe aller Verse von Mephisto(pheles)\n",
    "root = ET.parse(\"../../3_Dateien/Output/faust_1_mit_szenentitel.xml\").getroot() #Anderer Pfad als im Notebook!\n",
    "\n",
    "for character in root.iter(\"character\"):\n",
    "    if not character.get(\"name\") == \"Mephistopheles\":\n",
    "        continue\n",
    "    for line in character.iter(\"line\"):\n",
    "        print(line.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c866453-bb50-4bee-8710-b12213fe7514",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 4:** XML ist wie gesagt ein beliebtes Format, um Daten zu speichern und zu teilen. Neben XML arbeiten wir beim Programmieren auch oft mit csv-Dateien (vgl. Notebooks \"Input und Output\" sowie \"Datenanalyse\"). Welchen Vorteil hat XML im Gegensatz zu csv?\n",
    "\n",
    "*L√∂sung: Informationen, die bei XML **einmal** gespeichert sind (hier s√§mtliche Informationen au√üer die W√∂rter der Rede), m√ºssen bei csv f√ºr jedes Element auf der tiefsten Ebene (also f√ºr jedes Wort) **wiederholt** werden. Das liegt an der fehlenden Hierarchie bei csv-Dateien. XML spart deshalb Speicherplatz. Allerdings k√∂nnen csv-Dateien aufgrund der h√§ufig wiederholten, identischen Informationen sehr stark komprimiert werden. Der Vorteil von XML besteht also nur, wenn man aktiv mit den Daten arbeitet. Zum Speichern bzw. Teilen k√∂nnen csv-Dateien √§hnlich kompakt gemacht werden wie XML-Dateien.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da003c3-16ff-4575-a559-2cbf0e0402c2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "***\n",
    "<table>\n",
    "      <tr>\n",
    "        <td>\n",
    "            <img src=\"../../3_Dateien/Lizenz/CC-BY-SA.png\" width=\"400\">\n",
    "        </td> \n",
    "        <td>\n",
    "            <p>Dieses Notebook sowie s√§mtliche weiteren <a href=\"https://github.com/yannickfrommherz/exdimed-student/tree/main\">Materialien zum Programmierenlernen f√ºr Geistes- und Sozialwissenschaftler:innen</a> sind im Rahmen des Projekts <i>Experimentierraum Digitale Medienkompetenz</i> als Teil von <a href=\"https://tu-dresden.de/gsw/virtuos/\">virTUos</a> entstanden. Erstellt wurden sie von Yannick Frommherz unter Mitarbeit von Anne Josephine Matz. Sie stehen als Open Educational Resource nach <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY SA</a> zur freien Verf√ºgung. F√ºr Feedback und bei Fragen nutz bitte das <a href=\"https://forms.gle/VsYJgy4bZTSqKioA7\">Kontaktformular</a>.\n",
    "        </td>\n",
    "      </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}