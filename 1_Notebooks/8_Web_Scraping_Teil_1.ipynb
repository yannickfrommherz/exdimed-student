{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefd49df-ee28-40b8-9b0a-92f8b62a95d4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Web Scraping Teil 1\n",
    "\n",
    "In diesem zweiteiligen Notebook lernen wir, wie wir mit **wenig Code** sehr **viel Inhalt** aus dem Internet herunterladen und f√ºr die Analyse aufbereiten k√∂nnen. Die dazu notwendige Technik nennt sich, wie schon mehrfach erw√§hnt, *Web Scraping*. Das englische Verb *to scrape* bedeutet \"sch√ºrfen\" ‚Äì wir sch√ºrfen blo√ü nicht Gold, sondern (potenziell goldwerte) Inhalte aus dem *Web*. ‚ú®\n",
    "\n",
    "Zu Beginn des Notebooks lernst Du wichtige Hintergr√ºnde zum Web Scraping. Danach wird es deutlich angewandter. Einige Inhalte sind eingeklappt, da sie nicht zwingend notwendig sind, um \"mitzukommen\". Lies sie gerne f√ºr ein vertieftes Verst√§ndnis des Themas.\n",
    "\n",
    "<details>\n",
    "    <summary><b>‚ùì Warum lohnt sich Web Scraping?</b></summary>\n",
    "    <br>Das Notebook \"Einf√ºhrung\" hielt gleich zu Beginn fest, dass Web Scraping mit ein Grund ist, warum sich Programmierkenntnisse lohnen. Als Beispiel diente ein Forschungsprojekt, in dem Bildunterschriften in Artikeln bei ZEIT Online gesammelt werden sollten. W√§hrend die manuelle Herangehensweise (Aufruf jedes Artikels im Browser, Markieren und Kopieren der Bildunterschriften, Einf√ºgen in eine Excel-Tabelle, Anreichern mit Metadaten wie Artikellink und -titel) bei einem quantitativen Vorhaben vollkommen impraktikabel ist, k√∂nnen wenige Zeilen Web Scraping-Code diese Aufgabe im Fluge erledigen.<br><br>Web Scraping er√∂ffnet Geistes- und Sozialwissenschaftler:innen neue M√∂glichkeiten, indem es die stetig wachsenden Mengen an Daten aus dem Internet f√ºr die Forschung nutzbar macht. Von online verf√ºgbaren Werken (z.&nbsp;B. beim <a href=\"https://www.projekt-gutenberg.org/info/texte/index.html\">Projekt Gutenberg</a>, s.&nbsp;u.), √ºber Kommentare unter Zeitungsartikeln bis hin zu Restaurantbewertungen bei Tripadvisor lassen sich die meisten Webinhalte so effizient und flexibel herunterladen. Web Scraping-Skills schlie√üen damit optimal an unsere bereits erlernten Fertigkeiten an, Daten aufzubereiten sowie zu analysieren (Notebook \"Datenanalyse\"), machen sie in vielen F√§llen die notwendigen Daten doch erst √ºberhaupt verf√ºgbar.<br><br>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>‚ùóÔ∏è Abgrenzung zu APIs</b></summary>\n",
    "    <br>Manche Plattformen (etwa soziale Medien, Beh√∂rden oder Datenbanken) bieten auch sog. <i>APIs</i> (auf Deutsch: <i>Programmierschnittstellen</i>), √ºber die (ausgew√§hlte) Daten abgefragt werden k√∂nnen. In den Zusatz√ºbungen zum Notebook \"Regul√§re Ausdr√ºcke\" benutzen wir etwa die offizielle API von Wikipedia. APIs sind jedoch sehr plattformspezifisch zu bedienen, unterliegen h√§ufigen √Ñnderungen und werden bisweilen ganz deaktiviert. Au√üerdem erfolgt ihr Zugriff meist direkt √ºber die Command Line, es sei denn es gibt einen sog. <i>Python Wrapper</i> (wie f√ºr die Wikipedia-API). Aus diesen Gr√ºnden behandelt dieses Notebook nur die etwas rohere, aber auch wesentlich flexiblere Technik des Web Scraping.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a358ff-308a-4fc8-8197-99f87b74dafe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Web Scraping besteht grunds√§tzlich aus **zwei Schritten**:\n",
    "\n",
    "1. **Abruf**: Wir rufen iterativ eine Webseite nach der anderen auf und speichern den ihnen zugrundeliegenden, sog. *Quelltext* (s.&nbsp;u.) ab.\n",
    "2. **Extraktion**: Da wir i.&nbsp;d.&nbsp;R. nur an einem bestimmten Teil des Quelltexts interessiert sind (z.&nbsp;B. an den erw√§hnten Bildunterschriften oder an den Pseudonymen von Kommentarschreibenden), extrahieren wir nun die relevanten Informationen aus dem Quelltext, d.&nbsp;h. \"wir trennen die Spreu vom Weizen\".\n",
    "\n",
    "Unten gehen wir auf beide Schritte detailliert ein. Zun√§chst wird der Anwendungsfall vorgestellt, den Du im zweiten Teil des Notebooks \"Web Scraping\" bearbeiten wirst. Anschlie√üend werfen wir einen Blick hinter die Kulissen von Webseiten. Ein grundlegendes Verst√§ndnis vom Aufbau einer Webseite, also vom Quelltext, ist insbesondere f√ºr den Extraktionsschritt unerl√§sslich. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306e187-ab81-4f17-9625-b1f21bd83f3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "## üîß Anwendungsfall: Den Faust I scrapen üé≠\n",
    "\n",
    "*Faust I* von Johann Wolfgang von Goethe ist eines der bedeutendsten Werke der deutschsprachigen Literatur. Eine Vielzahl an (quantitativen) Fragestellungen lie√üe sich an die Trag√∂die herantragen, z.&nbsp;B. k√∂nnten wir die Reinheit der Reimmuster untersuchen. Dank unserer stets wachsenden Python-Kenntnisse w√§ren wir dazu mehr und mehr in der Lage. \n",
    "\n",
    "Im Anwendungsfall in diesem Notebook soll es aber nicht um die Daten*auswertung* gehen, sondern um die vorgelagerte Daten*beschaffung*. Das gesamte Werk findet sich auf insgesamt 27 Seiten verteilt frei verf√ºgbar bei [Projekt Gutenberg](https://www.projekt-gutenberg.org/goethe/faust1/chap002.html). Wir wollen es automatisiert herunterladen. Konkret wird es Deine Aufgabe sein, s√§mtliche Strophen des Dramas inkl. der Figuren, die sie sprechen, in eine Textdatei zu √ºberf√ºhren. Regieanweisungen und Szenentitel ignorieren wir dabei. \n",
    "\n",
    "Da es sich \"nur\" um 27 Seiten handelt, w√ºrde es wohl nicht allzu lange dauern, den gesamten Werkinhalt auf jeder Seite zu markieren, zu kopieren und in eine Textdatei einzuf√ºgen. Bl√∂derweise befinden sich jedoch zwischen den einzelnen Strophen und manchmal sogar in den Versen versteckt jede Menge Regieanweisungen. Es k√§me einer Sisyphusarbeit gleich, diese zuverl√§ssig von Hand auszusortieren. Gl√ºcklicherweise ist der Quelltext der Webseiten aber so strukturiert, dass wir beim Web Scraping zielgerichtet nur die relevanten Textteile extrahieren k√∂nnen. Deswegen sowie aufgrund der Tatsache, dass wir unseren fertigen Code mit wenigen Anpassungen f√ºr die meisten der √ºber 10.000 Werke von Projekt Gutenberg verwenden k√∂nnen, ist das automatisierte Web Scraping der manuellen Herangehensweise klar √ºberlegen. \n",
    "\n",
    "Die Textdatei soll final wie folgt aussehen (der Auszug entstammt der Szene [*Nacht*](https://www.projekt-gutenberg.org/goethe/faust1/chap004.html)):\n",
    "\n",
    "    Geist\n",
    "\n",
    "        Wer ruft mir?\n",
    "\n",
    "    Faust\n",
    "\n",
    "        Schreckliches Gesicht!\n",
    "\n",
    "    Geist\n",
    "\n",
    "        Du hast mich m√§chtig angezogen,\n",
    "        An meiner Sph√§re lang gesogen,\n",
    "        Und nun ‚Äì\n",
    "\n",
    "    Faust\n",
    "\n",
    "        Weh! ich ertrag dich nicht!\n",
    "        \n",
    "Wie gewohnt kannst Du den Anwendungsfall sp√§ter ohne weitere Anleitung in Angriff nehmen oder die Schritt-f√ºr-Schritt-Anweisungen befolgen. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b808c-a142-46b6-b944-479e43c8975c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Hinter den Kulissen von Webseiten\n",
    "\n",
    "Alle Informationen, die Dein Browser braucht, um die Inhalte einer Webseite korrekt anzuzeigen (bzw. zu *rendern*), sind im Quelltext der Webseite enthalten. Bevor wir lernen, Quelltexte automatisiert \"anzuzapfen\", wollen wir den Schritt erst manuell durchf√ºhren. Wie auch im Notebook \"Regul√§re Ausdr√ºcke\" beschrieben, k√∂nnen wir uns den Quelltext jeder beliebigen Webseite wie folgt im Browser anzeigen lassen:\n",
    "\n",
    "- bei Google Chrome und Firefox mittels Rechtsklick \"Seitenquelltext anzeigen\" w√§hlen\n",
    "- bei Safari mittels Rechtsklick \"Seitenquelltext einblenden\" w√§hlen\n",
    "\n",
    "Der gesamte Quelltext ist i.&nbsp;d.&nbsp;R. sehr umfassend und meist sind wir wie erw√§hnt nur an bestimmten Elementen darin interessiert. Um ein solches Element innerhalb des Quelltexts zu lokalisieren, k√∂nnen wir es im Browser markieren und anschlie√üend\n",
    "\n",
    "- bei Google Chrome und Firefox mittels Rechtsklick \"Untersuchen\" w√§hlen\n",
    "- bei Safari mittels Rechtsklick \"Element-Informationen\" w√§hlen\n",
    "\n",
    "So k√∂nnen wir uns etwa anzeigen lassen, wo im Quelltext die √úberschrift der Webseite des [Projekt Gutenberg](https://www.projekt-gutenberg.org) definiert ist (\"Startseite\", vgl. rote/runde Markierung; Stand: 03/24), n√§mlich einigerma√üen verschachtelt in einem Element mit dem sog. *Tag* `<h3>`, wie der rechte Ausschnitt des folgenden Screenshots zeigt. Zu Elementen, Tags etc. gleich mehr!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464a701-9158-4289-8cb2-05b04e43ac0c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<img src=\"../3_Dateien/Grafiken_und_Videos/Seitenquelltext.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7b9a4-8bfc-4e39-9e29-ce7b714263f6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Um ein Gef√ºhl f√ºr das \"Ger√ºst\" einer Webseite zu erhalten, √ºben wir diesen manuellen Abruf von Quelltexten. Analog zur √úbung 3 im Notebook \"Regul√§re Ausdr√ºcke\" f√ºhren wir die erste √úbung nicht mithilfe von Python durch, sondern verwenden [Sublime Text](https://www.sublimetext.com) (vgl. Notebook \"Funktionen und Methoden Teil 2\" sowie \"Regul√§re Ausdr√ºcke\"), das insbesondere unter Verwendung von Tastenkombinationen √§u√üerst n√ºtzlich ist. In der √úbung verwenden wir regul√§re Ausdr√ºcke. Solltest Du damit noch nicht vertraut sein, extrahier stattdessen einen anderen, m√∂glichst oft vorkommenden literalen Suchbegriff aus dem Quelltext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e1ca3-0eb8-4f29-be4d-3cd1f93c5aa3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 1:** Besuch eine beliebige Webseite in einem Browser Deiner Wahl. Die Webseite sollte mehrere Links zu anderen Seiten beinhalten, was auf die meisten Webseiten zutrifft. Lass Dir wie oben beschrieben den Quelltext der Seite anzeigen. Geh nun wie folgt vor und verwend dabei die angegebenen Tastenkombinationen.\n",
    "\n",
    "1. Markier den gesamten Quelltext (`Strg` + `a` (Windows und Linux) bzw. `‚åò` + `a` (macOS)), kopier ihn (`Strg`/`‚åò` + `c`) und f√ºg ihn in ein leeres Dokument bei Sublime Text ein (`Strg`/`‚åò` + `v`). \n",
    "2. Formulier einen regul√§ren Ausdruck, der s√§mtliche Links in Deinem Quelltext matcht. Manche Webseiten enthalten komplette Links in ihrem Quelltext (z.&nbsp;B. \"ht<span>tps://www.</span>domain<span>.de/</span>subdomain/page\"), andere wiederum blo√ü abgek√ºrzte Links (\"subdomain/page\"), die, wenn man im Browser draufklickt, um den gemeinsamen \"Stammlink\" erg√§nzt werden. Dein regul√§rer Ausdruck soll sich nach dem Linkformat im Quelltext Deiner Webseite richten und m√∂glichst viele Links matchen.\n",
    "3. Aktivier die Suche bei Sublime Text √ºber `Strg`/`‚åò` + `f`). Gib den regul√§ren Ausdruck ein und such bzw. markier alle matches √ºber die Tastenkombination `Alt`/`option` + `Enter` (dies entspricht der Schaltfl√§che \"Find All\"). Ggf. musst Du regul√§re Ausdr√ºcke f√ºr die Suche aktivieren (links neben dem Suchfeld √ºber `.*`).\n",
    "5. Kopier alle Links. √ñffne ein weiteres Dokument bei Sublime Text und f√ºg die Links dort ein. F√ºhr auch diese Schritte nach M√∂glichkeit mithilfe von Tastenkombinationen aus.\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Tipp </summary>\n",
    "  <br>Folgender Screenshot zeigt links einen Ausschnitt des Quelltexts des <a href=\"https://de.wiktionary.org/wiki/Internet\" title=\"Wiktionary-Eintrag zu 'Internet'\">Wiktionary-Eintrags zu \"Internet\"</a> inklusive markierter matches des folgenden regul√§ren Ausdrucks, der sich zwar nicht als perfekt, aber als ausreichend praktikabel erwies:<br><br><code>\"(https:)?//?\\S+\"</code><br><br>Rechts zeigt der Screenshot die in ein neues Dokument eingef√ºgten Links, bei denen noch die Anf√ºhrungszeichen weggestript werden m√ºssten. Ebenso m√ºssten die abgek√ºrzten Links um ihren Stammlink erg√§nzt werden, damit sie aufgerufen werden k√∂nnen.<br><br><img src=\"../3_Dateien/Grafiken_und_Videos/uebung1_web_scraping.png\">\n",
    "</details>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd9ea5-cf3d-4ac8-aeb2-a7cf8c4333a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## HTML\n",
    "\n",
    "Der Quelltext einer Webseite ist √ºblicherweise in der sog. *Auszeichnungssprache* HTML (ausgeschrieben: *<u>H</u>yper<u>t</u>ext <u>M</u>arkup <u>L</u>anguage*) verfasst ‚Äì eine maschinenlesbare Sprache, die genaue Anweisungen liefert, was wo wie dargestellt werden soll. Ohne HTML (oder eine andere Auszeichnungssprache) w√ºrden s√§mtliche Inhalte einer Webseite unstrukturiert nebeneinander angezeigt werden (vgl. rechts im Screenshot unten). HTML sorgt daf√ºr, dass die Inhalte sinnvoll in *Header*, √úberschriften, ein oder mehrere Spalten in sinnvollen Abmessungen, *Footer* etc. angezeigt werden (links im Screenshot). Die gezeigte fiktive Webseite ist zwar √§u√üerst grob strukturiert, dennoch wird klar, dass die Strukturierung von Inhalten ein Muss ist. \n",
    "\n",
    "<img src=\"../3_Dateien/Grafiken_und_Videos/strukturiert_vs_unstrukturiert.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf4702-1a46-434a-8ceb-6bc41a783918",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<details>\n",
    "    <summary><b>‚ùóÔ∏è Statische vs. dynamische Webseiten</b></summary>\n",
    "    <br>Traditionellerweise sind alle Webseiten <b>statisch</b> aufgebaut. Der Quelltext einer statischen Webseite liegt fertig gecodet auf dem Server. Wenn eine Benutzerin eine solche Webseite aufruft, muss der fertige Quelltext nur noch an ihren Browser √ºbermittelt werden. Unabh√§ngig davon, wer die statische Webseite aufruft, sie sieht f√ºr alle gleich aus.\n",
    "    <br><br>Moderne Webseiten sind allerdings meistens <b>dynamisch</b> aufgebaut, d.&nbsp;h. ihr Quelltext wird (teilweise) erst beim Aufruf der Seite generiert, abh√§ngig u.&nbsp;a. vom Standort der Benutzerin und ihren vorherigen Browseraktivit√§ten. Je nach Benutzer:in wird eine etwas andere Webseite an den Browser √ºbermittelt. Dynamische Webseiten beinhalten h√§ufig auch Elemente, mit denen die Benutzerin interagieren kann, ohne dass die Webseite komplett neugeladen wird, etwa ausklappbare Textfelder oder Buttons zum Laden von weiteren Inhalten (z.&nbsp;B. Kommentare). \n",
    "    <br><br>Beim Web Scraping k√∂nnen wir grunds√§tzlich beide Arten von Webseiten herunterladen, denn auch bei dynamischen Webseiten wird schlussendlich ein HTML-Quelltext zur √úbermittlung generiert. Dynamische Webseiten stellen uns als Web Scrapende aber vor zwei Herausforderungen: Erstens benutzen wir statt eines Browsers das Python-Modul <code>requests</code> (s.&nbsp;u.), um Quelltexte herunterzuladen. Personalisierte Inhalte, etwa basierend auf unserem Loginstatus, k√∂nnen wir so nicht erhalten. Zweitens enth√§lt der gescrapte Quelltext nur, was der Browser auch direkt nach dem Laden der Webseite anzeigen w√ºrde. Standardm√§√üig eingeklappte Elemente (etwa der zweite Teil bei langen Kommentaren, der erst durch Klicken auf einen \"Mehr\"-Button nachgeladen w√ºrde) erhalten wir so nicht. Um diesen Einschr√§nkungen zu entgehen, bietet sich das Modul <code><a href=\"https://selenium-python.readthedocs.io/installation.html\">Selenium</a></code> an, das einen Browser emuliert. In diesem Notebook beschr√§nken wir uns der Einfachheit halber auf Quelltexte ohne individualisierte bzw. nachgeladene Inhalte, wie sie uns <code>requests</code> ohne Tamtam liefert.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e881893-e2c3-42aa-9aad-45af6b0ce4ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Nun ist es nicht unser Ziel, selbst (gute) Webseiten zu coden, sondern interessante Inhalte von bestehenden Webseiten \"blo√ü\" zu extrahieren. Die folgenden Erl√§uterungen zu HTML umfassen also nur das, was f√ºrs Web Scraping relevant ist. \n",
    "\n",
    "### Grundstruktur\n",
    "\n",
    "HTML strukturiert s√§mtliche Inhalte von Webseiten in Form von **Elementen** hierarchisch. Wie bei einem Stammbaum steht jedes Element in einer Abh√§ngigkeitsbeziehung zum ihm √ºbergeordneten Element und ggf. zu einem oder mehreren ihm untergeordneten Elementen. \n",
    "\n",
    "Drei Aspekte charakterisieren jedes Element:\n",
    "\n",
    "- **Tag**: Jedes Element hat ein Tag, das mehr oder weniger eindeutige Strukturinformationen vorgibt. Das Tag `<h1>` f√ºr √úberschriften etwa spezifiziert eine gro√üe, fett geschriebene Schrift. Genauer gesagt gibt es jeweils ein Starttag am Anfang jedes Elements (`<tag>`) und ein Endtag (`</tag>`) an dessen Ende (Ausnahme: inhaltsleere Elemente wie `<img>`, die kein Endtag haben, s.&nbsp;u.). Tags sind bei HTML vorgegeben ‚Äì eine √úbersicht √ºber die wichtigsten aus Web Scraping-Perspektive folgt unten.\n",
    "- **Attribute**: Neben dem Tag kann ein Element optional Attribute umfassen, die weitere Eigenschaften des Elements in Form von Schl√ºssel-Werte-Paaren definieren. Ein Element kann dadurch etwa einer bestimmten Klasse von Elementen zugeordnet werden, die wiederum auf eine bestimmte Art formatiert wird. \n",
    "- **Inhalt**: Meistens enth√§lt das Element Inhalt in Form von Text oder in Form untergeordneter (verschachtelter) Elemente. Textinhalt wird nicht von Anf√ºhrungszeichen umrahmt.\n",
    "\n",
    "Daraus folgt diese **grundlegende HTML-Syntax** f√ºr ein Element mit keinem, einem oder mehreren Attributen sowie Textinhalt:\n",
    "\n",
    "- `<tag>content</tag>` (keine Attribute)\n",
    "- `<tag attribute=\"value\">content</tag>` (ein Attribut)\n",
    "- `<tag attribute1=\"value1\" attribute2=\"value2\">content</tag>` (zwei Attribute)\n",
    "\n",
    "Wie einem Element andere Elemente untergeordnet werden (also verschachtelter Inhalt), sehen wir gleich. \n",
    "\n",
    "Jeder HTML-Code beginnt mit `<!DOCTYPE html>`, was den Dokumenttyp deklariert. Darauf folgt das oberste Element in der Hierarchie, der *Stamm* bzw. die *Wurzel* (engl. *root*), n√§mlich ein Element mit dem Tag `<html>`. Dieses Element hat wiederum immer zwei *Kinder*: `<head>` und `<body>`. In `<head>` befinden sich f√ºr uns i.&nbsp;d.&nbsp;R. uninteressante Daten wie der Titel der Webseite (der im Browsertab angezeigt wird) oder die verwendete Zeichencodierung. Der komplette Inhalt der Webseite befindet sich in `<body>`. Diese Grundstruktur zeigt sich sowohl im Screenshot des Quelltexts von www.projekt-gutenberg.org oben (das `<head>`-Element ist eingeklappt) als auch in der n√§chste Zelle. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "845930c5-1265-4144-bf78-64dc0e94a378",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Titel der Webseite</title>\n",
    "        <meta charset=\"UTF-8\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>1. √úberschrift</h1>\n",
    "        <p>Erster Textabschnitt</p>\n",
    "        <h2>2. √úberschrift</h2>\n",
    "        <p>Hier folgt eine Liste</p>\n",
    "        <ul>\n",
    "            <li>Punkt 1</li> \n",
    "            <li>Punkt 2</li> \n",
    "            <li>Punkt 3</li> \n",
    "        </ul>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d98c88-142c-4de8-a847-60360d132402",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Das Element `<body>` umfasst hier eine gro√üe √úberschrift (`<h1>`), einen normalen Textabschnitt (`<p>`), eine kleinere √úberschrift (`<h2>`), gefolgt von noch einem Textabschnitt (`<p>`) sowie einer Liste (`<ul>`) mit drei Unterpunkten (`<li>`). Das Listenelement mit seinen Unterpunkten exemplifiziert verschachtelten Inhalt (achte darauf, wo sich das Endtag des `<ul>`-Elements befindet).\n",
    "\n",
    "Wenn Du die Zelle obendran vom Typ \"Raw\" auf \"Markdown\" √§nderst und die Zelle ausf√ºhrst, wird der HTML-Code gerendert, wie es ein Browser auch tun w√ºrde, wenn er den entsprechenden Quelltext darstellen m√ºsste (abgesehen vom Titel der Webseite, den der Browser im Tab anzeigen w√ºrde). Das liegt daran, dass Markdown ebenfalls eine Auszeichnungssprache ist, die ihrerseits mit HTML-Code umgehen kann (vgl. auch die mittels HTML erzeugte zweispaltige Formatierung bei √úbung 2 unten ‚Äì indem Du in die Zelle doppelklickst, kannst Du den ungerenderten HTML-Code sehen).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b61b34-f541-41f1-b61d-09582dcfc359",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- Da es sich um in Markdown eingebetteten HTML-Code handelt, ist die grundlegende HTML-Struktur mit Dokumenttypdeklaration, html- und body-Element nicht n√∂tig. Markdown-Code kommt ohne diese Struktur aus. Dieser Text hier ist √ºbrigens ein Kommentar, der nicht gerendert wird.-->\n",
    "\n",
    "<table style=\"font-size: 16px; letter-spacing: 0.2px;\">\n",
    "      <tr>\n",
    "        <td>\n",
    "            <p>‚úèÔ∏è <b>√úbung 2:</b> Um die Struktur von HTML-Code weiter zu verinnerlichen, wollen wir trotzdem eine (sehr basale) Webseite schreiben. \n",
    "                Deine Aufgabe ist es, die im Screenshot gezeigte Rezeptwebseite nachzuprogrammieren. \n",
    "                Viel mehr als die oben gegebene HTML-Grundstruktur ben√∂tigst Du daf√ºr nicht. Blo√ü zwei Dinge fehlen Dir:</p>\n",
    "            <ol>\n",
    "                <li>Um den Link zum <a href=\"https://www.eat-this.org/baba-ganoush-orientalischer-auberginensalat/#recipe\">Originalrezept</a> einzubetten, \n",
    "                    kannst Du folgende Syntax verwenden, wobei Du den Link bei \"link\" innerhalb der Anf√ºhrungszeichen einsetzt.<br><br>\n",
    "                    <code>&lt;a href=\"link\"&gt;Eat this&lt;/a&gt;</code></li><br>\n",
    "                <li>Um eine Liste zu nummerieren, verwendest Du das Tag <code>&lt;ol&gt;</code> f√ºr <u>o</u>rdered <u>l</u>ist \n",
    "                    statt <code>&lt;ul&gt;</code> (<u>u</u>nordered <u>l</u>ist).</li>\n",
    "            </ol>\n",
    "            <p>Schreib den Code in einem Sublime Text-Dokument und speichere es anschlie√üend mit der Endung \".html\". \n",
    "               Wenn Du es nun in einem beliebigen Browser √∂ffnest, sollte Dir Deine Rezeptwebseite angezeigt werden.</p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../3_Dateien/Grafiken_und_Videos/lieblingsrezept.png\" width=\"1000\">\n",
    "        </td> \n",
    "      </tr>\n",
    "</table>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a48fe58-b30e-4b66-94d5-12c5d876aa68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Tags\n",
    "\n",
    "Nun schauen wir uns HTML-Tags an, die typischerweise interessante Inhalte umfassen. Wir unterscheiden grob zwischen strukturierenden Elementen (sog. *Blockelemente*) und formatierenden Elementen (sog. *Inline-Elemente*). Erstere grenzen Inhalte voneinander ab, z.&nbsp;B. mithilfe von Text*bl√∂cken* oder Spalten. Sie beginnen standardm√§√üig auf einer neuen Zeile. Letzere nehmen den Feinschliff vor, zumeist innerhalb von Blockelementen, z.&nbsp;B. indem sie Text **fett** oder *kursiv* setzen. \n",
    "\n",
    "Vorerst reicht es, wenn Du die folgende Zusammenstellung √ºberfliegst, um einen Eindruck der wichtigsten Tags zu erhalten. Sp√§ter kannst Du hier nachschlagen.\n",
    "\n",
    "| Tag | Verwendung |\n",
    "|-|-|\n",
    "|\n",
    "|**Strukturierende Elemente**\n",
    "|`<div>` | generisches Blockelement, um Inhalte zu strukturieren |\n",
    "|`<main>`, `<article>`, `<section>`| weitere Blockelemente, um Inhalte zu strukturieren | \n",
    "|`<header>`, `<footer>`| Blockelemente f√ºr einf√ºhrenden bzw. abschlie√üenden Inhalt (etwa Ver√∂ffentlichungsdatum bzw. Copyright)| \n",
    "|`<h1>` bis `<h6>`| Blockelemente f√ºr gro√üe bis kleine √úberschriften |\n",
    "|`<p>`| Blockelement f√ºr Text | \n",
    "|`<table>`, `<th>`, `<tr>`, `<td>`| generiert eine (√ºblicherweise unsichtbare) Tabelle zur Anordnung von Inhalten in mehreren Spalten (`<table>`); `<th>` enth√§lt Spalten√ºberschriften, `<tr>` Zeileninhalte und `<td>` Spalteninhalte, vgl. √úbung 2 oben | \n",
    "|`<ul>`, `<ol>`, `<li>`| generiert Liste mit Punkten (`<ul>`) oder Nummerierung (`<ol>`), wobei einzelne Punkte in `<li>` enthalten sind | \n",
    "|`<img>`, `<audio>`, `<video>` | Blockelemente f√ºr Bilder, Audio- und Videoinhalte; Quelle steht im `src`-Attribut: `<img src=\"path\">`; kein Elementinhalt (Endtag entf√§llt)\n",
    "|\n",
    "| **Formatierende Elemente**\n",
    "|`<span>`| generisches Inline-Element f√ºr Text; mithilfe von Attributen wird darin enthaltener Text formatiert |\n",
    "|`<a>`| Inline-Element f√ºr klickbaren Link (Link steht im `href`-Attribut, angezeigter Text im Elementinhalt: `<a href=\"https://leo.org\">W√∂rterbuch</a>`) | \n",
    "|`<b>`, `<i>`, `<u>`, `<s>`, `<code>` | Inline-Elemente, um Text **fett** (`<b>`), *kursiv* (`<i>`), <u>unterstrichen</u> (`<u>`), <s>durchgestrichen</s> (`<s>`) oder als `code` (`<code>`) zu formatieren | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea2632-93b7-4bc8-add6-ffb68429ae10",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Attribute\n",
    "\n",
    "Wie erw√§hnt k√∂nnen Tags zus√§tzlich Attribute besitzen. Im Web Scraping-Zusammenhang ist besonders das `href`-Attribut bei Elementen mit `<a>`-Tag interessant, etwa wenn wir basierend auf dem Quelltext einer \"Startseite\" alle Links zu Unterseiten ausfindig machen wollen, um diese anschlie√üend ebenfalls zu scrapen. \n",
    "\n",
    "Weiter ist das Attribut `class` spannend, das von Webseitenerstellenden bei verschiedenen Tags dazu genutzt wird, √§hnliche Inhalte gleich zu formatieren. F√ºr jede Klasse von Elementen kann n√§mlich ein bestimmter \"Style\" definiert werden. Nun interessiert uns nicht die Formatierung per se, wohl aber k√∂nnen wir uns die Gruppierung zunutze machen, um √§hnliche Elemente zielgerichtet zu extrahieren. \n",
    "\n",
    "**Beim Extraktionsschritt kombinieren wir i.&nbsp;d.&nbsp;R. Tags und Attribute, um in zumeist endlosen Quelltexten genau das \"herauszufischen\", was uns interessiert.**\n",
    "\n",
    "Schlie√üen wir den theoretischen Teil mit zwei √úbungen ab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513c261-4016-479c-bccf-d77138b642ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 3:** In folgenden HTML-Code haben sich diverse Fehler eingeschlichen. Wie viele findest Du? Beheb sie alle.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "45f97187-537b-452c-af5a-c3501426a346",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<kopf>\n",
    "    <title>\"Finde den Fehler!\"</title>\n",
    "    <meta charset=\"UTF-9\">\n",
    "</kopf>\n",
    "<body>\n",
    "    <p>\"Es wimmelt nur so vor <b>Fehlern</b> in diesem HTML-Code\"</q>\n",
    "    <div>\n",
    "        <p>\"Eine gute Ressource zum Weiterlernen ist <a href=www.w3schools.com/html>w3schools</a>\"\n",
    "        <img src=\"picture.png\">\n",
    "<body>\n",
    "<\\html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62e6ea-5dd1-4f58-adaa-d52a30dc25de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 4:** Reproduzier folgenden Text mithilfe von HTML. Achte auf die korrekte Hierarchie der Elemente sowie darauf, alle Tags zu schlie√üen. Handelt es sich um formatierende Inline-Elemente oder um strukturierende Blockelemente?\n",
    "\n",
    "<img src=\"../3_Dateien/Grafiken_und_Videos/HTML_Formatierung.png\">\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288d25a-06a8-427b-b571-ca937d851df4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Nun folgt der angewandte Teil. \n",
    "\n",
    "Zun√§chst schauen wir uns die M√∂glichkeit an, ganz bestimmte Elemente von Webseiten zu scrapen (z.&nbsp;B. √úberschriften oder Links). Dazu m√ºssen wir uns im zweiten Schritt, der Extraktion, vertieft mit dem HTML-Quelltext auseinandersetzen. Das ist mitunter aufwendig, andererseits erlaubt es uns, Inhalte zielgerichtet \"anzusprechen\".\n",
    "\n",
    "Zum Abschluss des ersten Teils dieses Notebooks lernen wir dann noch eine simplere Option kennen, um ganz einfach den \"Haupttext\" einer Webseite zu scrapen (bei einer Nachrichtenseite etwa der eigentliche Artikeltext). Dabei f√§llt die Auseinandersetzung mit dem Quelltext weg, gleichzeitig k√∂nnen wir nicht mehr punktgenau steuern, welche Inhalte wir sammeln. Je nach Anwendungsfall eignet sich eine der beiden Herangehensweisen besser.\n",
    "\n",
    "## Abruf\n",
    "\n",
    "F√ºr den ersten Schritt, das Abrufen von Quelltexten, installieren wir zun√§chst das daf√ºr verwendete Modul `requests` √ºber die Command Line: `pip3 install requests`. Anschlie√üend importieren wir es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96f724-ce0a-44aa-807b-284366acc287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1dde8-d716-4e77-bf3e-4fe270497875",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Den Quelltext einer Webseite abzurufen ist denkbar einfach. Wir m√ºssen blo√ü der `get`-Funktion des Moduls den gew√ºnschten URL, also den Link, √ºbergeben, hier etwa zum Wikipedia-Artikel unserer Lieblingsprogrammiersprache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3045fa2-e675-459f-b426-7328276b2a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "link = \"https://de.wikipedia.org/wiki/Python_(Programmiersprache)\"\n",
    "response = requests.get(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc389c-6b00-4232-9592-58e594d37931",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "‚ö†Ô∏è Achtung: F√ºhr diesen Code am besten nur einmal aus. Dies gilt auch f√ºr alle weiteren Codes hier und in Deinen eigenen Skripten, in denen Inhalte von einem Server heruntergeladen werden. Bei h√§ufiger Ausf√ºhrung innerhalb kurzer Zeit kann es sein, dass Du vom Server **blockiert** wirst. Mehr zu verantwortungsvollem Web Scraping findest Du im zweiten Teil des Notebooks.\n",
    "\n",
    "`source_code` enth√§lt jetzt den Quelltext der Wikipedia-Seite zu unserer Programmiersprache. Es handelt sich aber nicht um einen gew√∂hnlichen string, sondern um ein sog. `Response`-Objekt. Wenn wir uns das Objekt einfach ausgeben lassen, erhalten wir den HTTP-Statuscode vom Abruf der Webseite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa933c34-c014-4ba9-9af1-75ff672df247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4dbe75-9be5-4f12-83b7-afee41f7ac85",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "200 und alle anderen Codes beginnend mit einer Zwei bedeuten, dass alles geklappt hat. Andere Codes wie 404 bedeuten, dass der Quelltext nicht abgerufen werden konnte. [Hier](https://de.wikipedia.org/wiki/HTTP-Statuscode#Liste_der_HTTP-Statuscodes) erf√§hrst Du mehr √ºber die verschiedenen Statuscodes.\n",
    "\n",
    "Indem wir das Attribut `encoding` anh√§ngen, k√∂nnen wir in Erfahrung bringen, wie die Webseite enkodiert ist (vgl. Notebooks \"Input und Output Teil 1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd899b-1dc4-4a87-90a8-1dbf9a537e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.encoding #Nach Attributen stehen keine Klammern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075b1cb-abdb-42f9-92b0-6bb41da5cc28",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "‚ö†Ô∏è Achtung: Verwirrenderweise bezieht sich der Begriff *Attribut* auf zwei verschiedene Dinge in diesem Notebook: Einerseits sind damit bestimmte \"Teile\" eines Objekts gemeint (hier das Encoding oder der eigentliche Quelltext des `Response`-Objekts, weiter unten auch der Textinhalt eines `BeautifulSoup`-Objekts), andererseits die Schl√ºssel-Werte-Paare, die optional Eigenschaften von HTML-Elementen definieren (s.&nbsp;o.).\n",
    "\n",
    "Um das Encoding herauszufinden, schaut `requests` im `<head>`-Element des gescrapten Quelltexts nach (s.&nbsp;o.). Falls diese Angabe dort fehlt oder falsch ist, k√∂nnten wir diesem Attribut auch einfach den korrekten Wert zuweisen.\n",
    "\n",
    "Auf den *eigentlichen* Quelltext greifen wir √ºber das Attribut `text` zu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0da6c4-bd1d-4c2a-9aa3-a79cbcf2d9cf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c411da-85cd-4f3e-bb9d-4363c33ad41e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Dabei erhalten wir einen langen string zur√ºck. Nun wissen wir aber, dass HTML-Code hierarchisch aufgebaut ist. Um die Beziehungen zwischen den einzelnen Elementen zu verdeutlichen, k√∂nnen wir den Quelltext mithilfe des Moduls `BeautifulSoup` sog. *parsen*, also entsprechend der HTML-Syntax interpretieren.\n",
    "\n",
    "Dazu installieren wir das Modul √ºber die Command Line: `pip3 install beautifulsoup4`. Weiter installieren wir `lxml` (`pip3 install lxml`), das von `BeautifulSoup` intern ben√∂tigt wird. Anschlie√üend importieren wir `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bce69f-60e9-4e3c-a012-b9a6ca0aa71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #Beachte die korrekte Schreibweise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795d264-a4d9-4624-9d02-e2cccefa3d14",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Nun k√∂nnen wir den Quelltext parsen, indem wir ein `BeautifulSoup`-Objekt erstellen bzw. eine sog. *Suppe kochen*. Wir √ºbergeben `BeautifulSoup` dazu den Quelltext (und nur diesen, also nicht das gesamte `Response`-Objekt). Anschlie√üend k√∂nnen wir uns die Suppe √ºber die Methode `prettify` sch√∂n formatiert ausgeben lassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cf2b8-766c-47f5-a48d-62fa6c3d4720",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text)\n",
    "print(soup.prettify()) #'prettify' funktioniert nur innerhalb eines 'print'-Befehls!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb1903-56f5-44cf-9619-ddbc5368b8dc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Zur√ºck zum Abruf von Quelltexten: Die `get`-Funktion von `requests` akzeptiert weitere Parameter, von denen insbesondere zwei wichtig sind: `timeout` und `headers`. \n",
    "\n",
    "√úber `timeout` definieren wir, wie viele Sekunden `requests` maximal auf eine Antwort vom Server warten soll. Tun wir dies nicht, l√§uft `requests` im schlimmsten Fall bis in alle Ewigkeit. `headers` wiederum beinhaltet die Informationen, die ein Browser normalerweise an den Server schickt. Indem wir ebensolche Informationen beim Abruf via `requests` verwenden, tun wir so, als w√ºrden wir einen gew√∂hnlichen Browser benutzen. Das minimiert unser Risiko, blockiert zu werden.\n",
    "\n",
    "In der folgenden Zelle scrapen wir die Startseite der [Tagesschau](https://www.tagesschau.de) mit einem Timeout von f√ºnf Sekunden, nur f√ºr den Fall, dass die Webseite gerade down w√§re oder wir keinen Zugriff darauf h√§tten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d490531-8af0-47cc-9a15-1e8b059ff73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "response = requests.get(\"https://www.tagesschau.de\", timeout=5, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fdd574-46f3-44f9-996f-8b83a14662d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Als Vorblick auf den Extraktionsschritt unten lassen wir uns mal eben die aktuelle, oberste Schlagzeile bei [Tagesschau](https://www.tagesschau.de) ausgeben. Dazu parsen wir den Quelltext wiederum mithilfe von `BeautifulSoup`. √úber die `find`-Methode und eine Kombination von Tag (`<span>`) und Attribut (`class=\"teaser__headline\"`) suchen wir nach dem gew√ºnschten Element. \n",
    "\n",
    "Dass sich die Schlagzeile in einem `<span>`-Element mit dem Attribut `class=\"teaser__headline\"` befindet, k√∂nnen wir einfach herausfinden, indem wir wie [oben beschrieben](#Hinter-den-Kulissen-von-Webseiten) im Browser das entsprechende Element im Quelltext lokalisieren. \n",
    "\n",
    "Sollte der folgende Code eine Fehlermeldung produzieren, liegt das daran, dass der Quellcode der Seite in der Zwischenzeit ge√§ndert wurde und ein Element mit der angegebenen Tag-Attribut-Kombination nicht gefunden werden konnte. Bring in diesem Fall selbst in Erfahrung, in welchem Element sich die oberste Schlagzeile befindet. Eine solche wird es bei der Tagesschau ja immer geben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67293fd-268f-4315-b07a-fccab7858263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text) #Parsen des eigentlichen Quelltexts\n",
    "headline = soup.find(\"span\", class_=\"teaser__headline\") #Suchen nach erstem Vorkommen des Elements mit Tag <span> und spezifiertem Attribut (zum Unterstrich in \"class_\" s. u.)\n",
    "headline.text #Ausgabe des Textinhalts des Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba71c8-149c-4df2-943c-ce1678a91816",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Wie Du siehst, ist der Abrufschritt sehr trivial. \n",
    "\n",
    "Bis hierhin haben wir immer nur eine Webseite auf einmal gescrapt. Mithilfe einer Schleife ist es aber nat√ºrlich kein bisschen komplizierter, mehrere Webseiten hintereinander abzurufen. Am einfachsten geht dies, wenn wir bereits eine Liste mit vollst√§ndigen Links haben, √ºber die wir iterieren k√∂nnen. Oft k√∂nnen wir uns auch den Umstand zunutze machen, dass URLs von Webseiten nach einem einheitlichen Schema aufgebaut sind. Einen Fall davon wollen wir nun √ºben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278b0f8-c591-4cbc-9a65-9c21ad6b9aa9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 5:** Besuch die Webseite der [Tagesschau](https://www.tagesschau.de) (und √ºberpr√ºf gleich mal, ob die oben extrahierte Schlagzeile wirklich die aktuelle ist!) und erg√§nz die in `categories` bereits angefangene Liste mit Ressorts, die Nachrichten zu spezifischen Themen bieten. Besuch au√üerdem die Webseiten einiger Ressorts und analysier, wie die jeweiligen Links aufgebaut sind. Nutz dieses Wissen und iterier √ºber `categories`, um den Quelltext jedes Ressorts herunterzuladen. Benutz denselben Code wie oben, um die oberste Schlagzeile f√ºr jedes Ressort zu extrahieren. Lass Dir Ressort und aktuelle oberste Schlagzeile nacheinander ausgeben. \n",
    "\n",
    "Schreib den Code f√ºr den Abruf- bzw. Extraktionsschritt in separaten Zellen. In der ersten Zelle rufst Du alle Quelltexte ab und h√§ngst sie einer Liste an. In der zweiten Zelle extrahierst Du aus jedem Quelltext die gew√ºnschten Informationen. So f√ºhrst Du den Abrufschritt nicht mehr aus, wenn Du am Extraktionsschritt arbeitest, wodurch der Server der Tagesschau nicht unn√∂tig belastet wird. Dadurch sinkt das Risiko, dass Du blockiert wirst.\n",
    "\n",
    "<details><summary>üìå Herausforderung </summary>\n",
    "<br>Extrahier zus√§tzlich zur obersten Schlagzeile den Link, der zum entsprechenden Artikel f√ºhrt und lass ihn Dir ebenfalls ausgeben. Klick auf die ausgegebenen Links, um zu schauen, ob sie wie erwartet funktionieren.\n",
    "<br><br>\n",
    "Das entsprechende Element findest Du auf die gleiche Weise wie f√ºr die oberste Schlagzeile beschrieben. Da sich der Link aber im <code>href</code>-Attribut (und nicht im Elementinhalt) verbirgt, h√§ngst Du anstatt <code>text</code> die Methode <code>get(\"href\")</code> an das im Quelltext gefundene Element an. \n",
    "</details>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c6843-3150-41e8-90f0-902968cda8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zum Abrufschritt schreiben.\n",
    "categories = [\"Inland\", \"Ausland\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801eceb5-e35c-41e8-bd56-5e4db85a302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zum Extraktionsschritt schreiben.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89c88d-e989-4b6f-9758-c9a866ec19e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "In √úbung 5 haben wir implizit den Umstand ausgenutzt, dass die Unterseiten von [Tagesschau](https://www.tagesschau.de) analog zur Hauptseite aufgebaut sind. Folglich war es kein Problem, denselben Extraktionscode bei allen Unterseiten anzuwenden. Diese Logik ist matchentscheidend beim Web Scraping! ‚öΩÔ∏è\n",
    "\n",
    "In echten Anwendungsf√§llen scrapen wir wie hier fast immer mehrere Seiten auf einmal ‚Äì je nach Forschungsinteresse (Aber-)Tausende. Die eben praktizierte Herangehensweise mit einer `for`-Schleife ist dabei der Standardansatz. Entweder wir verf√ºgen bereits √ºber eine Liste mit kompletten Links oder wir bauen wie eben die Links zu den einzelnen Seiten \"on the fly\" zusammen.\n",
    "\n",
    "Eine weitere Variante des iterativen Abrufens von Webseiten kommt bei sog. *paginierten* Seiten zum Einsatz. Paginierte Seiten verteilen zusammengeh√∂renden Inhalt auf mehrere Seiten ‚Äì ein typisches Beispiel ist ein auf mehreren Seiten fortlaufender Thread in einem Forum (etwa [Stack Overflow](https://stackoverflow.com)). Anstatt zu einer `for`-Schleife, greifen wir in diesem Fall zu einer `while`-Schleife (vgl. Notebook \"Kontrollstrukturen\"). Auf jeder gescrapten Seite suchen wir nach dem Link zur darauffolgenden Seite, der sich √ºblicherweise hinter einer Schaltfl√§che wie \"N√§chste Seite\" verbirgt. Anschlie√üend scrapen wir diese. Die Schleife bricht ab, wenn es keine folgende Seite mehr gibt. Im Anwendungsfall im zweiten Teil dieses Notebooks wirst Du diese Technik einsetzen.\n",
    "\n",
    "Damit wissen wir alles Wichtige zum Abrufschritt. Machen wir weiter mit der Extraktion!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5a57a-c688-453e-8149-33e2f51cebe7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Extraktion\n",
    "\n",
    "Wie oben vorweggenommen parsen wir gescrapte Quelltexte mit `BeautifulSoup`. Das daraus resultierende `BeautifulSoup`-Objekt erlaubt es uns, mit der hierarchischen HTML-Struktur zu arbeiten. \n",
    "\n",
    "`BeautifulSoup` h√§lt zahlreiche praktische Techniken bereit, die das Navigieren √ºber HTML-Code sowie das Extrahieren bestimmter Elemente daraus einfach macht. Um diese kennenzulernen, scrapen wir in der folgenden Zelle einen bestimmten [Eintrag](https://scilogs.spektrum.de/hirn-und-weg/schlecht-vernetzt-psychische-stoerungen-im-gehirn/) des Wissenschaftsblogs SciLogs. Es handelt sich um einen Artikel √ºber Hirnforschung im Zusammenhang mit psychischen Erkrankungen. Sollte die Seite in der Zwischenzeit gel√∂scht worden sein, kannst Du ihren Quelltext stattdessen √ºber die auskommentierten Code-Zeilen einlesen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb375d-beeb-4e0c-b13f-524f954f2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = requests.get(\"https://scilogs.spektrum.de/hirn-und-weg/schlecht-vernetzt-psychische-stoerungen-im-gehirn/\", timeout=5, headers=headers).text\n",
    "\n",
    "#with open(\"../3_Dateien/HTML/artikel.html\") as f:\n",
    "    #source_code = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0fa80-4a07-4705-9a97-64d53c5ae2a5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Als n√§chstes erstellen wir aus `source_code` ein `BeautifulSoup`-Objekt. \n",
    "\n",
    "Als Erinnerung: Wir √ºbergeben `BeautifulSoup` nur den eigentlichen Quelltext, den wir in der Zelle obendran √ºber das `text`-Attribut beim `Response`-Objekt erhalten haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16f082-16fc-4e4c-ad11-dcaef9a7cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source_code) #Falls es Dich interessiert: 'BeautifulSoup' ist ein sog. Constructor bei Python, d. h. es erstellt/initialisiert ein 'BeautifulSoup'-Objekt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3fa4e-8663-4f23-a92b-46bfe1e152fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Werfen wir einen Blick in den sch√∂n formatierten HTML-Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca913f-2d99-4f13-be3e-cc032c79ee4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(soup.prettify()) #'prettify' funktioniert wie gesagt nur innerhalb eines 'print'-Befehls!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d81b52-00c8-479f-a4a1-997d092d4b87",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Scroll einmal durch den Quelltext durch. √ñffne ebenfalls die [Webseite](https://scilogs.spektrum.de/hirn-und-weg/schlecht-vernetzt-psychische-stoerungen-im-gehirn/) und lokalisier einige wichtige Elemente wie √úberschriften, Bildunterschriften oder die Kommentare im Quelltext. Bei der weiteren Arbeit mit diesem Quelltext ‚Äì sowie wenn Du sp√§ter eigene Webseiten scrapst ‚Äì kommst Du nicht umhin, Dich immer wieder in den Quelltext \"reinzufuchsen\".\n",
    "\n",
    "Wie oben gesehen, k√∂nnen wir mithilfe der Methode `find`, das erste Element des als Argument √ºbergebenen Tags aus dem Quelltext extrahieren. Hier tun wir dies f√ºr den ersten Textparagraphen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ea58c-2777-4e86-aaf3-6201079d3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110bb8c5-7568-48d0-9872-71e832716599",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Die Methode gibt uns das ganze Element inkl. Start- und Endtag (sowie, falls vorhanden, Attribute) zur√ºck. Um den Textinhalt zu isolieren, k√∂nnen wir wie oben bereits demonstriert das Attribut `text` anh√§ngen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349aafe-8b07-4b0e-8c00-d757907d5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paragraph = soup.find(\"p\") #Das Ergebnis der Suche l√§sst sich nat√ºrlich auch zwischenzeitlich in einer Variablen speichern.\n",
    "first_paragraph.text #Sehr komfortabel: 'text' ist ein Attribut sowohl bei 'Response'- als auch bei 'BeautifulSoup'-Objekten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2832de8b-e6f2-420d-97fd-dba37c67ccdb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "`BeautifulSoup` erm√∂glicht es einem auch, √ºber die dot-Notation (vgl. Notebook \"Datenanalyse Teil 1\") auf das erste Element eines gew√ºnschten Tags zuzugreifen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790476b-d40b-4796-8598-bdbc62ac3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p\n",
    "#soup.p.text #Zugriff nur auf den Textinhalt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5267df-10c5-4b72-9b80-31eb7b542601",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Wie wir oben auch schon gesehen haben, k√∂nnen wir neben dem gesuchten Tag auch weitere Suchkriterien angeben, etwa ein bestimmtes Schl√ºssel-Werte-Paar bei den Attributen. Unter den Textparagraphen im gescrapten Artikel gibt es welche, die einer bestimmten `class` zugeordnet sind, etwa der Abschnitt √ºber die Autorin des Artikels. Dieser verf√ºgt √ºber das Attribut `class=\"author-description\"`. Angenommen wir sind an genau dieser Information interessiert, k√∂nnen wir zus√§tzlich zum Tag `<p>` nach dem entsprechenden Attribut suchen.\n",
    "\n",
    "‚ö†Ô∏è Achtung: `class` ist ein \"reserviertes\" Schl√ºsselwort bei Python, es entspricht dem, was wir unter Datentypen verstehen (vgl. Notebooks \"Einf√ºhrung\" und \"Datentypen\"). Um einen Konflikt mit Python zu verhindern, h√§ngt man ganz einfach einen Unterstrich an: `class_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c4065-c92f-4bea-83d9-16fce1dfc1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"p\", class_=\"author-description\").text #Ausgabe nur des Textinhalts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75333ef-f3a6-4534-9c90-ccd6c307197b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Dadurch erhalten wir nicht mehr das erste beliebige `<p>`-Element im Quelltext, sondern das erste, das au√üerdem √ºber das gesuchte Attribut verf√ºgt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be63330-6570-4f73-b7ec-2a9a9a801f4f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 6:** Extrahier die oberste √úberschrift (\"Schlecht vernetzt? Psychische St√∂rungen im Gehirn\") sowie die erste √úberschrift auf der n√§chstniedrigeren Ebene (\"Hirnforschung: Damals und heute\") aus dem Quelltext. Extrahier ebenfalls die erste Bildunterschrift (\"Die Phrenologie (Bildquelle)\"). \n",
    "\n",
    "<details>\n",
    "  <summary>üí° Tipp</summary>\n",
    "  <br>Als Erstes musst Du daf√ºr herausfinden, mit welchen Tags diese Elemente versehen sind. \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67523dc3-b935-4c7e-9ff0-473bfed9908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c4539-7f10-4fa4-b9aa-5e2a03ee5199",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "Wie oben gesehen, erhalten wir durch `find` stets das ganze Element zur√ºck. Dieses Element kann sich irgendwo in der Hierarchie des HTML-Stammbaums befinden. Entsprechend k√∂nnen ihm auch (viele) weitere Elemente untergeordnet sein. Was wir auf `soup` im Gro√üen anwenden k√∂nnen (etwa die Methode `find`), k√∂nnen wir auch auf ein durch `find` zur√ºckgegebenes Element im Kleinen anwenden.\n",
    "\n",
    "Machen wir's konkret: Der in √úbung 6 extrahierten ersten Bildunterschrift ist ein weiteres Element untergeordnet. √úberleg Dir kurz, worum es sich dabei handelt und schau ggf. auf der Webseite nach. \n",
    "\n",
    "Genau, es handelt sich um das (Inline-)Element `<a>`, also um einen Link. Dieses Element extrahieren wir ganz einfach mit derselben Logik wie bis anhin, d.&nbsp;h. wir wenden `find` ein zweites Mal auf das durch `soup.find(\"figcaption\")` zur√ºckgegebene Element an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b785139-d81e-448d-9261-d5ca85dd5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drei Wege zum Ziel\n",
    "print(soup.find(\"figcaption\").find(\"a\")) #Zweimal mithilfe von 'find'\n",
    "print(soup.figcaption.a) #Zweimal mit dot-Notation\n",
    "print(soup.find(\"figcaption\").a) #'find' und dot-Notation lassen sich auch kombinieren (auch umgekehrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512025b-5922-42b3-bb85-2a63c7481cce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Wiederum erhalten wir das ganze Element. \n",
    "\n",
    "Angenommen wir sind nur am Link interessiert, so k√∂nnen wir nicht die bisher verwendete Technik, das `text`-Attribut, einsetzen, um darauf zuzugreifen. Schlie√ülich befindet sich der Link im Wert des `href`-Attributs und nicht im Textinhalt (da steht \"Bildquelle\"). \n",
    "\n",
    "Stattdessen verwenden wir eine Methode namens `get` und √ºbergeben ihr als Argument das gew√ºnschte Attribut (bzw. den Schl√ºssel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382966a-8d8f-45b4-aaca-de0160f2683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.figcaption.a.get(\"href\") #'get' erinnert Dich vielleicht an dictionaries, Attribute von HTML-Elementen sind auch wie dictionaries aufgebaut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be98187-c7a6-4f92-9828-261f446063ec",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Klappt wunderbar!\n",
    "\n",
    "Was, wenn wir nicht blo√ü am ersten Treffer eines bestimmten Suchkriteriums interessiert sind, sondern an allen? Dann kommt die Methode `find_all` ins Spiel. Da die Methode eine Liste mit den gefundenen Elementen zur√ºckgibt, bauen wir sie am besten gleich in eine Schleife ein. \n",
    "\n",
    "Im folgenden, kompakten Code erstellen wir zun√§chst eine Liste mit allen `<a>`-Elementen, also allen Links (`soup.find_all(\"a\")`), iterieren dar√ºber (`for link in ...`) und greifen bei jedem `link` auf den Wert seines `href`-Attributs zu (`link.get(\"href\")`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc500786-3efd-4042-b1a9-db6517b66084",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Beachte den Unterschied in der Schreibweise im Vergleich zur 'findall'-Funktion f√ºr regul√§re Ausdr√ºcke\n",
    "for link in soup.find_all(\"a\"):\n",
    "    print(link.get(\"href\")) \n",
    "    \n",
    "#Verk√ºrzte Syntax f√ºr 'find_all', die 'soup' wie eine Funktion behandelt\n",
    "#for link in soup(\"a\"):\n",
    "    #print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be3002-f1d0-45a4-894b-e6cb9203db6c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Et voil√†: alle Links aus dem Quelltext dieser Seite. \n",
    "\n",
    "Angenommen wir sind nur an den Links aus dem eigentlichen Artikel interessiert, lohnt es sich, statt des kompletten Quelltexts (`soup`) nur den relevanten Teil davon abzusuchen. Dazu m√ºssen wir im Quelltext herausfinden, welches am niedrigsten in der HTML-Hierarchie befindliche Element trotzdem den gesamten Artikelinhalt \"unter sich\" enth√§lt. Zwar gibt es ein Element mit Tag `<article>`, das den Quelltext schon sinnvoll \"zuschneidet\", folgende Tag-Attribut-Kombination ist aber noch etwas spezifischer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b9305-12ee-4179-b6a3-6cfe81ce66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_blog_post = soup.find(\"div\", class_=\"hentry__content post__content hentry__content--is-content post__content--is-content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f189fa10-b682-494a-8bf0-e35bff96b3cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Mit `actual_blog_post` haben wir ein Element, das den relevanten Teil des Quelltexts enth√§lt. Dieses k√∂nnen wir nun wie gewohnt nach Links absuchen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c87dd5-ad96-4969-a362-420c604c245a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for link in actual_blog_post.find_all(\"a\"):\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f18ef-13cf-4e28-b31b-7675846691b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Damit w√§ren wir unserem Ziel schon wesentlich n√§her. Nach demselben Prinzip k√∂nnten wir bei Bedarf noch spezifischer extrahieren, etwa um die Links zum Teilen des Artikels (via Facebook, Xing, etc.) herauszufiltern. \n",
    "\n",
    "In vielen F√§llen macht es Sinn, zun√§chst √ºber `find` ein `BeautifulSoup`-Objekt zu erstellen, das m√∂glichst wenig des gesamten Quelltexts umfasst, aber alles davon, was wir anschlie√üend extrahieren wollen. Es handelt sich dabei um eine Art Vorsortierung, bevor wir uns an die eigentliche Extraktion machen.\n",
    "\n",
    "Genau wie `find` k√∂nnen wir auch `find_all` mehr als ein Suchkriterium √ºbergeben. Stellen wir uns vor, wir sind an allen √úberschriften *innerhalb* des Artikels interessiert. Wie wir in √úbung 6 herausgefunden haben, sind diese Teil eines `<h3>`-Elements. Folgender Code mit nur dem Tag als Suchauftrag ist noch zu unspezifisch, liefert er uns doch auch entsprechende Elemente von au√üerhalb des Artikels (sog. *False Positives*, vgl. \"Die Wahrheitsmatrix\" in Notebook \"Input und Output Teil 1\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de9ed3-a81b-47be-bbd4-6e1eb1a277cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup.find_all(\"h3\")\n",
    "#soup(\"h3\") #Verk√ºrzte Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147c6e3-5dd9-4131-9f4b-5661f319de77",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Ein kurzer Blick in die Ausgabe verr√§t aber, dass alle gew√ºnschten √úberschriften (und nur diese) √ºber das Attribut `class=\"wp-block-heading\"` spezifiziert sind. Bauen wir dieses in den Suchauftrag ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e56b8-d726-4064-baee-500d3acd1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"h3\", class_=\"wp-block-heading\")\n",
    "#soup(\"h3\", class_=\"wp-block-heading\") #Verk√ºrzte Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3ea8f-48ab-40ab-aa30-087a0751e960",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Das klappt hervorragend!\n",
    "\n",
    "Wie h√§tten wir diese sieben √úberschriften noch extrahieren k√∂nnen? \n",
    "\n",
    "Genau, indem wir anstatt `soup` das ma√ügeschneiderte Element `actual_blog_post` abgesucht h√§tten, wodurch das Suchkriterium des Attributs obsolet geworden w√§re:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3794a8-08c5-4822-bd7d-5a255c6a53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#√úberpr√ºfung, ob die beiden Herangehensweisen dasselbe Resultat zeitigen\n",
    "soup.find_all(\"h3\", class_=\"wp-block-heading\") == actual_blog_post.find_all(\"h3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3248b-63ca-4df9-8698-a03a2d0213c6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Neben Tag-Attribut-Kombinationen k√∂nnen wir unsere Suchkriterien bei `find_all` weiter verfeinern und u.&nbsp;a. folgendes √ºbergeben:\n",
    "\n",
    "- eine Liste mit Tags statt einem einzelnen Tag\n",
    "- ein regul√§rer Ausdruck f√ºr Tags bzw. Werte bei Attributen (bzw. Schl√ºsseln; vgl. Notebook \"Regul√§re Ausdr√ºcke\")\n",
    "- eine Liste mit Werten f√ºr ein bestimmtes Attribut (bzw. Schl√ºssel) statt einem einzigen Wert\n",
    "\n",
    "In der folgenden Zelle werden diese Suchkriterien exemplifiziert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d0b35-cb3c-498b-94b0-a57f808e49e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Liste mit Tags statt einem einzelnen Tag\n",
    "for header in soup.find_all([\"h1\", \"h2\", \"h3\"]):\n",
    "    print(header.text.strip()) #Bereinigung von leading/trailing whitespace\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "#Regul√§rer Ausdruck f√ºr Tags (beachte: 'find_all' erwartet einen mittels 're.compile' kompilierten regul√§ren Ausdruck).\n",
    "import re\n",
    "\n",
    "for header in soup.find_all(re.compile(\"h[1-3]\")):\n",
    "    print(header.text.strip()) #Bereinigung von leading/trailing whitespace\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#Liste mit Werten f√ºr 'class'-Attribut\n",
    "for element in soup.find_all(class_=[\"newsletter-area\", \"widgets-after-content\"]):\n",
    "    print(element.text.strip(), \"\\n\") #Bereinigung von leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59779465-f265-4f69-8b81-0529a121d526",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Lass uns das bisher Erlernte zum Extraktionsschritt in die Tat umsetzen!\n",
    "\n",
    "***\n",
    "\n",
    "‚úèÔ∏è **√úbung 7:** In dieser √úbung sollst Du die Kommentare unter dem Artikel extrahieren. Identifizier dazu ein oder mehrere Suchkriterien im Quelltext, anhand derer Du alle Kommentare extrahieren kannst. Konzentrier Dich vorerst auf die Kommentare auf der ersten Ebene (ignorier also Kommentare zu Kommentaren). \n",
    "\n",
    "Lass Dir f√ºr jeden Kommentar folgende Informationen ausgeben:\n",
    "\n",
    "1. Wer hat den Kommentar verfasst?\n",
    "2. Wann wurde der Kommentar verfasst?\n",
    "3. Wie lautet der Text des Kommentars?\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Tipp 1</summary>\n",
    "  <br>Extrahier zun√§chst ein <code>BeautifulSoup</code>-Objekt, das m√∂glichst wenig vom gesamten Quelltext umfasst, aber genug, um alle Kommentare zu enthalten. Arbeite anschlie√üend damit. \n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "  <summary>üí° Tipp 2</summary>\n",
    "  <br>Es gibt verschiedene Kombinationen von Suchkriterien, die Dich ans Ziel bringen. Eine verwendet das <code>class</code>-Attribut, das bei den gew√ºnschten Elementen in mehreren, leicht unterschiedlichen Ausf√ºhrungen vorkommt. Wie kannst Du mehrere Werte f√ºr ein Attribut spezifizieren?\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "  <summary>üí° Tipp 3</summary>\n",
    "    <br>Wendest Du das <code>text</code>-Attribut auf ein Element mit untergeordneten Elementen an, so erh√§ltst Du s√§mtlichen Textinhalt, auch denjenigen der Unterelemente, konkateniert zur√ºck. Setz diese Tatsache produktiv ein!\n",
    "</details>\n",
    "<br>\n",
    "<details><summary>üìå Herausforderung </summary>\n",
    "<br>Extrahier zus√§tzlich die Kommentare zu den Kommentaren und lass sie Dir unterhalb des zugeh√∂rigen Hauptkommentars, wiederum mit Verfasser:in, Datum und Text, ausgeben. Benutz daf√ºr einen regul√§ren Ausdruck.\n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c45b4-75d0-4e07-8556-84dda270f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f31850-3a80-4194-bfcf-fa68b4be786e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "Oben haben wir diverse Schlagzeilen von unterschiedlichen Tagesschau-Ressortseiten mit ein und demselben Code extrahiert. Das hat nur deshalb geklappt, weil die verschiedenen Seiten einheitlich aufgebaut sind. In der n√§chsten √úbung wollen wir sehen, ob dieser Umstand auch f√ºr das Wissenschaftsblog SciLogs gilt (Spoiler: Nat√ºrlich tut es das). \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f433e5b-eb7d-43e8-b26c-aa40bea11cce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "‚úèÔ∏è **√úbung 8:** Begib Dich auf die [SciLogs-Startseite](https://scilogs.spektrum.de) und trag einige Links zu Blogeintr√§gen manuell in `links` zusammen. Ruf in derselben Zelle die Quelltexte zu allen Seiten ab und speichere sie in einer weiteren Liste. \n",
    "\n",
    "Parse anschlie√üend in der zweiten Zelle iterativ einen Quelltext nach dem anderen und extrahier jeweils die Kommentare. Hierf√ºr kannst Du grunds√§tzlich den gleichen Code wie in √úbung 7 verwenden. Pass ihn aber (sofern n√∂tig) so an, dass nun alle Kommentare extrahiert werden, auch jene auf zweiter, dritter etc. Ebene. Stell sicher, dass Du auch wirklich alle Kommentare pro Blogeintrag extrahierst.\n",
    "\n",
    "Lass Dir zus√§tzlich zu den Informationen aus √úbung 7 vor den Kommentaren jeweils den Titel des Blogeintrags ausgeben.\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Tipp</summary>\n",
    "  <br>Verwend wieder das <code>class</code>-Attribut, um nach den Kommentaren zu suchen. Wie schon in √úbung 7 bemerkt, gibt es mehrere m√∂gliche Werte, die Du in den Suchfilter einbauen musst. Da wir jetzt au√üerdem Kommentare auf zweiter, dritter etc. Ebene extrahieren, nimmt die Zahl an m√∂glichen Werten nochmal zu. Zwar kannst Du nach wie vor mit einer Liste an m√∂glichen Werten arbeiten, doch dr√§ngt sich ein regul√§rer Ausdruck stark auf. Solltest Du mit regul√§ren Ausdr√ºcken noch nicht vertraut sein, dann kopier den entsprechenden Code aus der L√∂sung zur Herausforderung bei √úbung 7. \n",
    "</details>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4db0c-8624-422c-a3b1-dc14d8af403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrufschritt\n",
    "links = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749f39c-5a26-4bb0-9952-ce1b356bc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraktionsschritt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ff0f1-a90a-4812-99a9-3e9e642741df",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*** \n",
    "\n",
    "Sehr gut!\n",
    "\n",
    "Nun haben wir gelernt, wie wir *zielgerichtet* relevante Informationen aus langen Quelltexten extrahieren k√∂nnen.\n",
    "\n",
    "Wie versprochen lernen wir zum Abschluss des ersten Teils dieses Notebooks noch einen unkomplizierten Weg kennen, der sich dann anbietet, wenn wir ganz *generell* am Haupttext einer Webseite interessiert sind. Wir arbeiten dazu mit dem Modul `trafilatura`.\n",
    "\n",
    "## Texte sammeln mit `trafilatura`\n",
    "\n",
    "`trafilatura` ist mit den g√§ngigsten HTML-Strukturen vertraut und wei√ü, in welchen Elementen sich √ºblicherweise dieser Haupttext verbirgt. Das Modul trumpft insbesondere bei gro√üen Datensammlungen, in denen der (gesamte) Haupttext von Webseiten im Fokus steht, z.&nbsp;B. wenn wir f√ºr ein Korpus s√§mtliche Texte aller Artikel von ZEIT Online aus einem bestimmten Zeitraum sammeln wollen. \n",
    "\n",
    "`trafilatura` l√§sst sich unabh√§ngig von `requests` und `BeautifulSoup` verwenden, da es sowohl den Abruf- als auch den Extraktionsschritt beherrscht.\n",
    "\n",
    "Installiere `trafilatura` erst √ºber die Command Line (`pip3 install trafilatura`) und setz es anschlie√üend wie folgt ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ebf2f-ffc6-4a22-baed-8723b24b1517",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "downloaded = trafilatura.fetch_url('https://scilogs.spektrum.de/hirn-und-weg/schlecht-vernetzt-psychische-stoerungen-im-gehirn/') #Abrufschritt\n",
    "trafilatura.extract(downloaded) #Extraktionsschritt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a93c48-84ad-4161-a6c3-f9c7cf5e8f86",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Wenn Du durch die Ausgabe scrollst, siehst Du, dass `trafilatura` den Artikeltext (inkl. √úberschriften) sowie die Kommentare extrahiert hat ‚Äì vollkommen ohne einen Blick unsererseits in den Quelltext. \n",
    "\n",
    "`extract` akzeptiert weitere Parameter, √ºber die sich der Extraktionsschritt verfeinern l√§sst, z.&nbsp;B. kann √ºber `output_format` das gew√ºnschte Extraktionsformat spezifiziert werden (u.&nbsp;a. csv oder XML, vgl. zweiter Teil dieses Notebooks). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975edf8f-b3f0-4360-88aa-407b6b8d2324",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "**‚úèÔ∏è √úbung 9:** Benutz `trafilatura`, um Dir die Artikeltexte s√§mtlicher Blogeintr√§ge ausgeben zu lassen, deren URLs Du in √úbung 8 in `links` zusammengetragen hast. Recherchier in der [Dokumentation](https://trafilatura.readthedocs.io/en/latest/corefunctions.html#trafilatura.bare_extraction) von `trafilatura`, wie Du die Kommentare dabei herausfiltern kannst. Lass Dir bei jedem Blogeintrag den Link, die ersten 300 Zeichen des Texts, das Auslassungszeichen [...] sowie die finalen 300 Zeichen ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5796b-9871-48ab-96fe-3323d8252664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde59ad-6482-4a9d-a3a1-86872c2ffc07",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "\n",
    "Damit sind wir am Ende des ersten Teils dieses Notebooks angelangt. Gute Arbeit bis hierhin! üéâ\n",
    "\n",
    "Weiter geht's im zweiten Teil des Notebooks \"Web Scraping\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fa4cc-609f-4ce3-b21b-a2ee97a5b370",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "***\n",
    "<table>\n",
    "      <tr>\n",
    "        <td>\n",
    "            <img src=\"../3_Dateien/Lizenz/CC-BY-SA.png\" width=\"400\">\n",
    "        </td> \n",
    "        <td>\n",
    "            <p>Dieses Notebook sowie s√§mtliche weiteren <a href=\"https://github.com/yannickfrommherz/exdimed-student/tree/main\">Materialien zum Programmierenlernen f√ºr Geistes- und Sozialwissenschaftler:innen</a> sind im Rahmen des Projekts <i>Experimentierraum Digitale Medienkompetenz</i> als Teil von <a href=\"https://tu-dresden.de/gsw/virtuos/\">virTUos</a> entstanden. Erstellt wurden sie von Yannick Frommherz unter Mitarbeit von Anne Josephine Matz. Sie stehen als Open Educational Resource nach <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY SA</a> zur freien Verf√ºgung. F√ºr Feedback und bei Fragen nutz bitte das <a href=\"https://forms.gle/VsYJgy4bZTSqKioA7\">Kontaktformular</a>.\n",
    "        </td>\n",
    "      </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}