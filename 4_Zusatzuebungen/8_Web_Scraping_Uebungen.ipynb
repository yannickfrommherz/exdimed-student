{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07d363b-9fee-4793-87c8-a3739fe73b3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Zusatz√ºbungen zum Notebook \"Web Scraping\"\n",
    "\n",
    "üëÜ Beim Web Scraping (konkret: beim Extrahieren der Daten) wird zumeist mit regul√§ren Ausdr√ºcken gearbeitet. Dabei gibt es oft verschiedene M√∂glichkeiten, einen Suchausdruck zu konstruieren. Bei der Abw√§gung zwischen Genauigkeit (decke ich alle gew√ºnschten F√§lle und nur diese ab?) und Effizienz (wie lange brauche ich, um den regul√§ren Ausdruck zu konstruieren?) gibt es nicht immer Richtig und Falsch.\n",
    "\n",
    "Es ist kein Problem, falls Du noch keine Erfahrung mit regul√§ren Ausdr√ºcken hast, denn alle Aufgaben sind auch ohne dieses Wissen l√∂sbar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf990ba-263a-404f-a1d5-01e52ce48c1a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "‚úèÔ∏è **√úbung 1:** Der in der Code-Zelle gegebene HTML-Code soll folgende Ausgabe erzeugen:\n",
    "\n",
    "![](../3_Dateien/Grafiken_und_Videos/HTML_Code.png)\n",
    "\n",
    "Aktuell fehlen im `<body>`-Element noch einige Tags. √úberleg Dir mithilfe des Bildes, welche das sein k√∂nnten und an welchen Stellen sie fehlen. Erg√§nz sie in den bereits gegebenen `<>`. Du kannst Dir den vervollst√§ndigten Code z. B. auf [dieser Website](https://www.sejda.com/de/html-to-pdf) als PDF anzeigen lassen, um ihn mit dem Screenshot abzugleichen.\n",
    "\n",
    "<details>\n",
    "    <summary>üí° Tipp </summary>\n",
    "    <br>Es fehlen die Tags f√ºr die √úberschriften, Texte und die geordnete Liste. Wenn Du Dir unsicher bist, wie diese geschrieben werden, schau nochmal im Notebook \"Web Scraping Teil 1\" nach. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5465a41-ee32-4d52-8b48-7239f03d7a86",
   "metadata": {
    "tags": []
   },
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"de\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Web Scraping</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <header>\n",
    "        <>Was ist Web Scraping?<> \n",
    "        <>Web Scraping ist ein automatisierter Prozess, bei dem Computerprogramme Daten von Websites extrahieren. Diese Technik wird h√§ufig verwendet, um Informationen aus dem Internet zu sammeln, sei es f√ºr Forschungszwecke, zur Marktforschung, f√ºr Datenanalysen oder f√ºr die Erstellung von ma√ügeschneiderten Inhalten.<>\n",
    "        <>Um Web Scraping durchzuf√ºhren, nutzen Entwickler:innen spezielle Programme oder Skripte, die den HTML-Code einer Webseite analysieren und bestimmte Daten daraus extrahieren. Diese Daten k√∂nnen Texte, Bilder, Preise, Produktbewertungen, Kontaktinformationen oder praktisch jede andere Art von Inhalten sein, die auf einer Website verf√ºgbar sind.<>\n",
    "    </header>\n",
    "    <main>\n",
    "        <section>\n",
    "            <>Worauf muss ich beim Web Scraping achten?<>\n",
    "            <>Bevor Daten von einer Internetseite gescraped werden, muss sich der/die Programmierer:in Gedanken zu folgenden Themen machen:<>\n",
    "            <>\n",
    "                <>Einhaltung der Nutzungsbedingungen, Gesetze, Richtlinien<>\n",
    "                <>Vermeiden einer √úberlastung der Website<>\n",
    "                <>Umgang mit/Scrapen von dynamischen Inhalten<>\n",
    "                <>Datenschutz und pers√∂nliche Daten<>\n",
    "                <>Schnelllebigkeit und Ver√§nderlichkeit der Daten mit der Zeit<>\n",
    "                <>Ethik und Verantwortungsbewusstsein<>\n",
    "            <>\n",
    "        </section>\n",
    "    </main>\n",
    "    <footer>\n",
    "        <p>Quelle: Texte leicht modifiziert √ºbernommen von ChatGPT</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18fd0bb-1ac1-401d-8e2d-5a9481b106ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 2:** Die Website [manova.news](https://manova.news) (ehemals *Rubikon*, [siehe Wikipedia-Eintrag](https://de.wikipedia.org/wiki/Rubikon_(Website))) versteht sich als Nachrichtenportal, verbreitet aber h√§ufig Verschw√∂rungstheorien. Wir wollen die Sprache der Webseite genauer unter die Lupe nehmen. Dazu konzentrieren wir uns zun√§chst auf Schlagzeilen bzw. √úberschriften in der Rubrik [*Natur und Mitwelt*](https://www.manova.news/section/21). \n",
    "\n",
    "Verschaff Dir einen √úberblick √ºber die Seite und √ºberleg Dir, worauf Du beim Scraping *aller* Schlagzeilen bzw. √úberschriften in dieser Rubrik achten musst. Schau dazu im Quelltext nach. Notier Deine Antwort in der folgenden Code-Zelle.\n",
    "    \n",
    "Halt au√üerdem stichpunktartig fest, wie Du beim Abruf- und Extraktionsschritt vorgehen willst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38718b-d81b-4bd8-90ee-38104acb821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Worauf muss geachtet werden?\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''Vorgehen beim Scraping und Extrahieren:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712a06e-5a0d-45e8-8a82-80c2ce4f87c3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 3:** Ruf nun die Seitenquelltexte der gegebenen Rubrik ab und speicher sie in einer Liste. Verwend hier wenn m√∂glich einen regul√§ren Ausdruck. Lass Dir anschlie√üend in der zweiten Code-Zelle zur Kontrolle die ersten 1000 Zeichen des ersten Quelltexts ausgeben.\n",
    "\n",
    "<details>\n",
    "    <summary>üí° Tipp f√ºr die Umsetzung ohne regul√§re Ausdr√ºcke</summary>\n",
    "    <br>Die Nutzung eines f-strings in Verbindung mit einer Schleife sowie einer Z√§hlvariablen bietet sich an, um nacheinander alle Seiten zu scrapen.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbe5f6-6ecb-4784-bd18-46aadaa27d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code f√ºr den Abrufschritt schreiben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a90c8-1008-4a67-88be-073af865a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur Ausgabe der ersten 1000 Zeichen des ersten Quelltexts schreiben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfe1d0-ac75-46b3-8405-a2b7e8c477c3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 4:** Extrahier nun alle Schlagzeilen bzw. √úberschriften der angeteaserten Artikel und lass sie Dir ausgeben.  \n",
    "\n",
    "<details>\n",
    "    <summary>üí° Tipp</summary>\n",
    "    <br>Nutz dazu <code>BeautifulSoup</code> in Verbindung mit der Methode <code>find_all</code>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da53d05-1ac3-45f6-9555-2143aed88c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e5fc6-9b5a-41d7-9f83-f54f5fe3054b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 5:** Da Web Scraping die angerufenen Server immer belastet, sollten wir so selten wie m√∂glich Inhalte herunterladen. Wenn wir mehrfach die gleichen Daten f√ºr ein gr√∂√üeres Projekt ben√∂tigen, sollten wir sie nur einmal herunterladen und anschlie√üend lokal speichern. \n",
    "\n",
    "Die Quelltexte der manova-Seiten sind momentan nur in unserem Arbeitsspeicher (in der Variablen `all_pages`). Sobald wir diese Sitzung (also den Kernel) beenden, gehen sie verloren. Um auch in der n√§chsten Sitzung mit den Daten arbeiten zu k√∂nnen (ohne sie erneut vom Server herunterladen zu m√ºssen), ist es nun Deine Aufgabe, jeden Quelltext in einem separaten HTML-Dokument zu speichern. W√§hl als Speicherort das Verzeichnis \"3_Dateien/Output\".\n",
    "    \n",
    "√úberpr√ºf abschlie√üend √ºber Deinen Dateimanager, ob die neuen Dateien existieren sowie sinnvoll beschrieben wurden.\n",
    "\n",
    "<details>\n",
    "    <summary>üí° Tipp </summary>\n",
    "    <br>Zum externen Speichern von Daten gibt es bei Python verschiedene Wege. Einige hast Du bereits im zweiteiligen Notebook \"Input und Output\" kennengelernt. Du kannst aber auch selbst im Internet suchen, wie Du die Quelltexte am besten speicherst. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13ac78-5340-4987-ac85-fd4b40900d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5acd65-f494-48c4-9247-4138bf30dc99",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53668c-11e0-4318-abab-9be06327beaf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 6:** Nun wollen wir mit den soeben lokal gespeicherten HTML-Dokumenten weiterarbeiten. Lies sie nacheinander ein und extrahier bei jedem angeteaserten Artikel das zugeh√∂rige Schlagwort. Beim Artikel im Screenshot unten w√§re dies \"#WASSERSPEZIAL\" (andere Schlagworte beginnen nicht mit einem Hashtag).\n",
    "  \n",
    "![](../3_Dateien/Grafiken_und_Videos/Schlagwort_manova.png)\n",
    "\n",
    "Ermittle, wie h√§ufig jedes Schlagwort auf den gescrapten Seiten zum Einsatz kommt. Lass Dir das Ergebnis in absteigender Reihenfolge ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6647db-6118-4b60-a68e-c4ee1cc02c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c44e1c-85ab-4258-87f7-a955c622e01a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 7:** Stell analog zu √úbung 6 eine √úbersicht √ºber die flei√üigsten Autor:innen in dieser Rubrik zusammen. Lass Dir die Namen derer ausgeben, die mehr als einen Artikel verfasst haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7e4d8-7b09-43cf-90f6-9447163f2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbc209-237e-441b-b238-a87ec0969f49",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 8:** Bislang haben wir nur die √úberblicksseiten der Rubrik \"Natur und Mitwelt\" gescrapt. Nun wollen wir die eigentlichen Artikeltexte scrapen und daraus ein eigenes Korpus erstellen. \n",
    "\n",
    "Extrahier dazu zun√§chst alle Links auf den √úberblicksseiten, die zu den eigentlichen Artikeln f√ºhren. Speichere die *vollst√§ndigen* Links in einer Liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffac271-da2d-41f3-9524-c7566286dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur √úbung schreiben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1892765-c422-4d78-9012-91f562288bf8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 9:** Lade f√ºr unser Korpus in der ersten Code-Zelle den Quelltext s√§mtlicher Artikel, deren Links Du eben zusammengetragen hast, herunter. Da wir die relevanten Teile davon gleich lokal speichern werden, kannst Du die Quelltexte zwischenzeitlich in Deinem Arbeitsspeicher belassen.\n",
    "\n",
    "Extrahier anschlie√üend in der zweiten Code-Zelle aus jedem Quelltext den **Haupttext** des Artikels. Speichere alle Texte in *einer* XML-Datei im Ordner \"3_Dateien/Output\". Das XML-Dokument wird unserem Korpus entsprechen. Leg dessen hierarchische Struktur so an, dass die Artikeltexte voneinander unterschieden werden k√∂nnen.\n",
    "    \n",
    "<details>\n",
    "    <summary>üí° Tipp 1 </summary>\n",
    "    <br>Verwend <code>trafilatura</code> sowohl f√ºr den Abruf- als auch f√ºr den Extraktionsschritt. Das Modul eignet sich in diesem Fall, da wir nur am Haupttext interessiert sind.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "    <summary>üí° Tipp 2 </summary>\n",
    "    <br>Um die Fortschrittsanzeige, die wir etwa in der L√∂sung zu √úbung 3 verwendet haben, zu professionalisieren, kannst Du das Modul <code>tqdm</code> √ºber die Command Line installieren. Importiere es anschlie√üend mithilfe von <code>from tqdm import tqdm</code> und bau es wie folgt in die <code>for</code>-Schleife zur Iteration √ºber <code>links</code> ein: <code>for i in tqdm(range(len(links)))</code>. Die <code>range</code>-Funktion benutzen wir auch hier, um einen Z√§hler f√ºr die Dateibenennung zu erhalten.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "    <summary>ü¶ä Herausforderung </summary>\n",
    "    <br>Extrahier nicht nur den Haupttext aller Artikel, sondern ebenfalls den Namen des/der Autor:in, den Titel, das Publikationsdatum sowie weitere Metadaten, die Dich interessieren. Um diese Elemente gezielt extrahieren zu k√∂nnen, musst Du die Quelltexte mit <code>requests</code> abrufen und mit <code>BeautifulSoup</code> parsen. Speicher die Metadaten an geeigneter Stelle in der Hierarchie Deines XML-Dokuments. Nutz daf√ºr beispielsweise Attribute. F√ºr diese Herausforderung wird keine L√∂sung vorgegeben.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991fe7ae-bfe9-4854-b714-56c3a35b15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code f√ºr den Abrufschritt schreiben.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86413c-b466-4f5a-b1c3-7f129ace1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In diese Zelle kannst Du den Code zur Extraktion und Speicherung schreiben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54479a18-f6dc-48a2-82b5-d1743773f046",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 10:** Auch wenn es beim Web Scraping nur um die Daten*beschaffung* geht, so bietet folgender vorgegebener Code einen Einblick in die Daten*auswertung*, die m√∂glich ist, jetzt wo wir unser eigenes Korpus zusammengestellt haben. Der Code l√§sst sich ausf√ºhren, wenn Du die XML-Elemente gleich genannt hast, wie in der L√∂sung vorgeschlagen. Du kannst ihn nat√ºrlich an Deine Namensgebung anpassen. \n",
    "\n",
    "Wir verwenden das im Notebook \"Funktionen und Methoden Teil 2\" erstellte Modul `keywords` bzw. die Funktion `get_freqs` daraus. Diese haben wir ja geschrieben, um die H√§ufigkeit von W√∂rtern zu berechnen. Da wir es nun mit einer viel gr√∂√üeren Datenmenge zu tun haben, l√§uft der Code eine Weile. Bau gerne eine Fortschrittsanzeige mit `tqdm` in den Modulcode ein, um die Ausf√ºhrung im Blick behalten zu k√∂nnen. Bedenk aber, dass Du nach dem √Ñndern des Modulcodes den Kernel neustarten musst, um das Modul mit dem ge√§nderten Code zu importieren. \n",
    "    \n",
    "Behalt auch im Hinterkopf, dass wir diesen Code in einem fr√ºheren Stadium unserer Programmierkarriere geschrieben haben. Mittlerweile kennen wir vermutlich effizientere Wege, um W√∂rter auszuz√§hlen. Ungeachtet dessen offenbart sich hier (einmal mehr) der Vorteil von *Code Reuse* und konkret von Modularisierung, wie er im zweiteiligen Notebook \"Funktionen und Methoden\" aufgezeigt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5cb9b-8e78-4fcc-8de3-3458d9795fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einlesen des eben erstellen XML-Dokuments\n",
    "tree = ET.parse(\"../3_Dateien/Output/manova_corpus.xml\")\n",
    "root = tree.getroot() \n",
    "\n",
    "text = \"\" #Initialisieren eines leeren strings, an den wir unten die Artikeltexte h√§ngen \n",
    "\n",
    "#Iteration (rekursiv!) √ºber alle <p>-Elemente und Anh√§ngen des Textinhalts an 'text'\n",
    "for paragraph in root.iter(\"p\"):\n",
    "    text += paragraph.text\n",
    "    \n",
    "#Einlesen der Stoppw√∂rter, die wir auch im Notebook \"Funktionen und Methoden Teil 2\" benutzt haben\n",
    "stopwords_list = []\n",
    "with open(\"../3_Dateien/Koalitionsvertraege/stopwords-de.txt\", encoding=\"utf-8\") as h:\n",
    "    for line in h:\n",
    "        stopwords_list.append(line.rstrip())\n",
    "\n",
    "#Hinzuf√ºgen eines Verzeichnis, in dem Python nach Modulen sucht\n",
    "import sys\n",
    "sys.path.append(\"../3_Dateien/Module\")\n",
    "\n",
    "#Importieren der Funktion 'get_freqs'\n",
    "from keywords import get_freqs\n",
    "\n",
    "#Berechnen der Worth√§ufigkeiten\n",
    "word_frequencies = get_freqs(text, stopwords_list)\n",
    "\n",
    "#Ausgabe der zehn h√§ufigsten W√∂rter\n",
    "print(\"Die zehn h√§ufigsten W√∂rter (exkl. Stoppw√∂rter) sind...\")\n",
    "for word, frequency in word_frequencies[0:10]:\n",
    "    print(word, \"kommt\", frequency, \"vor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9e319-90bd-494b-a54f-d9f7dfdb12fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "<table>\n",
    "      <tr>\n",
    "        <td>\n",
    "            <img src=\"../3_Dateien/Lizenz/CC-BY-SA.png\" width=\"400\">\n",
    "        </td> \n",
    "        <td>\n",
    "            <p>Dieses Notebook sowie s√§mtliche weiteren <a href=\"https://github.com/yannickfrommherz/exdimed-student/tree/main\">Materialien zum Programmierenlernen f√ºr Geistes- und Sozialwissenschaftler:innen</a> sind im Rahmen des Projekts <i>Experimentierraum Digitale Medienkompetenz</i> als Teil von <a href=\"https://tu-dresden.de/gsw/virtuos/\">virTUos</a> entstanden. Erstellt wurden sie von Anne Josephine Matz und Yannick Frommherz. Sie stehen als Open Educational Resource nach <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY SA</a> zur freien Verf√ºgung. F√ºr Feedback und bei Fragen nutz bitte das <a href=\"https://forms.gle/VsYJgy4bZTSqKioA7\">Kontaktformular</a>.\n",
    "        </td>\n",
    "      </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}