{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958eddb3-e696-4db7-a180-4097589d74cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Zusatz√ºbungen zum Notebook \"Datenanalyse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edacc49-c17c-419b-8ede-bf3e2288dc0a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1. Lad Dir den Datensatz mit einer Million S√§tzen aus deutschsprachigen Nachrichtentexten (\"News\") aus dem Jahr 2022 von der Seite des Projekts [Wortschatz Leipzig](https://wortschatz.uni-leipzig.de/de/download/German) herunter und speicher ihn an einem sinnvollen Ort. Entpack die Datei (falls das bei Windows auf Anhieb nicht funktioniert, empfiehlt sich das Programm [WinRAR](https://www.winrar.de)) und lies die Datei \"deu_news_2022_1M-sentences.txt\" mithilfe von pandas ein. Das DataFrame soll ```news_df``` hei√üen und aus zwei Spalten (```sentence_id``` und ```sentence```) sowie einer Million Zeilen bestehen. Spaltennamen kannst Du mithilfe des ```names```-Argument beim Einlesen definieren. Lass Dir die ersten zehn Zeilen ausgeben.\n",
    "  \n",
    "   Befinden sich wirklich eine Million S√§tze im DataFrame? Falls dies bei Deinem DataFrame nicht der Fall ist, √ºberleg Dir, was schief gelaufen sein k√∂nnte, denn in der Datei \"deu_news_2022_1M-sentences.txt\" befinden sich garantiert eine Million S√§tze.\n",
    "\n",
    "   üí° Tipp: Zeilen, die ein √∂ffnendes, aber kein schlie√üendes Anf√ºhrungszeichen beinhalten, f√ºhren dazu, dass pandas \"\\t\" am Ende der Zeile literal anstatt als Trennzeichen interpretiert. Dadurch landen mehrere S√§tze in *einer* Zeile (n√§mlich alle bis zum n√§chsten \"schlie√üenden\" Anf√ºhrungszeichen), weswegen schlussendlich weniger als eine Million Zeilen im DataFrame stehen. Informier Dich in der Dokumentation von pandas √ºber den Parameter ```quoting```, mit dessen Hilfe Du dieses Verhalten √ºbersteuern kannst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae4310-3430-4ae4-89d6-0eb1e74578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77ccd9-64d5-4971-bb5e-e83e5fdddf2e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. Find heraus, ob es in den deutschsprachigen Nachrichten 2022 h√§ufiger um die Ukraine oder Corona ging (unter der Annahme, dass der Datensatz repr√§sentativ f√ºr die deutschsprachigen Nachrichten ist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c83732-9d6f-4a3e-9ced-18997cdfe2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0ed30-784d-41b1-bd04-c685a0e62b8f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Schaff eine zus√§tzliche Spalte im DataFrame, in der die jeweilige L√§nge des Satzes in W√∂rtern verzeichnet ist.\n",
    "\n",
    "   Wie lang ist der l√§ngste Satz? Wie lang ist der k√ºrzeste Satz? Und was ist die durchschnittliche Satzl√§nge?\n",
    "\n",
    "   Lass Dir den l√§ngsten sowie den k√ºrzesten Satz ausgeben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f8fd1-ce81-4a63-960e-85177141aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134217b6-86e7-46a8-a392-454fe0f40fff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "4. Sortier das DataFrame nach der L√§nge der S√§tze in absteigender Reihenfolge. Welcher Schritt ist anschlie√üend noch sinnvoll?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73f820-d529-4de4-bf96-d5ea4ed2e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f543b92-96d4-43af-98b8-52b288ad438c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "5. Bereinige s√§mtliche S√§tze derart, dass Sonderzeichen von allen Wortanf√§ngen und -enden entfernt werden. Dazu steht Dir die Liste ```special_signs``` zur Verf√ºgung. Gro√ü- und Kleinschreibung sollst Du beibehalten. Jeder Satz soll auch nach dem Preprocessing als ein string vorliegen. \n",
    "\n",
    "    üí° Tipp: Verwend die pandas-eigene, vektorisierte Art der Datenbearbeitung und denk daran, dass Du diese auch in Kombination mit selbst definierten Funktionen verwenden kannst.  \n",
    "    \n",
    "    üìå Herausforderung: Verwend maximal eine Zeile zur Bereinigung der S√§tze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff981af-f891-4949-931c-323472eb6093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_signs = ['‚Ä∞', ':', '¬ß', '¬¥', '.', 'ÃÅ', ';', '‚ùì', '‚Äë', '‚Äù', ')', ',', '<', '‚Ä≥', '¬ª', '‚àí', '‚úî', '‚Ä¢', '\"', '`', '„Äâ', '‚Ä†', '*', '>', '&', \"'\", '‚Äπ', '/', '‚Äö', '¬Æ', '¬∞', '‚Äí', '‚ñ∂', '(', '%', '‚Äò', '‚Ç¨', '¬´', '≈Å', '‚ïê', '‚Äû', '!', '‚Äì', '?', '-', 'Ô∏é', '‚Äî', '‚Äú', '¬∑', '‚Ä¶', '‚Äü', '‚Ä°','‚Äô', '$', '≈Ç', '~', '‚Ñ¢', '‚Ä∫', '+']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc22931f-fec9-4454-af94-a7c4f2e74fa4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "6. Mit welchen zehn W√∂rtern beginnen die Nachrichtens√§tze am h√§ufigsten?\n",
    "\n",
    "   üí° Tipp: Um jeweils auf das n-te Element einer Liste in jeder Zeile einer bestimmten Spalte zuzugreifen, kannst Du ```.str[n]``` an den Spaltennamen anh√§ngen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1085d-acae-427f-8c4c-c569ebcf1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15347b0-018b-4686-a8c1-cb68d554af4f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "7. Und mit welchen zehn W√∂rtern enden die Nachrichtens√§tze am h√§ufigsten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd4824-edae-4092-b45a-365aeef3d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecffa3ba-524e-45f9-9b40-e702518c6100",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "8. Wortschatz Leipzig stellt nicht nur eine Million S√§tze zur Verf√ºgung, sondern listet auch auf, wann und von welchem Nachrichtenportal die S√§tze extrahiert wurden. Diese Informationen sind jedoch in zwei weiteren Dateien gespeichert, die Du ebenfalls bereits heruntergeladen und entpackt hast. S√§mtliche Quellen sowie Daten (wann wurde der Satz heruntergeladen?) sind in \"deu_news_2022_1M-sources.txt\" aufgelistet und mit Quellen-IDs versehen, die Zuordnung zwischen Quellen-ID und Satz-ID findet sich wiederum in \"deu_news_2022_1M-inv_so.txt\".\n",
    "\n",
    "    Deine Aufgabe ist es nun, diese Informationen (Quelle und Datum) mit dem DataFrame ```news_df``` zu vereinen. Dazu bietet sich eine Dir vermutlich noch unbekannte Methode namens ```merge``` an. Informier Dich [hier](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) √ºber die Methode. Bei ```merge``` kannst Du mithilfe des ```on```-Parameters spezifizieren, auf Basis der Werte welcher Spalte die Zusammenf√ºhrung zweier DataFrames erfolgen soll. Wichtig ist dabei, dass der angegebene Spaltenname in beiden DataFrames existiert. Stell sicher, dass Du am Ende immer noch eine Million S√§tze in Deinem DataFrame hast, sowie dass keine fehlenden Werte (die etwa aufgrund eines falschen Mergings eingetragen wurden) darin vorkommen. Verwend dazu entweder ```if```-Bedingungen, oder, eleganter, ```assert```-Statements (mehr dazu [hier](https://realpython.com/python-assert-statement/#getting-to-know-assertions-in-python)). \n",
    "    \n",
    "    √úberpr√ºf abschlie√üend bei ein paar Quellenlinks, ob die jeweiligen S√§tze tats√§chlich auf der entsprechenden Website zu finden sind.\n",
    "    \n",
    "    ‚ö†Ô∏è Achtung: Wenn Du mehrfach hintereinander dieselben DataFrames zusammenf√ºgst, riskierst Du einen ```KeyError``` bei der als ```on```-Parameter √ºbergebenen Spalte. Die Fehlermeldung r√ºhrt daher, dass pandas bei mehrmaligem Mergen bereits existierende Spaltennamen um ein Suffix erg√§nzt, da Spaltennamen einzigartig sein m√ºssen. Schaff deshalb als erstes eine Kopie von ```news_df``` mithilfe von ```news_df_enriched = news_df.copy()``` und arbeite fortan mit ```news_df_enriched```. Jedes Mal, wenn Du die ganze Zelle ausf√ºhrst, um die DataFrames zu mergen, passiert dies somit auf Basis einer (immer wieder) neu erstellten Kopie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce126da8-439d-4cf2-881d-b1bc4812c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df_enriched = news_df.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94c799-0fec-4f48-90ab-266796124fdf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "9. F√ºg eine weitere Spalte namens ```country``` hinzu, die basierend auf der Top-Level-Domain dem Quellenlink (z.&nbsp;B. \".de\" oder \".ch\") angibt, aus welchem Land der jeweilige Nachrichtensatz stammt. Dies l√§sst sich am besten mithilfe eines regul√§ren Ausdrucks (vgl. Notebook \"Regul√§re Ausdr√ºcke\") l√∂sen. Pandas wiederum stellt mit ```findall``` eine n√ºtzliche string-Methode zur Verf√ºgung, die wir wie gewohnt auf eine ganze Spalte anwenden k√∂nnen und der wir problemlos einen regul√§ren Ausdruck √ºbergeben k√∂nnen. \n",
    "\n",
    "    üìå Herausforderung: Lass Dir eine sch√∂n formatierte √úbersicht √ºber die absolute und eine relative H√§ufigkeitsverteilung der L√§nder ausgeben. Folgender Screenshot ist eine Idee f√ºr die Formatierung, Du kannst es aber auch anders umsetzen.\n",
    "    \n",
    "    <img src=\"../3_Dateien/Grafiken_und_Videos/tld_overview.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81834c1c-c8df-43e9-8218-50ee2f5000c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df_enriched #\"Zur√ºckverweisen\" von 'news_df_enriched' auf 'news_df', da dies der simplere Variablenname ist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073b44d-1dd5-4147-a915-27e53986188b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "10. Schaff ein Sub-DataFrame mit Nachrichtens√§tzen aus dem [DACH](https://de.wikipedia.org/wiki/D-A-CH)-Raum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d384950-4273-4eea-bfd9-6542bd1377ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111709e8-3dfa-4f77-8a73-8e2ab1d0bdef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "11. Zum Schluss wollen wir noch √ºberpr√ºfen, ob das Sampling der Nachrichtens√§tze im Datensatz einigerma√üen repr√§sentativ f√ºr die deutschsprachigen L√§nder ist, wobei wir uns der Einfachheit halber wieder auf den DACH-Raum beschr√§nken. Deutschland ist ungef√§hr um den Faktor zehn gr√∂√üer als √ñsterreich resp. die Schweiz (√ºberpr√ºf gerne die aktuellen Einwohner:innenzahl der drei L√§nder). Entsprechend sollten zirka zehn mal so viele Nachrichtens√§tze aus Deutschland stammen als aus √ñsterreich bzw. der Schweiz. Gleichzeitig sollte der Anteil an Nachrichtens√§tzen aus Deutschland, √ñsterreich und der Schweiz aber auch einigerma√üen gleichm√§√üig √ºber das Jahr 2022 verteilt sein. Idealerweise wurden nicht blo√ü im Januar Schweizer Quellen, im Februar √∂sterreichische und den Rest des Jahres deutsche gesammelt... Die Verteilung nach Land √ºber die Monate des Jahres 2022 hinweg wollen wir uns als Plot ausgeben lassen. Bevor Du diesen Plot erstellst, f√ºhr folgende Vorbereitungsschritte aus:\n",
    "\n",
    "    - Wenn Du Dir die Daten (wann wurde der Satz heruntergeladen?) in ```news_df``` anschaust (z.&nbsp;B. mittels ```news_df[~news_df.date.str.startswith(\"2022\")])```, siehst Du, dass einige unrealistische Daten dabei sind. Da hat wohl das Web-Scraping bzw. das Postprocessing versagt... Filter das DataFrame derart, dass nur S√§tze aus dem Jahr 2022 √ºbrigbleiben.\n",
    "    - Da wir die Verteilung anstatt √ºber 365 Tage vereinfacht √ºber zw√∂lf Monate hinweg plotten wollen, schaff eine neue Spalte mit dem jeweiligen Extraktionsmonat des Nachrichtensatzes. Sortier das DataFrame anschlie√üend nach den Werten dieser neuen Spalte.\n",
    "    </br></br>\n",
    "    \n",
    "    Analysier nun folgenden Plot und erstell ihn anschlie√üend selbst. √úberleg Dir abschlie√üend, ob das Sampling repr√§sentativ f√ºr die Gr√∂√üe der DACH-L√§nder √ºber die Monate hinweg ist.\n",
    "    \n",
    "    \n",
    "    <img src=\"../3_Dateien/Grafiken_und_Videos/news_dach.png\" width=\"600\"/> </br>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f08e4b-c8d0-4f42-9473-c220974b6922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}