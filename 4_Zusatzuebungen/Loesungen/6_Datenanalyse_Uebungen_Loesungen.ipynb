{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b10def-f13d-45a0-80b8-47cfc99afe53",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# L√∂sungen zu den Zusatz√ºbungen zum Notebook \"Datenanalyse\"\n",
    "\n",
    "‚òùÔ∏è Beachte 1: Es gibt beim Programmieren fast immer verschiedene L√∂sungswege. Deine L√∂sung mag anders aussehen, aber dennoch zum gew√ºnschten Resultat f√ºhren. Das richtige Resultat ist das Wichtigste. \n",
    "\n",
    "‚òùÔ∏è Beachte 2:  In diesem Notebook arbeiten wir gr√∂√ütenteils mit einer Datei, die Du in √úbung 1 selbst herunterladen musst. Die meisten L√∂sungen lassen sich folglich erst ausf√ºhren, wenn Du den korrekten Pfad zur heruntergeladenen Datei bei ```path``` eingesetzt hast. \n",
    "\n",
    "‚òùÔ∏è Beachte 3:  Anders als im zweiteiligen Notebook \"Datenanalyse\" kommt hier auch die dot-Notation f√ºr den Spaltenzugriff (```df.column``` statt ```df[\"column\"]```) zum Einsatz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ce585-0594-4e02-af43-73815d7ff215",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "‚úèÔ∏è **√úbung 1:** Lad Dir den Datensatz mit einer Million S√§tzen aus deutschsprachigen Nachrichtentexten (\"News\") aus dem Jahr 2022 von der Seite des Projekts [Wortschatz Leipzig](https://wortschatz.uni-leipzig.de/de/download/German) herunter und speicher ihn an einem sinnvollen Ort. Entpack die Datei (falls das bei Windows auf Anhieb nicht funktioniert, empfiehlt sich das Programm [WinRAR](https://www.winrar.de)) und lies die Datei \"deu_news_2022_1M-sentences.txt\" mithilfe von pandas ein. Das DataFrame soll ```news_df``` hei√üen und aus zwei Spalten (```sentence_id``` und ```sentence```) sowie einer Million Zeilen bestehen. Spaltennamen kannst Du mithilfe des ```names```-Argument beim Einlesen definieren. Lass Dir die ersten zehn Zeilen ausgeben.\n",
    "  \n",
    "   Befinden sich wirklich eine Million S√§tze im DataFrame? Falls dies bei Deinem DataFrame nicht der Fall ist, √ºberleg Dir, was schief gelaufen sein k√∂nnte, denn in der Datei \"deu_news_2022_1M-sentences.txt\" befinden sich garantiert eine Million S√§tze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e091c30-60f2-4094-8839-ff6a30c63852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd #Importieren des Moduls\n",
    "\n",
    "#Spezifizieren des Pfades zur einzulesenden Datei (hier leer, da individueller Speicherort)\n",
    "path = \"\"\n",
    "\n",
    "\"\"\"Einlesen der Daten, Spezifizieren der Spaltennamen, sowie Steuern des Verhaltens bei Anf√ºhrungszeichen:\n",
    "Der Parameter 'quoting' l√∂st das Problem, dass mehr als ein Satz in einer Zeile landet. Das Problem entsteht bei S√§tzen,\n",
    "die ein √∂ffnendes Anf√ºhrungszeichen beinhalten, aber kein schlie√üendes. pandas interpretiert dadurch die folgenden Trennzeichen '\\t'\n",
    "literal anstatt als Trennzeichen bis zum n√§chsten Anf√ºhrungszeichen. Indem wir 'quoting=3' spezifizieren, teilen wir pandas mit,\n",
    "dass KEINE Anf√ºhrungszeichen beim Einlesen als literal interpretiert werden sollen, vgl. https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html;\n",
    "Anstatt einer omin√∂sen Drei kannst Du auch zus√§tzlich das Modul 'csv' importieren und 'quoting=csv.QUOTE_NONE' spezifizieren,\n",
    "die beiden Optionen sind gleichbedeutend.\"\"\"\n",
    "news_df = pd.read_csv(path, sep=\"\\t\", encoding=\"utf-8\", names=[\"sentence_id\", \"sentence\"], quoting=3)\n",
    "\n",
    "print(news_df.shape) #Ausgabe der Anzahl an Spalten und Zeilen des DataFrame: das DataFrame sollte eine Million Zeilen umfassen\n",
    "news_df.head(10) #Ausgabe der ersten zehn Zeilen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa273c4-bae0-403b-b4b8-b1648fab4f83",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 2:** Find heraus, ob es in den deutschsprachigen Nachrichten 2022 h√§ufiger um die Ukraine oder Corona ging (unter der Annahme, dass der Datensatz repr√§sentativ f√ºr die deutschsprachigen Nachrichten ist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8feff41-6ac4-4134-bd8c-16d1d793cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine = news_df[news_df.sentence.str.contains(\"Ukraine\")] #Filtern des DataFrame nach dem Vorkommen von \"Ukraine\" in der Spalte \"sentence\"\n",
    "corona = news_df[news_df.sentence.str.contains(\"Corona\")] #Filtern des DataFrame nach dem Vorkommen von \"Corona\" in der Spalte \"sentence\"\n",
    "\n",
    "#Ausgabe der L√§nge der beiden Sub-DataFrames\n",
    "print(\"S√§tze mit 'Ukraine':\", len(ukraine))\n",
    "print(\"S√§tze mit 'Corona':\", len(corona))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c511c-69f5-49be-99ea-f00e41480e6e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 3:** Schaff eine zus√§tzliche Spalte im DataFrame, in der die jeweilige L√§nge des Satzes in W√∂rtern verzeichnet ist.\n",
    "\n",
    "   Wie lang ist der l√§ngste Satz? Wie lang ist der k√ºrzeste Satz? Und was ist die durchschnittliche Satzl√§nge?\n",
    "\n",
    "   Lass Dir den l√§ngsten sowie den k√ºrzesten Satz ausgeben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed1b7d-e510-4edc-bb6e-d9cd7fc8c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"length\"] = news_df.sentence.str.count(\" \") + 1 #Z√§hlen der Leerschl√§ge und Addieren von eins, um Anzahl W√∂rter pro Satz auszurechnen\n",
    "print(news_df.length.max(), news_df.length.min(), news_df.length.mean()) #Ausgabe des maximalen, minimalen und durchschnittlichen Werts in der Spalte \"length\"\n",
    "\n",
    "#Filtern des DataFrame danach, dass in der Spale \"length\" der maximale bzw. minmale Wert verzeichnet steht, Ausgabe der Werte mithilfe der 'values'-Methode (um nur die Werte zu erhalten)\n",
    "print(news_df[news_df.length == news_df.length.max()].sentence.values)\n",
    "print(news_df[news_df.length == news_df.length.min()].sentence.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc9ac6-3d4d-4670-91a4-afd87c50979b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 4:** Sortier das DataFrame nach der L√§nge der S√§tze in absteigender Reihenfolge. Welcher Schritt ist anschlie√üend noch sinnvoll?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a8c10-12d9-45da-b352-3c46065209e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sortieren des DataFrame nach den Werten der Spalte \"length\" sowie sinnvollerweise Zur√ºcksetzen der Indizes inkl. \"droppen\" der alten Indizes\n",
    "news_df = news_df.sort_values(\"length\", ascending=False).reset_index(drop=True)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5d7a8-58c4-4223-8d50-9666ef2ee2f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 5:** Bereinige s√§mtliche S√§tze derart, dass Sonderzeichen von allen Wortanf√§ngen und -enden entfernt werden. Dazu steht Dir die Liste ```special_signs``` zur Verf√ºgung. Gro√ü- und Kleinschreibung sollst Du beibehalten. Jeder Satz soll auch nach dem Preprocessing als ein string vorliegen. \n",
    "    \n",
    "<details>\n",
    "    <summary>ü¶ä Herausforderung </summary>\n",
    "    <br>Verwend maximal eine Zeile zur Bereinigung der S√§tze.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518968f-e981-43d7-9126-9d895b0c667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_signs = ['‚Ä∞', ':', '¬ß', '¬¥', '.', 'ÃÅ', ';', '‚ùì', '‚Äë', '‚Äù', ')', ',', '<', '‚Ä≥', '¬ª', '‚àí', '‚úî', '‚Ä¢', '\"', '`', '„Äâ', '‚Ä†', '*', '>', '&', \"'\", '‚Äπ', '/', '‚Äö', '¬Æ', '¬∞', '‚Äí', '‚ñ∂', '(', '%', '‚Äò', '‚Ç¨', '¬´', '≈Å', '‚ïê', '‚Äû', '!', '‚Äì', '?', '-', 'Ô∏é', '‚Äî', '‚Äú', '¬∑', '‚Ä¶', '‚Äü', '‚Ä°','‚Äô', '$', '≈Ç', '~', '‚Ñ¢', '‚Ä∫', '+']\n",
    "specials_signs_str = \"\".join(special_signs) #Casten in string, da strip-Methode einen string mit zu strippenden Zeichen erwartet\n",
    "\n",
    "#Definieren einer eigenen Funktion, die unten auf jeden Satz angewandt wird\n",
    "def preprocessing(sentence):\n",
    "    words = sentence.split(\" \") #Splitten in W√∂rter\n",
    "    #Bereinigen der W√∂rter (leere strings werden durch if-Bedingung √ºbersprungen)\n",
    "    preprocessed_words = [word.strip(specials_signs_str) for word in words if len(word.strip(specials_signs_str)) > 0] \n",
    "    return \" \".join(preprocessed_words) #R√ºckgabe eines wieder als string zusammengesetzten Satzes mit bereinigten W√∂rtern\n",
    "\n",
    "#√úberschreiben der Spalte \"sentence\"\n",
    "news_df[\"sentence\"] = news_df.sentence.apply(preprocessing) #Anwenden der benutzerdefinierten Funktion auf alle Zeilen in der Spalte \"sentence\"\n",
    "\n",
    "#Herausforderung\n",
    "#news_df[\"sentence\"] = news_df.sentence.apply(lambda sentence: \" \".join([word.strip(specials_signs_str) for word in sentence.split(\" \") if len(word.strip(specials_signs_str)) > 0]))\n",
    "\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef371d-c6ca-40a4-9bcd-db2a9ea0dcaf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 6:** Mit welchen zehn W√∂rtern beginnen die Nachrichtens√§tze am h√§ufigsten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d42cda-3359-4e8a-b780-d10ae6d348f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitten der S√§tze in W√∂rter, Indizieren des ersten Elements, Berechnen der H√§ufigkeitsverteilung und Ausgabe der obersten zehn Elemente\n",
    "news_df.sentence.str.split(\" \").str[0].value_counts().head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538e31a-d319-4178-bf95-9ddf90b83db0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 7:** Und mit welchen zehn W√∂rtern enden die Nachrichtens√§tze am h√§ufigsten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe75fa4-a64f-4919-8b67-62e655f476f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitten der S√§tze in W√∂rter, Indizieren des letzten Elements, Berechnen der H√§ufigkeitsverteilung und Ausgabe der obersten zehn Elemente\n",
    "news_df.sentence.str.split(\" \").str[-1].value_counts().head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1472b-d141-4d9e-8de3-856cf46aca02",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 8:** Wortschatz Leipzig stellt nicht nur eine Million S√§tze zur Verf√ºgung, sondern listet auch auf, wann und von welchem Nachrichtenportal die S√§tze extrahiert wurden. Diese Informationen sind jedoch in zwei weiteren Dateien gespeichert, die Du ebenfalls bereits heruntergeladen und entpackt hast. S√§mtliche Quellen sowie Daten (wann wurde der Satz heruntergeladen?) sind in \"deu_news_2022_1M-sources.txt\" aufgelistet und mit Quellen-IDs versehen, die Zuordnung zwischen Quellen-ID und Satz-ID findet sich wiederum in \"deu_news_2022_1M-inv_so.txt\".\n",
    "\n",
    "Deine Aufgabe ist es nun, diese Informationen (Quelle und Datum) mit dem DataFrame ```news_df``` zu vereinen. Dazu bietet sich eine Dir vermutlich noch unbekannte Methode namens ```merge``` an. Informier Dich [hier](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) √ºber die Methode. Bei ```merge``` kannst Du mithilfe des ```on```-Parameters spezifizieren, auf Basis der Werte welcher Spalte die Zusammenf√ºhrung zweier DataFrames erfolgen soll. Wichtig ist dabei, dass der angegebene Spaltenname in beiden DataFrames existiert. Stell sicher, dass Du am Ende immer noch eine Million S√§tze in Deinem DataFrame hast, sowie dass keine fehlenden Werte (die etwa aufgrund eines falschen Mergings eingetragen wurden) darin vorkommen. Verwend dazu entweder ```if```-Bedingungen, oder, eleganter, ```assert```-Statements (mehr dazu [hier](https://realpython.com/python-assert-statement/#getting-to-know-assertions-in-python)). \n",
    "    \n",
    "√úberpr√ºf abschlie√üend bei ein paar Quellenlinks, ob die jeweiligen S√§tze tats√§chlich auf der entsprechenden Website zu finden sind.\n",
    "    \n",
    "‚ö†Ô∏è Achtung: Wenn Du mehrfach hintereinander dieselben DataFrames zusammenf√ºgst, riskierst Du einen ```KeyError``` bei der als ```on```-Parameter √ºbergebenen Spalte. Die Fehlermeldung r√ºhrt daher, dass pandas bei mehrmaligem Mergen bereits existierende Spaltennamen um ein Suffix erg√§nzt, da Spaltennamen einzigartig sein m√ºssen. Schaff deshalb als erstes eine Kopie von ```news_df``` mithilfe von ```news_df_enriched = news_df.copy()``` und arbeite fortan mit ```news_df_enriched```. Jedes Mal, wenn Du die ganze Zelle ausf√ºhrst, um die DataFrames zu mergen, passiert dies somit auf Basis einer (immer wieder) neu erstellten Kopie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e085e-4256-4c0b-b9a2-585ac5418a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df_enriched = news_df.copy()\n",
    "\n",
    "#Spezifizieren der Pfade zur einzulesenden Datei (hier leer, da individueller Speicherort)\n",
    "sources = pd.read_csv(\"\", sep=\"\\t\", quoting=3, names=[\"source_id\", \"source\", \"date\"], encoding=\"utf-8\")\n",
    "mapping = pd.read_csv(\"\", sep=\"\\t\", quoting=3, names=[\"source_id\", \"sentence_id\"], encoding=\"utf-8\")\n",
    "\n",
    "news_df_enriched = news_df_enriched.merge(mapping, on=\"sentence_id\") #Zusammenf√ºhren von 'news_df' und 'mapping', basierend auf Werten der Spalte \"sentence_id\"\n",
    "news_df_enriched = news_df_enriched.merge(sources, on=\"source_id\") #Zusammenf√ºhren von 'news_df' (jetzt mit \"source_id\" aus vorangegangenem Merging) und 'sources', basierend auf Werten der Spalte \"source_id\"\n",
    "\n",
    "#√úberpr√ºfen, ob L√§nge von 'news_df' immer noch eine Million mithilfe des 'assert'-Statements\n",
    "assert len(news_df_enriched) == 1000000\n",
    "\n",
    "\"\"\"√úberpr√ºfen, ob fehlende Werte in 'news_df' vorkommen mithilfe von 'isna()', das ein DataFrame mit True/False f√ºr jede\n",
    "einzelne Zelle ausgibt. False ist bei Python gleichbedeutung mit null (True mit eins), weswegen wir die 'sum'-Methode anh√§ngen k√∂nnen,\n",
    "die standardm√§√üig spaltenweise aufsummiert, wobei immer noch jede Spalte null ergeben sollte. Anschlie√üend k√∂nnen wir auf die\n",
    "resultierende Series noch einmal 'sum()' anwenden, um die Spaltensummen zusammenzuz√§hlen. Ergibt dies immer noch null, finden sich\n",
    "keinerlei fehlende Werte in 'news_df_enriched'. Lass Dir ggf. Zwischenschritte ausgeben, um das Ergebnis nachvollziehen zu k√∂nnen.\"\"\"\n",
    "assert news_df_enriched.isna().sum().sum() == 0\n",
    "\n",
    "#Ausgabe von ein paar Links zur manuellen √úberpr√ºfung, ob S√§tze tats√§chlich aus der jeweiligen Quelle stammen\n",
    "some_indices = [330, 8374, 99473]\n",
    "for index in some_indices:\n",
    "    print(news_df_enriched.loc[index, \"sentence\"], \"sollte von\", news_df_enriched.loc[index, \"source\"], \"stammen. Stimmts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e5cd9-21ff-40d0-b50f-41b2e47e07e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 9:** F√ºg eine weitere Spalte namens ```country``` hinzu, die basierend auf der Top-Level-Domain (TLD) des Quellenlinks (z.&nbsp;B. \".de\" oder \".ch\") angibt, aus welchem Land der jeweilige Nachrichtensatz stammt. Dies l√§sst sich am leichtesten mithilfe eines regul√§ren Ausdrucks (vgl. Notebook \"Regul√§re Ausdr√ºcke\") und der Methode```findall``` umsetzen. Wenn Du bereits Erfahrung mit regul√§ren Ausdr√ºcken hast, probier Dich gern daran aus. Sonst kannst Du die Aufgabe nat√ºrlich aber auch ohne regul√§re Ausdr√ºcke l√∂sen.\n",
    "\n",
    "<details>\n",
    "    <summary>ü¶ä Herausforderung </summary>\n",
    "    <br>Lass Dir eine sch√∂n formatierte √úbersicht √ºber die absolute und eine relative H√§ufigkeitsverteilung der L√§nder ausgeben. Folgender Screenshot ist eine Idee f√ºr die Formatierung, Du kannst es aber auch anders umsetzen:<br><br>\n",
    "    <img src=\"../../3_Dateien/Grafiken_und_Videos/tld_overview.png\" width=\"300\"/>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b4bfa-6a84-43aa-9f70-fbc75587f1d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#L√∂sungsweg ohne RegEx\n",
    "news_df = news_df_enriched #\"Zur√ºckverweisen\" von 'news_df_enriched' auf 'news_df', da dies der simplere Variablenname ist\n",
    "\n",
    "#Schaffen einer neuen Spalte, damit die \"source\"-Spalte im Folgenden nicht unwiderruflich √ºberschrieben wird\n",
    "'''Zun√§chst verr√§t uns ein Blick in die Daten in einem Editor, dass alle Links gleich aufgebaut sind. Nachdem wir an Schr√§gstrichen splitten,\n",
    "sehen wir dass das dritte Element der entstehenden Liste die TLD jeweils als letzte Buchstaben enth√§lt. Wir greifen auf das dritte Element\n",
    "√ºber seinen Index (2) zu.'''\n",
    "news_df[\"country\"] = news_df.source.str.split(\"/\").str[2]\n",
    "\n",
    "'''Nun m√ºssen wir noch am Punkt splitten und das letzte Element, was unserer TLD entspricht, in die Spalte schreiben. \n",
    "Damit ist \"country\" fertig.'''\n",
    "news_df[\"country\"] = news_df.country.str.split(\".\").str[-1]\n",
    "\n",
    "#L√∂sungsweg mit RegEx\n",
    "import re\n",
    "news_df = news_df_enriched #\"Zur√ºckverweisen\" von 'news_df_enriched' auf 'news_df', da dies der simplere Variablenname ist\n",
    "\n",
    "#Definieren der RegEx: ein Punkt (escaped mit Backslash), gefolgt von zwei oder mehr kleingeschriebenen Buchstaben, gefolgt von Schr√§gstrich (escaped mit Backslash)\n",
    "pattern = r\"\\.[a-z]{2,}\\/\"\n",
    "\n",
    "\"\"\"Schaffen einer neuen Spalte, die jeweils das erste Element (Index null) der durch 'findall(pattern)' produzierten Liste \n",
    "mit Regex-Matches beinh√§lt (bereinigt vom Punkt zu Beginn und dem Schr√§gstrich am Ende)\"\"\"\n",
    "news_df[\"country\"] = news_df.source.str.findall(pattern).str[0].str.strip(\"./\") \n",
    "\n",
    "#Herausforderung\n",
    "#Schaffen zweier dictionaries mit den absoluten und relativen H√§ufigkeitsverteilungen (dictionaries eignen sich besser als Series f√ºr die Iteration unten)\n",
    "absolute, relative = dict(news_df.country.value_counts()), dict(news_df.country.value_counts(normalize=True))\n",
    "\n",
    "#Ausgabe einer √úberschrift sowie eines horizontalen Trennbalkens, Formatierung mithilfe von f-strings\n",
    "print(f\"{'TLD':10}{'Absolute':>10}{'Relative':>10}\\n{'-'*30}\")\n",
    "\n",
    "#Iteration √ºber eines der beiden dictionaries, Ausgabe des Schl√ºssels sowie der beiden Werte aus den dictionaries, Formatierung mithilfe von f-strings\n",
    "for key, val in absolute.items():\n",
    "    print(f\"{key:10}{val:10}{relative[key]:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54eb17e-445b-4936-821c-463fd9bf2fe8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 10:** Schaff ein Sub-DataFrame mit Nachrichtens√§tzen aus dem [DACH](https://de.wikipedia.org/wiki/D-A-CH)-Raum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4c221-7629-4498-bff2-c4d8994bfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schaffen eines Sub-DataFrame, indem 'news_df' in der Spalte \"country\" nach den Werten \"de\", \"at\" und \"ch\" gefiltert wird\n",
    "dach_df = news_df[news_df.country.isin([\"de\", \"at\", \"ch\"])]\n",
    "#Alternativ:\n",
    "dach_df.head() #Ausgabe der obersten Zeilen des Sub-DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73e1b1-7db8-4e2d-a7c1-6dd1251124d0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "‚úèÔ∏è **√úbung 11:** Zum Schluss wollen wir noch √ºberpr√ºfen, ob das Sampling der Nachrichtens√§tze im Datensatz einigerma√üen repr√§sentativ f√ºr die deutschsprachigen L√§nder ist, wobei wir uns der Einfachheit halber wieder auf den DACH-Raum beschr√§nken. Deutschland ist ungef√§hr um den Faktor zehn gr√∂√üer als √ñsterreich resp. die Schweiz (√ºberpr√ºf gerne die aktuellen Einwohner:innenzahl der drei L√§nder). Entsprechend sollten zirka zehn mal so viele Nachrichtens√§tze aus Deutschland stammen als aus √ñsterreich bzw. der Schweiz. Gleichzeitig sollte der Anteil an Nachrichtens√§tzen aus Deutschland, √ñsterreich und der Schweiz aber auch einigerma√üen gleichm√§√üig √ºber das Jahr 2022 verteilt sein. Idealerweise wurden nicht blo√ü im Januar Schweizer Quellen, im Februar √∂sterreichische und den Rest des Jahres deutsche gesammelt... Die Verteilung nach Land √ºber die Monate des Jahres 2022 hinweg wollen wir uns als Plot ausgeben lassen. Bevor Du diesen Plot erstellst, f√ºhr folgende Vorbereitungsschritte aus:\n",
    "\n",
    "- Wenn Du Dir die Daten (wann wurde der Satz heruntergeladen?) in ```news_df``` anschaust (z.&nbsp;B. mittels ```news_df[~news_df.date.str.startswith(\"2022\")])```, siehst Du, dass einige unrealistische Daten dabei sind. Da hat wohl das Web-Scraping bzw. das Postprocessing versagt... Filter das DataFrame derart, dass nur S√§tze aus dem Jahr 2022 √ºbrigbleiben.\n",
    "- Da wir die Verteilung anstatt √ºber 365 Tage vereinfacht √ºber zw√∂lf Monate hinweg plotten wollen, schaff eine neue Spalte mit dem jeweiligen Extraktionsmonat des Nachrichtensatzes. Sortier das DataFrame anschlie√üend nach den Werten dieser neuen Spalte.\n",
    "    </br></br>\n",
    "    \n",
    "Analysier nun folgenden Plot und erstell ihn anschlie√üend selbst. √úberleg Dir abschlie√üend, ob das Sampling repr√§sentativ f√ºr die Gr√∂√üe der DACH-L√§nder √ºber die Monate hinweg ist.\n",
    "    \n",
    "    \n",
    "<img src=\"../../3_Dateien/Grafiken_und_Videos/news_dach.png\" width=\"600\"/> </br>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1458ecd-c0ec-4149-9f78-a9ca80983b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #Importieren von 'matplotlib.pyplot'\n",
    "\n",
    "dach_df = dach_df[dach_df.date.str.startswith(\"2022\")] #Wegfiltern von Daten au√üerhalb des Jahres 2022\n",
    "\n",
    "dach_df[\"month\"] = dach_df.date.str.split(\"-\").str[1] #Schaffen einer neuen Spalte mit Monaten durch Splitten nach \"-\" und Indizieren des zweiten Elements\n",
    "\n",
    "dach_df = dach_df.sort_values(\"month\", ascending=True) #Sortieren des DataFrame nach Monat\n",
    "\n",
    "tlds = [\"de\", \"at\", \"ch\"] #Definieren einer Liste mit DACH-TLDs\n",
    "\n",
    "#Iterieren √ºber die TLDs\n",
    "for tld in tlds:\n",
    "    tld_df = dach_df[dach_df.country == tld] #Schaffen eines Sub-DataFrame mit Nachrichtens√§tzen nur aus dem jeweiligen Land\n",
    "    x = tld_df.month.unique() #Definieren der Werte, die auf der x-Achse geplottet werden (einzigartige Werte in der Spalte \"month\", d. h. die zw√∂lf Monate)\n",
    "    \"\"\"Definieren der Werte, die auf der y-Achse geplottet werden, indem das Sub-DataFrame nach Werten in der Spalte \"month\" gruppiert wird und f√ºr jede Gruppe\n",
    "    die Anzahl an Zeilen √ºber 'size()' ermittelt wird. Der Wert jeden Monats wird anschlie√üend durch die Gesamtanzahl an Zeilen im jeweiligen Monat in 'dach_df' geteilt,\n",
    "    um den relativen Anteil des jeweiligen Landes im entsprechenden Monat auszurechnen.\"\"\"\n",
    "    y = tld_df.groupby([\"month\"]).size() / dach_df.groupby([\"month\"]).size()\n",
    "    plt.plot(x, y, 'o-') #Plotting im 'o-'-Stil\n",
    "\n",
    "#Plotten von Titel, Achsenbeschriftungen und Legende\n",
    "plt.title(\"Anzahl an Nachrichtens√§tzen aus den DACH-L√§ndern\")\n",
    "plt.xlabel(\"Monate 2022\")\n",
    "plt.ylabel(\"Anteil an DACH-Quellen/Monat\")\n",
    "plt.legend(tlds, loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e3dfe-b087-4b29-aea3-0cedd06a9405",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Antwort auf abschlie√üende Frage: Das Sampling ist definitiv gleichm√§√üig √ºber die Monate hinweg. Der jeweilige Anteil der L√§nder entspricht einigerma√üen ihrer Gr√∂√üenordnung. Obwohl die Einwohner:innenzahlen von √ñsterreich und der Schweiz nicht allzu weit auseinanderliegen, l√§sst sich der geringere Anteil an Schweizer Quellen im Vergleich zu √∂sterreichischen durch die Schweizer Mehrsprachigkeit (Franz√∂sisch, Italienisch, R√§toromanisch) erkl√§ren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf3d60-1b6e-438e-a6fd-96bf293c85d8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "<table>\n",
    "      <tr>\n",
    "        <td>\n",
    "            <img src=\"../../3_Dateien/Lizenz/CC-BY-SA.png\" width=\"400\">\n",
    "        </td> \n",
    "        <td>\n",
    "            <p>Dieses Notebook sowie s√§mtliche weiteren <a href=\"https://github.com/yannickfrommherz/exdimed-student/tree/main\">Materialien zum Programmierenlernen f√ºr Geistes- und Sozialwissenschaftler:innen</a> sind im Rahmen des Projekts <i>Experimentierraum Digitale Medienkompetenz</i> als Teil von <a href=\"https://tu-dresden.de/gsw/virtuos/\">virTUos</a> entstanden. Erstellt wurden sie von Yannick Frommherz unter Mitarbeit von Anne Josephine Matz. Sie stehen als Open Educational Resource nach <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY SA</a> zur freien Verf√ºgung. F√ºr Feedback und bei Fragen nutz bitte das <a href=\"https://forms.gle/VsYJgy4bZTSqKioA7\">Kontaktformular</a>.\n",
    "        </td>\n",
    "      </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}