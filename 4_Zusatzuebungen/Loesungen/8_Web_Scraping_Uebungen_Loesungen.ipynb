{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b564f25e-d7f5-493e-89a0-0b4ed8053686",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Lösungen zu den Zusatzübungen zum Notebook \"Web Scraping\"\n",
    "\n",
    "☝️ Beachte: Es gibt beim Programmieren fast immer verschiedene Lösungswege. Deine Lösung mag anders aussehen, aber dennoch zum gewünschten Resultat führen. Das richtige Resultat ist das Wichtigste. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff9468-c8b4-449e-afda-689dc6836908",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1. Der in der Code-Zelle gegebene HTML-Code soll folgende Ausgabe erzeugen:\n",
    "\n",
    "    ![](../../3_Dateien/Grafiken_und_Videos/HTML_Code.png)\n",
    "\n",
    "    Aktuell fehlen im `<body>`-Element noch einige Tags. Überleg Dir mithilfe des Bildes, welche das sein könnten und an welchen Stellen sie fehlen. Ergänz sie in den bereits gegebenen `<>`. Du kannst Dir den vervollständigten Code z. B. auf [dieser Website](https://www.sejda.com/de/html-to-pdf) als PDF anzeigen lassen, um ihn mit dem Screenshot abzugleichen."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f5d47dc-5eca-42e9-9050-85e6672a2297",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"de\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Web Scraping</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <header>\n",
    "        <h1>Was ist Web Scraping?</h1> \n",
    "        <p>Web Scraping ist ein automatisierter Prozess, bei dem Computerprogramme Daten von Websites extrahieren. Diese Technik wird häufig verwendet, um Informationen aus dem Internet zu sammeln, sei es für Forschungszwecke, zur Marktforschung, für Datenanalysen oder für die Erstellung von maßgeschneiderten Inhalten.</p>\n",
    "        <p>Um Web Scraping durchzuführen, nutzen Entwickler:innen spezielle Programme oder Skripte, die den HTML-Code einer Webseite analysieren und bestimmte Daten daraus extrahieren. Diese Daten können Texte, Bilder, Preise, Produktbewertungen, Kontaktinformationen oder praktisch jede andere Art von Inhalten sein, die auf einer Website verfügbar sind.</p>\n",
    "    </header>\n",
    "    <main>\n",
    "        <section>\n",
    "            <h2>Worauf muss ich beim Web Scraping achten?</h2>\n",
    "            <p>Bevor Daten von einer Internetseite gescraped werden, muss sich der/die Programmierer:in Gedanken zu folgenden Themen machen:</p>\n",
    "            <ol>\n",
    "                <li>Einhaltung der Nutzungsbedingungen, Gesetze, Richtlinien</li>\n",
    "                <li>Vermeiden einer Überlastung der Website</li>\n",
    "                <li>Umgang mit/Scrapen von dynamischen Inhalten</li>\n",
    "                <li>Datenschutz und persönliche Daten</li>\n",
    "                <li>Schnelllebigkeit und Veränderlichkeit der Daten mit der Zeit</li>\n",
    "                <li>Ethik und Verantwortungsbewusstsein</li>\n",
    "            </ol>\n",
    "        </section>\n",
    "    </main>\n",
    "    <footer>\n",
    "        <p>Quelle: Texte leicht modifiziert übernommen von ChatGPT</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c1d87-697d-4e83-a1d5-25b31c64e547",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. Die Website [manova.news](https://manova.news) (ehemals *Rubikon*, [siehe Wikipedia-Eintrag](https://de.wikipedia.org/wiki/Rubikon_(Website))) versteht sich als Nachrichtenportal, verbreitet aber häufig Verschwörungstheorien. Wir wollen die Sprache der Webseite genauer unter die Lupe nehmen. Dazu konzentrieren wir uns zunächst auf Schlagzeilen bzw. Überschriften in der Rubrik [*Natur und Mitwelt*](https://www.manova.news/section/21). \n",
    "\n",
    "    Verschaff Dir einen Überblick über die Seite und überleg Dir, worauf Du beim Scraping *aller* Schlagzeilen bzw. Überschriften in dieser Rubrik achten musst. Schau dazu im Quelltext nach. Notier Deine Antwort in der folgenden Code-Zelle.\n",
    "    \n",
    "    Halt außerdem stichpunktartig fest, wie Du beim Abruf- und Extraktionsschritt vorgehen willst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e262a-efdc-4c2d-a693-e61c20c2d6c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Worauf muss geachtet werden?\n",
    "Pagination: Die Beiträge sind auf mehrere Unterseiten verteilt und es gibt keine Möglichkeit, alle Beiträge \n",
    "auf einer Seite anzuzeigen. Folglich müssen die Seiten nacheinander abgerufen werden, um wirklich alle Schlagzeilen \n",
    "extrahieren zu können.'''\n",
    "\n",
    "'''Vorgehen beim Abrufen und Extrahieren:\n",
    "1. Im Seitenquelltext herausfinden, wie man von einer Seite auf die jeweils nächste Seite gelangt. Dabei fällt auf, \n",
    "dass im Quelltext jeweils nur die Linkendung angegeben ist, welche noch um den Stammlink ergänzt werden muss \n",
    "(vgl. Notebook \"Web Scraping Teil 1\", Übung 1).\n",
    "2. Coden einer Kontrollstruktur, die die aktuelle Seite scrapt, den Link zur nächsten Seite extrahiert und auf diese\n",
    "Weise alle Seiten bis zur letzten nacheinander scrapt. Anhängen aller Quelltexte an eine Liste.\n",
    "3. Extrahieren der Headlines, Orientierung am Tag <h2>.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a3ad9-437a-495b-a4e6-91bb4be67200",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Ruf nun den Quelltext aller Unterseiten der gegebenen Rubrik ab und speicher deren Quelltexte in einer Liste. Verwend hier wenn möglich einen regulären Ausdruck. Lass Dir anschließend in der zweiten Code-Zelle zur Kontrolle die ersten 1000 Zeichen des ersten Quelltexts ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe21d1-5245-451b-86e2-ede409bf414a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Abrufschritt mit Nutzung regulärer Ausdrücke\n",
    "\n",
    "import re, requests\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "\n",
    "\"\"\"Definieren eines regulären Ausdrucks, um den Link zur jeweils nächsten Seite zu extrahieren;\n",
    "Runde Klammern umschließen Gruppe mit eigentlichem Link; '\\S+' matcht ein oder mehrere Zeichen, \n",
    "die nicht whitespace sind (vgl. Notebook \"Reguläre Ausdrücke\")\"\"\"\n",
    "regex = r'<a class=\"button secondary\" rel=\"next\" href=\"(\\S+)</a>' \n",
    "\n",
    "#Definieren von Stammlink, erster Linkendung sowie des kompletten Links zur ersten zu scrapenden Seite\n",
    "base_link = \"https://www.manova.news\"\n",
    "link_ending = \"/section/21\"\n",
    "link = base_link + link_ending\n",
    "\n",
    "all_pages = [] #Initialisieren einer leeren Liste, an die unten die einzelnen Quelltexte gehängt werden\n",
    "\n",
    "\"\"\"'while'-Schleife scrapt eine paginierte Seite nach der anderen, bis kein Link mehr auf eine nächste Seite\n",
    "im Quelltext gefunden wird\"\"\"\n",
    "while True:\n",
    "    \n",
    "    print(f\"Aktuell wird die Seite {link} gescrapt.\") #Ausgabe des Fortschritts\n",
    "    \n",
    "    current_page = requests.get(link, timeout=5, headers=headers) #Abruf des Quelltexts zum aktuellen Link\n",
    "    \n",
    "    \"\"\"Definieren des Encodings des Quelltexts ('requests' geht unspezifiziert vom falschen Encoding aus, \n",
    "    was sich z. B. an Umlauten zeigt)\"\"\"\n",
    "    current_page.encoding = \"UTF-8\" \n",
    "    \n",
    "    #Anfügen des eigentlichen Quelltexts der aktuellen Seite (Zugriff über 'text'-Attribut) an 'all_pages'\n",
    "    all_pages.append(current_page.text) \n",
    "    \n",
    "    #Quelltext nach regulärem Ausdruck absuchen, der den Link zur nächsten Seite matcht\n",
    "    next_page = re.search(regex, current_page.text)\n",
    "    \n",
    "    #Wenn ein match gefunden wird...\n",
    "    if next_page:\n",
    "        \"\"\"...Definieren des Links für die nächste Seite, indem wir mithilfe der \n",
    "        'group'-Methode auf die erste Gruppe zugreifen und die neue Linkendung an den Stammlink hängen.\"\"\"\n",
    "        link = base_link + next_page.group(1)\n",
    "    else: #Wenn nicht, ist die Rubrik komplett gescrapt und die 'while'-Schleife wird abgebrochen.\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a84976-7e53-4514-a1cd-ef13676b5800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Abrufschritt ohne die Nutzung regulärer Ausdrücke: Dieser Lösungsvorschlag enthält eine hardgecodete Abrufschleife, \n",
    "was die Übertragbarkeit des Codes etwa auf andere Rubriken der Webseite einschränkt. Um das Hardcoding zu vermeiden \n",
    "und trotzdem keine regulären Ausdrücke einzusetzen, könnten wir auch den jeweiligen Quelltext mit 'BeautifulSoup' parsen \n",
    "und die 'find'-Methode verwenden, um das Element mit dem Link auf die jeweils nächste Seite daraus zu extrahieren.\"\"\"\n",
    "\n",
    "import requests\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
    "\n",
    "all_pages = [] #An die Liste werden anschließend die Quelltexte angehängt\n",
    "\n",
    "#hardgecodete Zählschleife mit 'range' von 1 bis 9 (letzte Seitenzahl plus eins, da Endindex exklusive)\n",
    "for number in range(1,9):\n",
    "    link = f\"https://www.manova.news/section/21?page={number}\" #Nutzung eines f-strings, da sich bei der Linksendung nur die Seitenzahl ändert.\n",
    "    \n",
    "    print(f\"Aktuell wird die Seite {link} gescrapt.\")\n",
    "    \n",
    "    current_page = requests.get(link, timeout=5, headers=headers) #Abruf des Quelltexts zum aktuellen Link\n",
    "    \n",
    "    \"\"\"Definieren des Encodings des Quelltexts ('requests' geht unspezifiziert vom falschen Encoding aus, \n",
    "    was sich z. B. an Umlauten zeigt)\"\"\"\n",
    "    current_page.encoding = \"UTF-8\" \n",
    "    \n",
    "    #Anfügen des eigentlichen Quelltexts der aktuellen Seite (Zugriff über 'text'-Attribut) an 'all_pages'\n",
    "    all_pages.append(current_page.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e39b6-dd49-4e42-8fb1-974d43192dbe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ausgabe der ersten 1000 Zeichen des ersten Quelltexts (mit Index null)\n",
    "print(all_pages[0][0:1000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb391b-260d-42aa-8d40-e8542cbca247",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "4. Extrahier nun alle Schlagzeilen bzw. Überschriften der angeteaserten Artikel und lass sie Dir ausgeben.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37582d-d33b-46d0-8563-8fcc777d71af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for page in all_pages: #Iterieren über die Liste 'all_pages'\n",
    "    \n",
    "    soup = BeautifulSoup(page) #Initialisieren eines 'BeautifulSoup'-Objekts mit dem jeweiligen Quelltext\n",
    "    \n",
    "    #Da alle Überschriften im <body>-Element stehen, können wir unser 'BeautifulSoup'-Objekt \"maßschneidern\"\n",
    "    body = soup.find(\"body\")\n",
    "    \n",
    "    #Iterieren über eine Liste mit allen gefundenen <h2>-Elementen, also allen Schlagzeilen bzw. Überschriften\n",
    "    for header in body.find_all(\"h2\"):\n",
    "        print(header.text.strip()) #Ausgabe des Textinhalts jedes Elements, bereinigt von whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242664f2-b08c-4f28-b38b-a7147d960e40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "5. Da Web Scraping die angerufenen Server immer belastet, sollten wir so selten wie möglich Inhalte herunterladen. Wenn wir mehrfach die gleichen Daten für ein größeres Projekt benötigen, sollten wir sie nur einmal herunterladen und anschließend lokal speichern. \n",
    "\n",
    "    Die Quelltexte der manova-Seiten sind momentan nur in unserem Arbeitsspeicher (in der Variablen `all_pages`). Sobald wir diese Sitzung (also den Kernel) beenden, gehen sie verloren. Um auch in der nächsten Sitzung mit den Daten arbeiten zu können (ohne sie erneut vom Server herunterladen zu müssen), ist es nun Deine Aufgabe, jeden Quelltext in einem separaten HTML-Dokument zu speichern. Wähl als Speicherort das Verzeichnis \"3_Dateien/Output\".\n",
    "    \n",
    "    Überprüf abschließend über Deinem Dateimanager, ob die neu geschaffenen Dateien existieren sowie sinnvoll beschrieben wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6647db-6118-4b60-a68e-c4ee1cc02c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Iteration über 'all_pages' mithilfe von 'range', um einen Zähler für die Dateibenennung zu erhalten\n",
    "for i in range(len(all_pages)):\n",
    "    \n",
    "    #Anlegen und Öffnen einer neuen Datei mit 'i' im Dateinamen und \".html\" als Endung\n",
    "    with open(f\"../../3_Dateien/Output/manova_{i+1}.html\", \"w\") as write_file:\n",
    "        \n",
    "        #Schreiben des über 'i' indizierten Quelltexts auf 'all_pages' in das geöffnete 'write_file'\n",
    "        write_file.write(all_pages[i])\n",
    "\n",
    "#Überprüfen, ob das Speichern erfolgreich war, mittels Dateimanager und eines Editors (z. B. Sublime Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941fc78-a962-4859-a47e-e7d778d5812f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "6. Nun wollen wir mit den soeben lokal gespeicherten HTML-Dokumenten weiterarbeiten. Lies sie nacheinander ein und extrahier bei jedem angeteaserten Artikel das zugehörige Schlagwort. Beim Artikel im Screenshot unten wäre dies \"#WASSERSPEZIAL\" (andere Schlagworte beginnen nicht mit einem Hashtag).\n",
    "  \n",
    "    ![](../../3_Dateien/Grafiken_und_Videos/schlagwort_manova.png)\n",
    "\n",
    "    Ermittle, wie häufig jedes Schlagwort auf den gescrapten Seiten zum Einsatz kommt. Lass Dir das Ergebnis in absteigender Reihenfolge ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6abbd-ca4b-4d5d-8e27-f4c244934a4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os #Import von 'os', um über die Dateien im Verzeichnis \"3_Dateien/Output\" zu iterieren\n",
    "\n",
    "path = \"../../3_Dateien/Output/\" #Pfad zum Verzeichnis\n",
    "\n",
    "freq_dict = {} #Initialisieren eines dictionaries, um Schlagworte sowie ihr Vorkommen zu speichern\n",
    "\n",
    "#Iteration über alle Dateien im angegebenen Verzeichnis\n",
    "for file in os.listdir(path):\n",
    "    \n",
    "    #Falls 'file' nicht mit \"manova\" beginnt, überspringen wir die Datei\n",
    "    if not file.startswith(\"manova\"):\n",
    "        continue\n",
    "        \n",
    "    #Öffnen der jeweiligen HTML-Datei durch Konkatenieren von 'path' und 'file'\n",
    "    with open(path + file) as f:\n",
    "        \n",
    "        #Parsen des geöffneten Quelltexts mit 'BeautifulSoup'\n",
    "        soup = BeautifulSoup(f.read())\n",
    "    \n",
    "    \"\"\"Iterieren über eine Liste mit allen Schlagworten, die wir über eine Kombination \n",
    "    von Tag (<div>) und Attribut 'class=\"tag\"' finden können\"\"\"\n",
    "    for tag in soup.find_all(\"div\", class_=\"tag\"):\n",
    "        \n",
    "        #Falls das im Textinhalt versteckte Schlagwort noch kein Schlüssel in 'freq_dict' ist...\n",
    "        if not tag.text in freq_dict:\n",
    "            freq_dict[tag.text] = 1 #legen wir den Schlüssel an und setzen den Wert auf eins\n",
    "        else: #ansonsten erhöhen wir den Wert beim entsprechenden Schlüssel um eins\n",
    "            freq_dict[tag.text] +=1\n",
    "\n",
    "#Sortieren von 'freq_dict' nach Werten             \n",
    "freq_dict_sorted = sorted(freq_dict.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "#Iteration über 'freq_dict_sorted' zur Ausgabe von Schlagworten und Vorkommen\n",
    "for tag, frequency in freq_dict_sorted:\n",
    "    print(tag, \"kommt\", frequency, \"Mal vor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ac31a-3c91-4e31-ab64-a43985f9ff59",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "7. Stell analog zu Übung 6 eine Übersicht über die fleißigsten Autor:innen in dieser Rubrik zusammen. Lass Dir die Namen derer ausgeben, die mehr als einen Artikel verfasst haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd4a74-10c5-4533-a819-98da9a852594",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#erster Teil des Codes analog zu Übung 6, Kommentare s. o.\n",
    "path = \"../../3_Dateien/Output/\" \n",
    "\n",
    "freq_dict = {} \n",
    "\n",
    "for file in os.listdir(path):\n",
    "    \n",
    "    if not file.startswith(\"manova\"):\n",
    "        continue\n",
    "        \n",
    "    with open(path + file) as f:\n",
    "        \n",
    "        soup = BeautifulSoup(f.read())\n",
    "    \n",
    "    \"\"\"Iterieren über eine Liste mit allen Elementen mit dem Tag <div> und dem Attribut\n",
    "    'class=article-meta', in deren untergeordnetem <a>-Element sich der Name der jeweiligen \n",
    "    Autor:in verbirgt\"\"\"\n",
    "    for meta_info in soup.find_all(\"div\", class_=\"article-meta\"):\n",
    "        \n",
    "        author = meta_info.find(\"a\").text #Extrahieren des Textinhalts des (einzigen) untergeordneten <a>-Elements\n",
    "        \n",
    "        #optionale Bereinigung der Namen von whitespace (etwa zwischen Vor- und Nachnamen)\n",
    "        import re\n",
    "        author = re.sub(r\"\\s+\", \" \", author)        \n",
    "        \n",
    "        #Code ab hier bis zur Ausgabe am Ende analog zu Übung 6, Kommentare s. o.\n",
    "        if not author in freq_dict:\n",
    "            freq_dict[author] = 1\n",
    "        else:\n",
    "            freq_dict[author] +=1\n",
    "            \n",
    "freq_dict_sorted = sorted(freq_dict.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "for author, frequency in freq_dict_sorted:\n",
    "    \n",
    "    #Überspringen von Autor:innen, die nur einen Artikel verfasst haben\n",
    "    if frequency == 1:\n",
    "        continue\n",
    "        \n",
    "    print(author, \"schrieb\", frequency, \"Artikel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41e28b-0453-49b6-8f34-073d0127d1e9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "8. Bislang haben wir nur die Überblicksseiten der Rubrik \"Natur und Mitwelt\" gescrapt. Nun wollen wir die eigentlichen Artikeltexte scrapen und daraus ein eigenes Korpus erstellen. \n",
    "\n",
    "    Extrahier dazu zunächst alle Links auf den Überblicksseiten, die zu den eigentlichen Artikeln führen. Speichere die *vollständigen* Links in einer Liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ece6d7-daa2-4e47-b99a-715540f1a49a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#erster Teil des Codes analog zu Übung 6, Kommentare s. o.\n",
    "path = \"../../3_Dateien/Output/\" \n",
    "\n",
    "links = [] #Initialisieren einer Liste, an die wir unten alle Links anhängen\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    \n",
    "    if not file.startswith(\"manova\"):\n",
    "        continue\n",
    "        \n",
    "    with open(path + file) as f:\n",
    "        \n",
    "        soup = BeautifulSoup(f.read())\n",
    "     \n",
    "    \"\"\"Iterieren über eine Liste mit allen Elementen mit dem Tag <h2>, in deren untergeordnetem <a>-Element \n",
    "    sich der Link zum jeweiligen Artikel im 'href'-Attribut verbirgt\"\"\"\n",
    "    for header in soup.find_all(\"h2\"):\n",
    "        \n",
    "        links.append(header.find(\"a\").get(\"href\")) #Extraktion des Werts (d. h. des Links)\n",
    "\n",
    "#Vervollständigung der Links, d. h. Ergänzung um Stammlink mittels List Comprehension\n",
    "links = [f\"https://www.manova.news/{link}\" for link in links]\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9aa7f9-2efe-498b-aedb-7918369c4281",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "9. Lade für unser Korpus in der ersten Code-Zelle den Quelltext sämtlicher Artikel, deren Links Du eben zusammengetragen hast, herunter. Da wir die relevanten Teile davon gleich lokal speichern werden, kannst Du die Quelltexte zwischenzeitlich in Deinem Arbeitsspeicher belassen.\n",
    "\n",
    "    Extrahier anschließend in der zweiten Code-Zelle aus jedem Quelltext den **Haupttext** des Artikels. Speichere alle Texte in *einer* XML-Datei im Ordner \"3_Dateien/Output\". Das XML-Dokument wird unserem Korpus entsprechen. Leg dessen hierarchische Struktur so an, dass die Artikeltexte voneinander unterschieden werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d320c8d-1c48-496d-bd90-e4f49173970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrufschritt\n",
    "from tqdm import tqdm #Import des Moduls für die Fortschrittsanzeige\n",
    "\n",
    "source_codes = [] #Erstellen einer leeren Liste, an die die Quelltexte unten gehängt werden\n",
    "\n",
    "#Iteration über 'links' mithilfe von 'range', um einen Zähler für die Dateibenennung zu erhalten; Fortschrittsanzeige mithilfe von 'tqdm'\n",
    "for i in tqdm(range(len(links))):\n",
    "    \"\"\"Verwenden von 'trafilatura', das zur Extraktion von Haupttexten wunderbar geeignet ist\"\"\"\n",
    "    source_codes.append(trafilatura.fetch_url(links[i])) #Indizieren des jeweiligen Links auf 'links' mit 'i'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064e669-b581-4ccb-b539-f7736a521af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraktions- und Speicherschritt\n",
    "import xml.etree.ElementTree as ET #Import der benötigten XML-Module\n",
    "from xml.dom import minidom\n",
    "\n",
    "corpus = ET.Element(\"corpus\") #Anlegen des obersten Elements in der XML-Hierarchie\n",
    "\n",
    "#Iteration über 'source_codes', mit Fortschrittsanzeige\n",
    "for source_code in tqdm(source_codes):\n",
    "\n",
    "    main_text = trafilatura.extract(source_code) #Extraktion des Haupttexts\n",
    "    \n",
    "    \"\"\"Für jeden Artikel schaffen wir ein Element namens 'article' (zweites Argument der 'SubElement'-Funktion), \n",
    "    das dem 'corpus'-Element untergeordnet sein soll (erstes Argument). Im Hinblick auf die Herausforderung:\n",
    "    Wenn Du das Korpus mit weiteren (Meta-)Daten anreichern möchtest (für deren Extraktion wie gesagt 'requests' und \n",
    "    'BeautifulSoup' nötig wären), dann könntest Du diese Informationen mittels der 'set'-Methode als Attribute von \n",
    "    'article' festlegen. Hier belassen wir es beim Artikeltext.\"\"\"\n",
    "    article = ET.SubElement(corpus, \"article\") \n",
    "    \n",
    "    #Iteration über die einzelnen Paragraphen des jeweiligen Artikeltexts\n",
    "    for paragraph in main_text.split(\"\\n\"):\n",
    "        \n",
    "        \"\"\"Für jeden Paragraphen des jeweiligen Artikels schaffen wir mithilfe derselben 'SubElement'-Funktion \n",
    "        ein Element namens 'p' (zweites Argument), das dem jeweiligen 'article'-Element untergeordnet sein soll \n",
    "        (erstes Argument). Anschließend weisen wir 'p' Textinhalt über das Attribut 'text' zu.\"\"\"\n",
    "        paragraph_xml = ET.SubElement(article, \"p\")\n",
    "        paragraph_xml.text = paragraph\n",
    "        \n",
    "#Schön formatieren, d. h. Einfügen von Einrückungen\n",
    "pretty_corpus = minidom.parseString(ET.tostring(corpus)).toprettyxml(indent=\"  \")\n",
    "\n",
    "#Speichern der XML-Datei\n",
    "with open(\"../../3_Dateien/Output/manova_corpus.xml\", \"w\", encoding=\"utf-8\") as write_file:\n",
    "    write_file.write(pretty_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11504f6-7247-40be-9039-93a06c6d0960",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "10. Auch wenn es beim Web Scraping nur um die Daten*beschaffung* geht, so bietet folgender vorgegebener Code einen Einblick in die Daten*auswertung*, die möglich ist, jetzt wo wir unser eigenes Korpus zusammengestellt haben. Der Code lässt sich ausführen, wenn Du die XML-Elemente gleich genannt hast, wie in der Lösung vorgeschlagen. Du kannst ihn natürlich an Deine Namensgebung anpassen. \n",
    "\n",
    "    Wir verwenden das im Notebook \"Funktionen und Methoden Teil 2\" erstellte Modul `keywords` bzw. die Funktion `get_freqs` daraus. Diese haben wir ja geschrieben, um die Häufigkeit von Wörtern zu berechnen. Da wir es nun mit einer viel größeren Datenmenge zu tun haben, läuft der Code eine Weile. Bau gerne eine Fortschrittsanzeige mit `tqdm` in den Modulcode, um die Ausführung im Blick behalten zu können. Bedenk aber, dass Du nach dem Ändern des Modulcodes den Kernel neustarten musst, um das Modul mit dem geänderten Code zu importieren. \n",
    "    \n",
    "    Behalt auch im Hinterkopf, dass wir diesen Code in einem früheren Stadium unserer Programmierkarriere geschrieben haben. Mittlerweile kennen wir vermutlich effizientere Wege, um Wörter auszuzählen. Ungeachtet dessen offenbart sich hier (einmal mehr) der Vorteil von *Code Reuse* und konkret von Modularisierung, wie er im zweiteiligen Notebook \"Funktionen und Methoden\" aufgezeigt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b2b12-417c-4128-bd5e-0ecb18d9c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einlesen des eben erstellen XML-Dokuments\n",
    "tree = ET.parse(\"../../3_Dateien/Output/manova_corpus.xml\") #Achtung: anderer Pfad als im Lehrnotebook\n",
    "root = tree.getroot() \n",
    "\n",
    "text = \"\" #Initialisieren eines leeren strings, an den wir unten die Artikeltexte hängen\n",
    "\n",
    "#Iteration (rekursiv!) über alle <p>-Elemente und Anhängen des Textinhalts an 'text'\n",
    "for paragraph in root.iter(\"p\"):\n",
    "    text += paragraph.text\n",
    "    \n",
    "#Einlesen der Stoppwörter, die wir auch im Notebook \"Funktionen und Methoden Teil 2\" benutzt haben\n",
    "stopwords_list = []\n",
    "with open(\"../../3_Dateien/Koalitionsvertraege/stopwords-de.txt\", encoding=\"utf-8\") as h: #Achtung: anderer Pfad als im Lehrnotebook\n",
    "    for line in h:\n",
    "        stopwords_list.append(line.rstrip())\n",
    "\n",
    "#Hinzufügen eines Verzeichnis, in dem Python nach Modulen sucht\n",
    "import sys\n",
    "sys.path.append(\"../../3_Dateien/Module\") #Achtung: anderer Pfad als im Lehrnotebook\n",
    "\n",
    "#Importieren der Funktion 'get_freqs'\n",
    "from keywords import get_freqs\n",
    "\n",
    "#Berechnen der Worthäufigkeiten\n",
    "word_frequencies = get_freqs(text, stopwords_list)\n",
    "\n",
    "#Ausgabe der zehn häufigsten Wörter\n",
    "print(\"Die zehn häufigsten Wörter (exkl. Stoppwörter) sind...\")\n",
    "for word, frequency in word_frequencies[0:10]:\n",
    "    print(word, \"kommt\", frequency, \"vor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a091a5f8-6feb-48b8-b3ed-1bbb4cb8469a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "<table>\n",
    "      <tr>\n",
    "        <td>\n",
    "            <img src=\"../../3_Dateien/Lizenz/CC-BY-SA.png\" width=\"400\">\n",
    "        </td> \n",
    "        <td>\n",
    "            <p>Dieses Notebook sowie sämtliche weiteren <a href=\"https://github.com/yannickfrommherz/exdimed-student/tree/main\">Materialien zum Programmierenlernen für Geistes- und Sozialwissenschaftler:innen</a> sind im Rahmen des Projekts <i>Experimentierraum Digitale Medienkompetenz</i> als Teil von <a href=\"https://tu-dresden.de/gsw/virtuos/\">virTUos</a> entstanden. Erstellt wurden sie von Yannick Frommherz unter Mitarbeit von Anne Josephine Matz. Sie stehen als Open Educational Resource nach <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY SA</a> zur freien Verfügung. Für Feedback und bei Fragen nutz bitte das <a href=\"https://forms.gle/VsYJgy4bZTSqKioA7\">Kontaktformular</a>.\n",
    "        </td>\n",
    "      </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}