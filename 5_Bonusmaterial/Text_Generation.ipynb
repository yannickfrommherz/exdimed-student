{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8645d52a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Wie Maschinen Sprache lernen: Textgeneration\n",
    "\n",
    "Hier coden wir einen simplen Textgenerator. Ausgehend von ein paar Wörtern als Input (z.&nbsp;B. \"gegen ende des\") wird er neue Wörter erzeugen, die eine mögliche (bzw. wahrscheinliche) Weiterführung des Textes darstellen (hier z.&nbsp;B. \"[gegen ende des] jahrhunderts könnte die erde laut aktuellem bericht gut 45 prozent\" und so weiter). Solche oder ähnliche Algorithmen finden wir etwa bei Vervollständigungsvorschlägen, wenn wir auf unserem Handy tippen oder bei Google eine Suchanfrage eingeben. Der verwendete Code wurde teils von [Usman Malik](https://stackabuse.com/python-for-nlp-developing-an-automatic-text-filler-using-n-grams/) übernommen.\n",
    "\n",
    "Wir verwenden die Programmiersprache Python für unseren Textgenerator. Sämtlicher Code ist bereits fertig geschrieben und wartet nur auf Deine Ausführung. Kommentare – alles was hinter einem `#` in einer Code-Zelle steht – erläutern Dir den Code. Wenn etwas unklar bleibt, frag den/die Lehrende(n), Deine Nachbarin oder warum nicht chatGPT?\n",
    "\n",
    "Zunächst importieren wir ein paar Module. Führe die folgende Code-Zelle aus, indem Du in sie reinklickst und anschließend `Shift + Enter` drückst. Diesen Befehl sollst Du ab sofort bei jeder Code-Zelle ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7340ed-9f29-4bb5-bfd7-694ab46fb502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657faed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Damit unser Algorithmus Vorhersagen treffen kann, welche Wörter wahrscheinlich auf bestimmte vorangehende Wörter folgen, müssen wir ihm ein Bild menschlicher Sprache (in unserem Fall: Deutsch) vermitteln. Das Bild sollte möglichst umfassend sein. Deshalb nutzen wir einen Datensatz von [Wortschatz Leipzig](https://wortschatz.uni-leipzig.de/de) mit 1 Million Sätzen aus deutschsprachigen Zeitungsartikeln aus dem Jahr 2022.\n",
    "\n",
    "## 1. Daten einlesen\n",
    "\n",
    "Beginnen wir, indem wir den Datensatz in der nächsten Code-Zelle von einem öffentlichen Cloud-Speicher herunterladen. Führe die Zelle wiederum mittels `Shift + Enter` aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528efd7f-8de8-4359-9691-73965a9196fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1dct9Eo0W697cf4Tv3cj1DfAAvThwC9u1 #Download der Trainingsdaten von einer öffentlichen Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e072135-dcb3-4915-b5bc-f9bfd2e79d37",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Im Dateimanager in der linken Seitenleiste sollte sich nun die Datei \"data_1M.txt\" befinden. Wenn Du sie per Doppelklick öffnest, siehst Du, dass es sich um eine Art Tabelle handelt, mit je einer Zahl sowie einem vollständigen Satz pro Zeile.\n",
    "\n",
    "In der folgenden Code-Zelle lesen wir die Datei in den Arbeitsspeicher ein. Wir erstellen ganz einfach eine lange Liste mit Wörtern in der Reihenfolge, wie sie in den einzelnen Sätzen vorkommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859df1e-60b0-41ac-a957-5fe0b09ce7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Öffnen der Datei\n",
    "with open(\"data_1M.txt\") as read_file:\n",
    "    \n",
    "    words = [] #Erstellen einer leeren Liste, an die unten Wort für Wort angehängt wird\n",
    "    \n",
    "    #Iterieren über die geöffnete Datei, d. h. \"Aufrufen\" einer Zeile nach der anderen\n",
    "    #\"tdqm\" sorgt für eine Fortschrittsanzeige während der Iteration\n",
    "    for line in tqdm(read_file):\n",
    "        \n",
    "        #Trennen (Splitten) der Zeile bei jedem Tabulator (\\t), der zwischen Zahl und Satz steht\n",
    "        #Zugriff auf das zweite dabei entstehende Element (also den Satz) mittels [1] (1, da Python bei 0 zu zählen beginnt!)\n",
    "        sentence = line.split(\"\\t\")[1]\n",
    "        \n",
    "        #Bereinigen des Satzes, indem überflüssige Leerzeichen entfernt (strip),\n",
    "        #sämtliche Buchstaben kleingeschrieben (lower) sowie diverse Sonderzeichen entfernt werden (re.sub)\n",
    "        sentence_preprocessed = sentence.strip().lower()\n",
    "        sentence_preprocessed = re.sub(r\"[^A-Za-z0-9ÄäÖöÜüß.,?!\\s\\-]\", \"\", sentence_preprocessed)\n",
    "        \n",
    "        words_per_sentence = nltk.word_tokenize(sentence_preprocessed) #Trennen (Splitten) des Satzes bei jedem Leerzeichen, d.h. Unterteilen nach Wörtern\n",
    "        \n",
    "        words.extend(words_per_sentence) #Anfügen von \"words_per_sentence\" an \"words\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5252c30-683b-4ade-83c1-0b166c1968d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Inspizieren wir mal die eingelesenen Daten und lassen uns die ersten 100 Wörter ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793b77b-2b1b-46ba-a8b0-a9af6cf67686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[0:100]) #Über die eckigen greifen wir auf die ersten 100 Wörter zu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778334b7-e8ba-4fc7-8a4a-eb5a04f36710",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Lass Dir gerne einen anderen Ausschnitt der Wortliste ausgeben, indem Du die \"Start-\" und \"Endzahl\" des Ausschnittes innerhalb der eckigen Klammern anpasst. Würdest Du die eckigen Klammern entfernen, so gäbe Python die gesamte Wortliste aus. Das könnte Deinen Rechner aber überfordern, da die Liste aus über 15 Millionen Wörtern besteht, wie die nächste Code-Zelle errechnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0921cd-9b07-4eb1-93f7-cc045097ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ausgabe der Länge (len) der Wortliste\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942306a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Unser Textgenerator basiert auf sog. *n-Grammen*. N-Gramme sind Sequenzen der Länge *n*. Ein Trigramm etwa besteht aus jeweils drei aufeinanderfolgenden Einheiten. In unserem Fall sind Wörter die Einheit (in anderen Zusammenhängen kann die Einheit aber auch was ganz anderes sein, z.&nbsp;B. Basen [A, C, G, T] bei der Vorhersage von DNA-Strängen). Dieser Textabschnitt etwa beginnt mit dem Wort-Trigramm \"Unser Textgenerator basiert\", gefolgt von \"Textgenerator basiert auf\", \"basiert auf sog.\" usw. \n",
    "\n",
    "Wort-n-Gramme, insbesondere Wort-Trigramme, eignen sich viel besser, um Vorhersagen über folgende Wörter zu treffen, als ein einzelnes Wort, da menschliche Sprache hochgradig musterhaft ist und sich diese Muster typischerweise über mehrere Wörter hinweg manifestieren. Konkret: Auf das einzelne Wort \"gegen\" können sehr viele verschiedene Wörter folgen, auf \"gegen ende\" schon weniger und bei \"gegen ende des\" machen nicht mehr allzu viele Folgewörter Sinn. \n",
    "\n",
    "Entscheidend ist, dass unser Algorithmus wird nicht nur lernen wird, welche Wörter auf welche Wort-Trigramme in den Trainingsdaten folgen, sondern auch, wie häufig ein bestimmtes Wort auf ein gegebenes Wort-Trigramm folgt. Bei \"gegen ende des\" vom Anfang folgt in unseren Trainingsdaten z.&nbsp;B. in 20% der Fälle das Wort \"jahres\", in 11% das Wort \"zweiten\" etc. Unser Algorithmus wird für sämtliche Wort-Trigramme alle darauffolgenden Wörter inkl. ihrer relativen Häufigkeit lernen und dadurch in der Lage sein, neuen Text zu generieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024335f9-0629-409d-961a-aed2ed39b786",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2. Algorithmus trainieren\n",
    "\n",
    "Der nächste Schritt ist das Herzstück des Codes, in dem wir die oben beschriebene Idee implementieren. Zunächst definieren wir ein leeres sog. *dictionary*, das man sich wie ein reales Wörterbuch vorstellen kann. Abstrakt gesehen besteht es also aus Schlüssel-Wert-Paaren. Bei einem Bedeutungswörterbuch entspräche der Schlüssel dem jeweiligen Wort und der zugehörige Wert dessen Bedeutung(en). \n",
    "\n",
    "In unserem Fall schaffen wir für jedes Wort-Trigram, das in den Trainingsdaten vorkommt, einen Schlüssel. Der zugehörige Wert wiederum ist eine Liste mit Wörtern, die jeweils direkt nach dem Wort-Trigramm in den Trainingsdaten vorkommen. Jedes Wort steht dabei so oft auf der Liste, wie es auf das entsprechende Wort-Trigramm im Text folgt. \"gegen ende des\" kommt insgesamt 44 Mal in unseren Trainingsdaten vor. In neun dieser Fälle folgt als nächstes Wort in den Trainingsdaten \"jahres\" (was den o.&nbsp;g. 20% entspricht). Entsprechend sieht das Schlüssel-Werte-Paar von \"gegen ende des\" so aus:\n",
    "\n",
    "`{'gegen ende des': ['15-minütigen', '18', '19', '19', '20', 'artikels', 'beitrags', 'buchs', 'dritten', 'ersten', 'ersten', 'ersten', 'festakts', 'gesprächs', 'halbjahres', 'interviews', 'jahres', 'jahres', 'jahres', 'jahres', 'jahres', 'jahres', 'jahres', 'jahres', 'jahres', 'jahrhunderts', 'jahrzehnts', 'konzertes', 'konzerts', 'krieges', 'monats', 'ns-regimes', 'reiches', 'rennens', 'rennens', 'spiels', 'spiels', 'spiels', 'stints', 'zweiten', 'zweiten', 'zweiten', 'zweiten', 'zweiten']`\n",
    "\n",
    "Wie genau die Wort-Trigramme berechnet werden und wie die jeweils darauffolgenden Wörter als Werte zum jeweiligen Wort-Trigramm in einem dictionary gespeichert werden, wird in den Kommentaren des folgenden Codes erläutert. Führe die Code-Zelle in jedem Fall aus, um unseren Algorithmus zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639a506-a3fd-4414-b8e4-75b52d67493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = {} #Erstellen eines leeren dictionaries, an das unten die Schlüssel-Werte-Paare (Trigram-Liste mit darauffolgenden Wörtern) angehängt werden\n",
    "n = 3 #Definieren der Länge des Wort-n-Gramms\n",
    "\n",
    "#Iterieren über die Indizes von \"words\", d. h. zuerst entspricht \"i\" 0 (Python beginnt bei 0 zu zählen), \n",
    "#dann 1, etc., bis die Länge von \"words\" erreicht wurde; \"tdqm\" sorgt für eine Fortschrittsanzeige während der Iteration\n",
    "for i in tqdm(range(len(words)-n)):\n",
    "    \n",
    "    #Schaffen des jeweiligen Wort-n-Gramms, indem das gegenwärtige Wort (word[i]) sowie alle weiteren Wörter bis Index i plus n zu einem\n",
    "    #string zusammengefügt werden (join)\n",
    "    current_ngram = \" \".join(words[i:i+n])\n",
    "    \n",
    "    #Überprüfen, ob \"current_ngram\" bereits ein Schlüssel in \"ngram\" ist, wenn nein...\n",
    "    if current_ngram not in ngrams.keys():\n",
    "        \n",
    "        #...Schaffen eines neuen Schlüssel-Werte-Paars mit dem jeweiligen Wort-n-Gramm als Schlüssel und zunächst leerer Liste,\n",
    "        #anschließend sowie bei jedem weiteren Vorkommen des gegebenen Wort-n-Gramms wird die Liste um das jeweils darauffolgende Wort\n",
    "        #erweitert (siehe letzter Schritt)\n",
    "        ngrams[current_ngram] = [] \n",
    "    \n",
    "    #Anfügen des Wortes, das auf das gegenwärtige Wort-n-Gramm folgt (also das Wort an Position i+n) an die Liste mit Werten zum jeweiligen Schlüssel\n",
    "    ngrams[current_ngram].append(words[i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05d519-ab70-4882-b5e3-ec43c7bb7e3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Um zu überprüfen, ob das Training geklappt hat, können wir uns etwa die Werte zum Schlüssel \"gegen ende des\" mithilfe folgender Syntax ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6abfdc-2d5a-4092-9032-c78b1ece6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(ngrams[\"gegen ende des\"])) #Zusätzlich alphabetisch sortiert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9859c5c-2054-41ee-9ed8-aa076e513eae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Setz gerne andere Wort-Trigramme zwischen den Anführungszeichen ein, um die entsprechenden Werte zu erhalten. Beachte, dass es sich um exakt drei Wörter handeln muss sowie, dass Du nur Wort-Trigramme abfragen kannst, die auch in den Trainingsdaten vorkamen. Ist dies nicht der Fall, erhältst Du einen `KeyError`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7dabe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3. Text generieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705994c-9419-4147-b770-7d6ca4ed1013",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Zum Schluss wollen wir unseren Algorithmus zur Textgeneration einsetzen. Dazu definieren wir mit `current_ngram` ein Trigramm, das wir dem Algorithmus als Startpunkt für den zu generierenden Text übergeben. Außerdem spezifizieren wir, wie lang der Text maximal sein soll (`len_text`). Der Algorithmus schaut nun zuerst, ob `current_ngram` im dictionary `ngrams` existiert. Wenn nicht, hat er keine Grundlage, um Folgewörter zu prognostizieren und bricht ab (`break`). Falls doch, greift er auf den Wert, der zum Schlüssel `current_ngram` in `ngrams` gespeichert ist, zu. Der Wert entspricht wie gesagt einer Liste mit Wörtern, die in den Trainingsdaten ein oder mehrere Male auf das gegebene Trigramm folgten. Nun wird **zufällig** ein Wort aus dieser Liste als nächstes Wort im generierten Text gewählt. Die Häufigkeit, mit der ein Wort in den Trainingsdaten auf ein bestimmtes Wort-Trigramm folgte und mit der es entsprechend in der Wortliste vertreten ist, korreliert natürlich mit der Wahrscheinlichkeit, mit der es nun zufällig als Folgewort gewählt wird. Abschließend werden die neuen letzten drei Wörter des generierten Textes als Wort-Trigramm definiert, das zur Generation des nächsten Folgeworts (in der nächsten Iteration) verwendet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ab1c5-9d3e-4b70-8730-d88d57e4dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ngram = \"gegen ende des\" #Definieren eines Startpunkts für den zu generierenden Text\n",
    "len_text = 100 #Definieren der maximalen Länge des zu generierenden Texts\n",
    "text = current_ngram #\"text\" beginnt mit \"current_ngram\"\n",
    "\n",
    "#Wiederholen des Generationsprozesses so oft, wie \"len_text\" definiert\n",
    "for i in range(len_text):\n",
    "    \n",
    "    #Überprüfen, ob \"current_ngram\" als Schlüssel in \"ngrams\" vorkommt, wenn nicht wird die Iteration abgebrochen\n",
    "    if current_ngram not in ngrams.keys():\n",
    "        break\n",
    "        \n",
    "    possible_words = ngrams[current_ngram] #Alle Folgewörter auf \"current_ngram\" werden \"possible_words\" zugewiesen\n",
    "    next_word = random.sample(possible_words, 1)[0] #Mithilfe von \"random\" wird zufällig ein Folgewort aus \"possible_words\" gewählt\n",
    "    text += \" \" + next_word #\"text\" wird um ein Leerzeichen und \"next_word\" verlängert\n",
    "    text_words = text.split() #\"text\" wird zu Wortliste umgewandelt, um...\n",
    "    current_ngram = \" \".join(text_words[len(text_words)-n:len(text_words)]) #...die letzten 3 (n) Wörter als neues Trigramm \"current_ngram\" zuzuweisen\n",
    "\n",
    "text = re.sub(r\"(\\s)(\\.|,|\\?|!)(\\s)\", r\"\\2\\3\", text) #Handling von Leerzeichen vor Interpunktion\n",
    "print(text) #Finale Ausgabe des generierten Textes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99324e7-ad51-4ff8-8c4f-15bde3da6831",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Generier verschiedene Texte mit unterschiedlichen Start-Trigrammen! Machen die Texte Sinn? Wo hapert es bei der Textgeneration?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885b02b-892e-4c1b-93b4-335924ef775e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Ein paar Gedanken zum Schluss: Dieser Algorithmus operiert einzig und allein basierend auf Wahrscheinlichkeiten. Er hat natürlich kein Sprachverständnis, wie wir Menschen es haben. Das wird in den generierten Texten offenbar, sobald es um größere Zusammenhänge geht. Da menschliche Sprache aber so musterhaft ist, wird auf den ersten Blick oft doch erstaunlich guter Text generiert. \n",
    "\n",
    "ChatGPT und andere Textgenerations-KIs funktionieren übrigens im Grunde genau gleich: Sie generieren Text, der am wahrscheinlichsten zum eingebenen Prompt passt. Von der Bedeutung des Prompt ebenso wie von der Bedeutung der gelieferten Antwort versteht chatGPT rein gar nichts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}