{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f992dd3-41f4-4fe2-a167-b73c332aa1fe",
   "metadata": {},
   "source": [
    "# Wie Maschinen menschliche Sprache lernen: Word Embeddings üßë‚Äçüíª\n",
    "\n",
    "Heute trainieren wir ein sog. *Sprachmodell*, also ein k√ºnstliches Modell nat√ºrlicher Sprache. Dazu \"f√ºttern\" wir einen darauf spezialisierten Algorithmus mit einer riesigen Menge an Sprachbeispielen. Als Sprachbeispiele verwenden wir einen Datensatz von [Wortschatz Leipzig](https://wortschatz.uni-leipzig.de/de) mit 100.000 S√§tzen aus deutschen Zeitungsartikeln aus dem Jahr 2022. \n",
    "\n",
    "Grob formuliert, schaut sich der Algorithmus die Beispiele immer und immer wieder an und analysiert, wie und wo die einzelnen W√∂rter darin auftreten. √úber die gro√üe Anzahl an Beispielen hinweg, findet der Algorithmus so Beziehungen zwischen den W√∂rtern sowie typische Muster, innerhalb derer sie auftreten (z.B. das vor \"bin\" typischerweise \"ich\" steht). Dadurch lernt der Algorithmus im Idealfall ann√§herungsweise die Bedeutung, die die W√∂rter f√ºr unser menschliches Sprachverst√§ndnis haben. \n",
    "\n",
    "Als Erstes importieren wir Python-Module, die wir zum Training ben√∂tigen und nehmen auch ein paar Einstellungen vor. F√ºhre diese Zelle aus, indem Du in sie reinklickst und anschlie√üend `Shift + Enter` dr√ºckt. Diesen Befehl sollst Du ab sofort bei jeder Code-Zelle ausf√ºhren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05dfb696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fb21a-1dee-40ae-af31-3ad0662f27d7",
   "metadata": {},
   "source": [
    "## 1. Daten einlesen\n",
    "\n",
    "Nun wollen wir unseren Datensatz \"deu_news_2022_100K-sentences.txt\" aus dem Ordner \"data\" einlesen. Wenn Du die Datei mit dem Standardprogramm auf Deinem Rechner √∂ffnest, siehst Du, dass es sich um eine Art Tabelle handelt, mit je einer Zahl sowie einem vollst√§ndigen Satz pro Zeile. \n",
    "\n",
    "Der Algorithmus, den wir zum Trainieren unseres Sprachmodells verwenden, n√§mlich [*word2vec*](https://en.wikipedia.org/wiki/Word2vec), verlangt eine Liste mit S√§tzen, wobei jeder Satz wiederum als Liste mit W√∂rtern erwartet wird. Deshalb tokenisieren wir jeden Satz beim Einlesen und h√§ngen ihn als Wortliste der Satzliste an. Zus√§tzlich entfernen wir die Zahl am Zeilenanfang. F√ºhre die Zelle wiederum mittels `Shift + Enter` aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05540d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:00, 223165.85it/s]\n"
     ]
    }
   ],
   "source": [
    "data = \"data_100K/data/deu_news_2022_100K-sentences.txt\"\n",
    "\n",
    "with open(data, encoding=\"utf-8\") as f:\n",
    "    \n",
    "    #Tokenisieren inkl. Wegsplitten der Indizes am Zeilenanfang\n",
    "    sentences = [sentence.split()[1:] for sentence in tqdm(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34080eb8",
   "metadata": {},
   "source": [
    "Inspizieren wir mal die eingelesenen Daten und lassen uns zwei S√§tze ausgeben. Eckige Klammern begrenzen bei Python Listen, wobei die einzelnen Elemente einer Liste jeweils mit Kommata voneinander abgetrennt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7574fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\"61', 'Gegentore', 'wie', 'in', 'der', 'vergangenen', 'Saison', '-', 'das', 'ist', 'eigentlich', 'abstiegsreif.'], ['\"80', 'Prozent', 'der', 'Landschaften', 'sind', 'in', 'einem', '√∂kologisch', 'schlechten', 'Zustand\",', 'mahnt', 'Wolfgang', 'Lucht,', 'Professor', 'am', 'Potsdam-Institut', 'f√ºr', 'Klimafolgenforschung.']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[10:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73df177-046f-4d16-bdba-49f85502740f",
   "metadata": {},
   "source": [
    "Das sieht schon mal sehr gut aus!\n",
    "\n",
    "Allerdings sehen wir, dass die W√∂rter diverse Zeichen enthalten, die wir beim Training nicht gebrauchen k√∂nnen. Das Anf√ºhrungzeichen hinter 'Zustand\"' etwa hilft dem Algorithmus nicht, sich der Bedeutung von \"Zustand\" anzun√§hern. Ebenso unn√∂tig ist der \".\" am Ende von \"Klimafolgenforschung.\". Wir m√ºssen unsere Daten also bereinigen. Diesen Schritt nennt man *Preprocessing*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f5eee-1ff6-4970-964d-1ab8ea6ffeba",
   "metadata": {},
   "source": [
    "## 2. Daten bereinigen (Preprocessing)\n",
    "\n",
    "Die folgende Zelle definiert die Funktion `strip_special_signs`, die wir in der Zelle darunter auf unsere Daten anwenden. Sie identifiziert in einem ersten Schritt induktiv s√§mtliche nicht-alphanumerischen Zeichen in unseren Daten und entfernt diese im zweiten Schritt von s√§mtlichen Wortanf√§ngen und -enden. Den Code musst Du nicht im Detail nachvollziehen k√∂nnen. Wenn er Dich aber interessiert, kannst Du die Kommentare (alles was mit `#` beginnt) lesen. Sie beschreiben jeweils, was der Code davor bzw. darunter tut. F√ºhre die beiden Zellen in jedem Fall aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac60a54-06f4-4fdd-924f-59a5088baa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_special_signs(list_of_lists):\n",
    "    \n",
    "    #Schritt 1\n",
    "    special_signs = set() #Definieren eines noch leeren Sets (math. Menge), an das wir s√§mtliche Spezialzeichen (Zeichen, die nicht zu den normal_signs geh√∂ren), anh√§ngen\n",
    "    normal_signs = list(\"abcdefghijklmnopqrstuvwxyz√§√∂√º√ü1234567890\") #Schaffen einer Liste mit s√§mtlichen normalen alphanumerischen Zeichen\n",
    "    \n",
    "    print(\"Identifying special signs\") #Ausgabe des aktuellen Schritts\n",
    "    \n",
    "    #Iteration √ºber alle S√§tze...\n",
    "    for sentence in tqdm(sentences):\n",
    "        #...und W√∂rter in unseren Daten\n",
    "        for word in sentence:\n",
    "            #√úberpr√ºfen, ob das erste Zeichen (mit Index 0) beim jeweiligen Wort in der Liste normal_signs ist und wenn NICHT...\n",
    "            if word[0].lower() not in normal_signs:\n",
    "                #...Anh√§ngen des jeweiligen Zeichens an das Set special_signs\n",
    "                special_signs.add(word[0])\n",
    "            #ebenfalls √úberpr√ºfen, ob letztes Zeichen beim jeweiligen Wort in der Liste normal_signs ist und wenn NICHT...\n",
    "            if word[-1].lower() not in normal_signs:  \n",
    "                #Anh√§ngen des jeweiligen Zeichens an das Set special_signs\n",
    "                special_signs.add(word[-1])\n",
    "        \n",
    "    #Schritt 2\n",
    "    print(\"Stripping off special signs\") #Ausgabe des aktuellen Schritts\n",
    "    \n",
    "    preprocessed_sentences = [] #Definieren einer noch leeren Liste, an die wir s√§mtliche bereingten S√§tze anh√§ngen\n",
    "    \n",
    "    #Iteration √ºber alle noch unbereingten S√§tze...\n",
    "    for sentence in tqdm(sentences):\n",
    "        preprocessed_sentence = [] #Definieren einer leeren Liste, an die wir s√§mtliche bereingten W√∂rter EINES Satzes anh√§ngen (Liste wird bei jeder Iteration neu geschaffen)\n",
    "        #...und unbereingten W√∂rter\n",
    "        for word in sentence:\n",
    "            #Entfernen (strip) aller Spezialzeichen am Wortanfang und -ende\n",
    "            preprocessed_word = word.strip(\"\".join(special_signs))\n",
    "            #Sofern Wort l√§nger als null Buchstaben...\n",
    "            if len(preprocessed_word) > 0:\n",
    "                #Anh√§ngen an Liste mit s√§mtlichen bereinigten W√∂rtern\n",
    "                preprocessed_sentence.append(preprocessed_word)\n",
    "        #Anh√§ngen der Liste mit bereingten W√∂rtern an Liste mit bereinigten S√§tzen\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "        \n",
    "    #R√ºckgabe der Liste mit bereingten S√§tzen\n",
    "    return preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1639394f-b9b7-4246-afce-97e64c4534f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 19226/100000 [00:00<00:00, 92863.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying special signs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:01<00:00, 98980.91it/s]\n",
      "  5%|‚ñà‚ñå                                | 4664/100000 [00:00<00:04, 23658.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripping off special signs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:04<00:00, 23257.02it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_sentences = strip_special_signs(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d14413-6e21-47ed-8fb4-f3c47a08ed6e",
   "metadata": {},
   "source": [
    "`preprocessed_sentences` bezeichnet jetzt die Liste mit bereinigten S√§tzen. Schauen wir uns nochmal die gleichen S√§tze wie oben an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070b4331-44a6-4493-9038-8292b599e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['61', 'Gegentore', 'wie', 'in', 'der', 'vergangenen', 'Saison', 'das', 'ist', 'eigentlich', 'abstiegsreif'], ['80', 'Prozent', 'der', 'Landschaften', 'sind', 'in', 'einem', '√∂kologisch', 'schlechten', 'Zustand', 'mahnt', 'Wolfgang', 'Lucht', 'Professor', 'am', 'Potsdam-Institut', 'f√ºr', 'Klimafolgenforschung']]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_sentences[10:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde3fca-b543-446e-a73f-8169c7453c36",
   "metadata": {},
   "source": [
    "Das hat doch wunderbar geklappt. \n",
    "\n",
    "Nun sind unsere Daten bereit f√ºrs Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a4ecb",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609588d",
   "metadata": {
    "tags": []
   },
   "source": [
    "W√§hrend die Bedeutung von W√∂rtern bei Menschen im Sprachzentrum des Gehirns abgespeichert ist, so werden Wortbedeutungen bei word2vec in Form von Vektoren repr√§sentiert, also grob formuliert als Zahlenreihen, z.B. so: \n",
    "\n",
    "    [0.9823969, -0.16720027,  0.69778556, -0.10027876,  0.70647484,  1.0204794, ...].\n",
    "\n",
    "### 3.1. Was ist ein Vektor?\n",
    "\n",
    "Ein Vektor besteht aus einer bestimmten Anzahl an Zahlen, wobei jede Zahl angibt, inwiefern ein bestimmtes *Feature* bei einem gegebenen Wort zutrifft. Vereinfacht kann man sich einen Vektor als eine Art Reihe an numerischen Antworten auf sinnvolle Fragen vorstellen. Die erste Zahl im Vektor (das erste Feature) st√ºnde z.B. immer f√ºr die Anzahl an Buchstaben in einem gegebenen Wort, die zweite Zahl f√ºr die Auftretensh√§ufigkeit im Satz, etc. Ein komplexer Algorithmus wie word2vec kodiert allerdings keine f√ºr Menschen sinnvollen Features (Frage-Antwort-Paare), sondern die abstrakten Beziehungen und Muster zwischen W√∂rtern, die er beim Training entdeckt. Bei gen√ºgend Daten kann ein Modell so durchaus semantisch-syntaktisch sinnvolle Repr√§sentationen von W√∂rtern erlernen. Insgesamt entsteht beim Training ein Vektorraum, in dem die einzelnen Wortvektoren eingebettet sind, weswegen wir auch von *Word Embeddings* sprechen.\n",
    "\n",
    "### 3.2. Wie funktioniert das Training?\n",
    "\n",
    "Grunds√§tzlich unterscheidet man beim Training eines Modells zwischen √ºberwachtem und un√ºberwachtem Lernen (*supervised* vs. *unsupervised learning*). Bei √ºberwachtem Lernen wird ein Algorithmus mit annotierten Daten gef√ºttert, z.B. Bildern von Katzen und anderen Tieren, wobei jedes Bild mit dem Label \"Katze\" oder \"Nicht-Katze\" versehen ist. Ein Katzendetektor-Algorithmus hat beim Training Zugang zur \"Wahrheit\", also zur korrekten Antwort (\"Katze\" oder \"Nicht-Katze\"). Ziel des Trainings ist es, dass der Algorithmus √ºber die Trainingsdaten hinaus zu *generalisieren* lernt, d.h. dass er nach dem Training auch unannotierten Input korrekt als Katze oder Nicht-Katze bestimmen kann. \n",
    "\n",
    "Un√ºberwachtes Lernen hingegen ben√∂tigt keinerlei menschlichen Input, abgesehen von unannotierten Trainingsdaten. Word2vec ist ein un√ºberwachter Algorithmus. F√ºrs Training schafft sich word2vec ganz einfach seine eigenen Trainingsdaten: Basierend auf den vorangehenden und folgenden W√∂rtern innerhalb eines Satzes versucht word2vec ein verdecktes Wort in der Mitte vorherzusagen: Bei \"Der schwarze Hund _____ laut\" w√§re \"bellt\" z.B. eine gute Vorhersage. Die Idee hinter word2vec und Word Embeddings im Allgemeinen ist nun, dass der Algorithmus f√ºr √§hnliche W√∂rter √§hnliche Vektoren erlernt, denn √§hnliche W√∂rter kommen in √§hnlichen Kontexten (mit √§hnlichen vorangehenden und folgenden W√∂rter) vor. \"knurrt\", das ebenfalls ein guter L√ºckenf√ºller w√§re, sollte demnach einen √§hnlichen Vektor wie \"bellt\" haben. Die √Ñhnlichkeit von W√∂rtern gem√§√ü unseres Sprachmodells schauen wir uns unten im Detail an. Zuerst m√ºssen wir das Modell nun aber trainieren.\n",
    "\n",
    "---\n",
    "\n",
    "Wir legen dazu ein paar Parameter fest, u.a.:\n",
    "\n",
    "- `vector_size`, das die Anzahl an Features pro Vektor festlegt.\n",
    "- `window` das festlegt, wie viele W√∂rter vor bzw. nach dem verdeckten Wort als Kontext beim Training ber√ºcksichtigt werden sollen.\n",
    "- `min_count`, das festlegt, wie oft ein Wort im Datensatz mindestens vorkommen muss, um beim Training ber√ºcksichtigt zu werden.\n",
    "- `epochs`, das festlegt, wie oft der Algorithmus den ganzen Datensatz analysieren soll.\n",
    "\n",
    "Und los geht\"s! Das Training nimmt ein paar Sekunden in Anspruch. Es ist fertig, wenn das Sternchen links neben der Zelle unten durch eine Zahl ersetzt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8310964",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 20:05:22,730 : INFO : collecting all words and their counts\n",
      "2023-06-07 20:05:22,732 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-06-07 20:05:22,807 : INFO : PROGRESS: at sentence #10000, processed 157242 words, keeping 32960 word types\n",
      "2023-06-07 20:05:22,861 : INFO : PROGRESS: at sentence #20000, processed 307188 words, keeping 52920 word types\n",
      "2023-06-07 20:05:22,913 : INFO : PROGRESS: at sentence #30000, processed 463034 words, keeping 69805 word types\n",
      "2023-06-07 20:05:22,955 : INFO : PROGRESS: at sentence #40000, processed 613719 words, keeping 84454 word types\n",
      "2023-06-07 20:05:23,016 : INFO : PROGRESS: at sentence #50000, processed 763044 words, keeping 97202 word types\n",
      "2023-06-07 20:05:23,074 : INFO : PROGRESS: at sentence #60000, processed 915275 words, keeping 110022 word types\n",
      "2023-06-07 20:05:23,111 : INFO : PROGRESS: at sentence #70000, processed 1070918 words, keeping 122585 word types\n",
      "2023-06-07 20:05:23,153 : INFO : PROGRESS: at sentence #80000, processed 1227819 words, keeping 134679 word types\n",
      "2023-06-07 20:05:23,198 : INFO : PROGRESS: at sentence #90000, processed 1379600 words, keeping 145439 word types\n",
      "2023-06-07 20:05:23,241 : INFO : collected 155270 word types from a corpus of 1536156 raw words and 100000 sentences\n",
      "2023-06-07 20:05:23,242 : INFO : Creating a fresh vocabulary\n",
      "2023-06-07 20:05:23,397 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 35995 unique words (23.182198750563533%% of original 155270, drops 119275)', 'datetime': '2023-06-07T20:05:23.397213', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-07 20:05:23,397 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1397860 word corpus (90.99726850658396%% of original 1536156, drops 138296)', 'datetime': '2023-06-07T20:05:23.397839', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-07 20:05:23,601 : INFO : deleting the raw counts dictionary of 155270 items\n",
      "2023-06-07 20:05:23,604 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2023-06-07 20:05:23,605 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1141521.314541568 word corpus (81.7%% of prior 1397860)', 'datetime': '2023-06-07T20:05:23.605409', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-07 20:05:23,937 : INFO : estimated required memory for 35995 words and 200 dimensions: 75589500 bytes\n",
      "2023-06-07 20:05:23,937 : INFO : resetting layer weights\n",
      "2023-06-07 20:05:23,970 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-06-07T20:05:23.970280', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-06-07 20:05:23,971 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 35995 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=6 shrink_windows=True', 'datetime': '2023-06-07T20:05:23.971064', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-07 20:05:24,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:24,842 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:24,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:24,844 : INFO : EPOCH - 1 : training on 1536156 raw words (1142078 effective words) took 0.9s, 1318641 effective words/s\n",
      "2023-06-07 20:05:25,674 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:25,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:25,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:25,683 : INFO : EPOCH - 2 : training on 1536156 raw words (1141436 effective words) took 0.8s, 1368933 effective words/s\n",
      "2023-06-07 20:05:26,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:26,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:26,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:26,526 : INFO : EPOCH - 3 : training on 1536156 raw words (1141297 effective words) took 0.8s, 1364249 effective words/s\n",
      "2023-06-07 20:05:27,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:27,387 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:27,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:27,391 : INFO : EPOCH - 4 : training on 1536156 raw words (1141151 effective words) took 0.9s, 1331456 effective words/s\n",
      "2023-06-07 20:05:28,258 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:28,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:28,267 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:28,267 : INFO : EPOCH - 5 : training on 1536156 raw words (1141461 effective words) took 0.9s, 1312252 effective words/s\n",
      "2023-06-07 20:05:28,268 : INFO : Word2Vec lifecycle event {'msg': 'training on 7680780 raw words (5707423 effective words) took 4.3s, 1328286 effective words/s', 'datetime': '2023-06-07T20:05:28.268370', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-07 20:05:28,268 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=35995, vector_size=200, alpha=0.025)', 'datetime': '2023-06-07T20:05:28.268741', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'created'}\n",
      "2023-06-07 20:05:28,269 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2023-06-07 20:05:28,269 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 35995 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=6 shrink_windows=True', 'datetime': '2023-06-07T20:05:28.269695', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-07 20:05:28,998 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:29,002 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:29,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:29,005 : INFO : EPOCH - 1 : training on 1543943 raw words (966363 effective words) took 0.7s, 1331927 effective words/s\n",
      "2023-06-07 20:05:29,732 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:29,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:29,736 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:29,736 : INFO : EPOCH - 2 : training on 1543943 raw words (964832 effective words) took 0.7s, 1328279 effective words/s\n",
      "2023-06-07 20:05:30,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:30,474 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:30,475 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:30,475 : INFO : EPOCH - 3 : training on 1543943 raw words (965629 effective words) took 0.7s, 1329181 effective words/s\n",
      "2023-06-07 20:05:31,194 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:31,198 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:31,199 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:31,199 : INFO : EPOCH - 4 : training on 1543943 raw words (966038 effective words) took 0.7s, 1347869 effective words/s\n",
      "2023-06-07 20:05:31,932 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:31,936 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:31,937 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:31,937 : INFO : EPOCH - 5 : training on 1543943 raw words (965941 effective words) took 0.7s, 1320586 effective words/s\n",
      "2023-06-07 20:05:32,660 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:32,664 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:32,666 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:32,666 : INFO : EPOCH - 6 : training on 1543943 raw words (965949 effective words) took 0.7s, 1337824 effective words/s\n",
      "2023-06-07 20:05:33,402 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:33,407 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:33,408 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:33,408 : INFO : EPOCH - 7 : training on 1543943 raw words (965411 effective words) took 0.7s, 1310376 effective words/s\n",
      "2023-06-07 20:05:34,189 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:34,193 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:34,196 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:34,196 : INFO : EPOCH - 8 : training on 1543943 raw words (965442 effective words) took 0.8s, 1236276 effective words/s\n",
      "2023-06-07 20:05:34,983 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:34,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:34,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:34,988 : INFO : EPOCH - 9 : training on 1543943 raw words (965849 effective words) took 0.8s, 1228150 effective words/s\n",
      "2023-06-07 20:05:35,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-06-07 20:05:35,769 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-06-07 20:05:35,771 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-06-07 20:05:35,772 : INFO : EPOCH - 10 : training on 1543943 raw words (965755 effective words) took 0.8s, 1243265 effective words/s\n",
      "2023-06-07 20:05:35,772 : INFO : Word2Vec lifecycle event {'msg': 'training on 15439430 raw words (9657209 effective words) took 7.5s, 1287169 effective words/s', 'datetime': '2023-06-07T20:05:35.772836', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 12:59:45) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.7-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9657209, 15439430)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(preprocessed_sentences, vector_size=200, window=6, min_count=3) \n",
    "model.train(sentences, total_examples=len(sentences), epochs=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97d83e",
   "metadata": {},
   "source": [
    "Nun ist das Modell fertig trainiert.\n",
    "\n",
    "Schauen wir uns mal den Vektor des Worts \"Universit√§t\" an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7623742-215b-44c2-935b-633169e8e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.76908806e-01 -6.20428979e-01  8.04425418e-01  5.98221958e-01\n",
      "  3.54229093e-01  5.86936712e-01  4.21542190e-02 -2.70387679e-01\n",
      " -8.12268853e-01  2.17557669e-01 -5.38275652e-02 -6.42810389e-02\n",
      " -6.98471427e-01 -1.42438188e-01 -1.74809113e-01 -6.62084669e-02\n",
      "  7.37658739e-01  1.70509338e-01  1.81972817e-01 -6.62572622e-01\n",
      "  1.02246296e+00 -2.77587742e-01  6.45766497e-01 -5.47019899e-01\n",
      " -6.13124013e-01  1.21455632e-01  5.43121621e-02 -1.27894491e-01\n",
      "  1.28955245e-01 -6.39330447e-01 -1.07921518e-01  2.71578282e-02\n",
      "  3.75380576e-01  5.06270647e-01  3.71149749e-01  5.35258591e-01\n",
      "  1.58502474e-01  5.89708805e-01  2.85069108e-01 -1.82113066e-01\n",
      "  7.29672194e-01 -4.83986050e-01  5.47285020e-01  7.11392105e-01\n",
      "  4.93690223e-01 -1.01192132e-01  5.81915915e-01 -7.60676324e-01\n",
      "  7.19942987e-01  8.66918623e-01 -5.15697658e-01 -2.63820022e-01\n",
      " -4.75525558e-01 -6.10494435e-01  1.15849838e-01 -1.29498827e+00\n",
      " -5.11104763e-01 -1.66428000e-01  3.17165852e-01 -6.09246969e-01\n",
      "  1.87628239e-01 -3.39562446e-01  8.81582737e-01  3.99406403e-01\n",
      "  1.92993367e-03 -2.05622837e-01 -1.25249580e-01  2.30905175e-01\n",
      " -3.76942456e-01 -1.49041846e-01 -3.52808237e-01 -1.79616660e-01\n",
      " -4.17706184e-02  2.13403940e-01  1.24605298e-01 -2.77769268e-01\n",
      "  4.64767247e-01  4.78369713e-01 -2.70648092e-01 -4.89047647e-01\n",
      " -4.91140842e-01 -9.14272666e-01 -5.09720221e-02  1.75296009e-01\n",
      "  1.69335619e-01 -2.11905271e-01  2.09123209e-01 -2.45915130e-01\n",
      " -2.18536947e-02 -1.43131912e-02 -3.88018265e-02 -2.78031111e-01\n",
      "  1.38372108e-01  2.94750766e-03  2.49838293e-01  1.37528004e-02\n",
      "  4.50554192e-01  4.75404471e-01 -5.28318249e-02  3.21628034e-01\n",
      " -9.03609470e-02 -6.35120153e-01 -2.08446726e-01 -3.76792908e-01\n",
      "  1.41428992e-01 -7.86477029e-01  1.58576876e-01  6.58388793e-01\n",
      " -3.91995758e-01 -7.57272482e-01 -4.19914484e-01 -5.02663434e-01\n",
      " -2.39528432e-01 -4.30838987e-02  9.02699769e-01  8.15864187e-03\n",
      " -3.42357576e-01 -2.04792663e-01  2.56498188e-01 -5.96869171e-01\n",
      " -2.09544264e-02  3.04900348e-01  6.04591966e-01 -1.98585436e-01\n",
      " -7.30962694e-01  3.73976916e-01 -1.11278141e+00  4.63381022e-01\n",
      "  2.49623626e-01 -8.30903351e-01  5.94646096e-01  2.04233512e-01\n",
      "  4.67983276e-01 -2.03277886e-01 -7.63474166e-01  8.16298544e-01\n",
      "  4.52142507e-01  4.90774482e-01  6.92679062e-02 -9.48911346e-03\n",
      "  4.86782372e-01  2.44626164e-01  4.77495790e-01  1.45432979e-01\n",
      "  1.08897924e-01 -6.51017157e-03  3.60984653e-02 -2.22645685e-01\n",
      " -5.06347835e-01  2.11459328e-03 -4.01090056e-01 -7.30002224e-02\n",
      "  5.58549225e-01  4.95683610e-01 -3.83938909e-01  7.81550646e-01\n",
      " -9.25915607e-04 -4.97176141e-01 -1.41722754e-01  1.33127898e-01\n",
      "  1.27418566e+00  3.35027844e-01 -2.45288044e-01 -6.77578598e-02\n",
      " -1.64562821e-01  2.97759742e-01 -3.34589660e-01 -1.31893063e+00\n",
      " -1.28372371e-01  2.13097513e-01 -2.64147192e-01 -6.12845719e-01\n",
      " -3.70203823e-01  6.65691614e-01 -2.98138261e-01  6.17276371e-01\n",
      " -3.26073676e-01  1.77319035e-01  6.95411742e-01 -8.38277936e-02\n",
      " -5.23017704e-01  1.50961295e-01 -3.68423574e-02  2.08965749e-01\n",
      " -9.43955127e-03 -1.04460463e-01  1.18922994e-01  4.55895960e-01\n",
      "  2.81339139e-01  1.78935215e-01  8.77791286e-01 -2.43886903e-01\n",
      " -4.74998742e-01  5.71605563e-01  6.72294915e-01 -2.39751384e-01\n",
      " -1.65879250e-01  3.04994613e-01 -3.41798723e-01 -3.30641776e-01]\n"
     ]
    }
   ],
   "source": [
    "search_term = \"Universit√§t\"\n",
    "print(model.wv[search_term]) #wv steht f√ºr word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798334db-07a4-4127-8d25-f8f6eceda05b",
   "metadata": {},
   "source": [
    "Spiel gerne mit anderen W√∂rter herum, indem Du sie bei `search_term` zwischen den Anf√ºhrungszeichen einsetzt. Bei W√∂rtern, die das Modell nicht kennt (weil sie nicht (gen√ºgend oft) in unseren Daten vorkamen) erh√§ltst Du einen `KeyError`.\n",
    "\n",
    "## 4. Das Modell\n",
    "\n",
    "Insgesamt sind diese Vektoren komplett nichtssagend f√ºr unser Sprachverst√§ndnis. Schauen wir aber, ob das Modell dennoch die Semantik von \"Universit√§t\" auf seine eigene, vektorielle Weise einfangen konnte.\n",
    "\n",
    "### 4.1. √Ñhnlichkeiten\n",
    "\n",
    "Dies k√∂nnen wir etwa tun, in dem wir uns das √§hnlichste Wort im Sprachmodell ausgeben lassen, also dasjenige Wort mit dem √§hnlichsten Vektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0e157a-fa2b-401e-a1ae-36c503aadff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hochschule</td>\n",
       "      <td>0.752539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Similarity\n",
       "1  Hochschule    0.752539"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = \"Universit√§t\"\n",
    "most_similar = pd.DataFrame(model.wv.most_similar(search_term, topn=10), columns=[\"Word\", \"Similarity\"], index=range(1,11))\n",
    "most_similar.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9c32f-bfd5-456b-a0bf-dbe986061a67",
   "metadata": {},
   "source": [
    "Vermutlich kommt \"Hochschule\" als √§hnlichstes Wort heraus. Faszinierend, oder? Setze gerne weitere W√∂rter bei `search_term` zwischen den Anf√ºhrungszeichen ein!\n",
    "\n",
    "‚ö†Ô∏è Achtung: Hier steht \"vermutlich\", da jedes Modell unterschiedlich ist, auch wenn die Trainingsdaten identisch waren. Dies liegt daran, dass unser Algorithmus nicht *deterministisch* ist. Ganz am Anfang des Trainings werden den Features der Vektoren n√§mlich zuf√§llige Werte zugewiesen. Das Training des Algorithmus besteht dann darin, die Features von Runde zu Runde anzupassen, um bessere Vorhersagen zu erzielen (also das verdeckte Wort besser zu erraten).\n",
    "\n",
    "Schauen wir uns die n√§chst√§hnlichen W√∂rter zu \"Universit√§t\" ebenfalls an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296daf15-84a0-4399-8c06-274d282892fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hochschule</td>\n",
       "      <td>0.752539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uni</td>\n",
       "      <td>0.679624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B√∂rse</td>\n",
       "      <td>0.673614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spitze</td>\n",
       "      <td>0.672711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TU</td>\n",
       "      <td>0.670073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AG</td>\n",
       "      <td>0.663688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kirchengemeinde</td>\n",
       "      <td>0.662947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tabellenspitze</td>\n",
       "      <td>0.656888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Raumstation</td>\n",
       "      <td>0.655456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Infektiologie</td>\n",
       "      <td>0.654944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Similarity\n",
       "1        Hochschule    0.752539\n",
       "2               Uni    0.679624\n",
       "3             B√∂rse    0.673614\n",
       "4            Spitze    0.672711\n",
       "5                TU    0.670073\n",
       "6                AG    0.663688\n",
       "7   Kirchengemeinde    0.662947\n",
       "8    Tabellenspitze    0.656888\n",
       "9       Raumstation    0.655456\n",
       "10    Infektiologie    0.654944"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9547232-ae1b-42b0-bf4c-27332069c857",
   "metadata": {},
   "source": [
    "Sp√§testens hier sollten sich Unterschiede zwischen verschiedenen Modellen zeigen. Vermutlich sind auch nicht mehr alle W√∂rter f√ºr unser menschliches Sprachgef√ºhl √§hnlich zu \"Universit√§t\". \n",
    "\n",
    "Wir k√∂nnten nun mit den Parametern oben experimentieren (z.B. mehr Epochen oder ein gr√∂√üeres/kleineres Kontextfenster), um bessere Resultate zu erzielen. Zielf√ºhrender ist es jedoch, das Modell mit einem gr√∂√üeren Datensatz zu f√ºttern. Die Quantit√§t an Trainingsdaten ist absolut entscheidend f√ºr ein gutes Sprachmodell, wobei die Qualit√§t der Trainingsdaten (d.h. z.B., ob sie ausgewogen und repr√§sentativ sind) auch nicht au√üer Acht gelassen werden sollte! \n",
    "\n",
    "Da wir an der Qualit√§t kurzfristig nichts √§ndern k√∂nnen, verzehnfachen wir einfach mal die Quantit√§t. Das Training mit 1 Million S√§tze aus dem Wortschatz Leipzig w√ºrde relativ lange dauern, weswegen wir ganz einfach das bereits trainierte Modell \"word2vec_1m.model\" aus dem Ordner \"model\" in den Arbeitsspeicher laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49d87d91-83d9-42c6-9127-26c1208fe5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 20:08:03,441 : INFO : loading Word2Vec object from model/word2vec_1M.model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWord2Vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel/word2vec_1M.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/word2vec.py:1930\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \n\u001b[1;32m   1913\u001b[0m \u001b[38;5;124;03mSee Also\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1927\u001b[0m \n\u001b[1;32m   1928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1929\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1930\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWord2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Word2Vec):\n\u001b[1;32m   1932\u001b[0m         rethrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/utils.py:485\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    481\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[1;32m    483\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[0;32m--> 485\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[1;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/utils.py:1460\u001b[0m, in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \n\u001b[1;32m   1458\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(\"model/word2vec_1M.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56bee6-1592-404f-a1e2-b1c8bfe6d6f6",
   "metadata": {},
   "source": [
    "Schauen wir uns nun die zehn √§hnlichsten W√∂rter zu \"Universit√§t\" an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd884364-dbe6-465f-b2e1-a9aa145a0677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hochschule</td>\n",
       "      <td>0.752539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uni</td>\n",
       "      <td>0.679624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B√∂rse</td>\n",
       "      <td>0.673614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spitze</td>\n",
       "      <td>0.672711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TU</td>\n",
       "      <td>0.670073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AG</td>\n",
       "      <td>0.663688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kirchengemeinde</td>\n",
       "      <td>0.662947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tabellenspitze</td>\n",
       "      <td>0.656888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Raumstation</td>\n",
       "      <td>0.655456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Infektiologie</td>\n",
       "      <td>0.654944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Similarity\n",
       "1        Hochschule    0.752539\n",
       "2               Uni    0.679624\n",
       "3             B√∂rse    0.673614\n",
       "4            Spitze    0.672711\n",
       "5                TU    0.670073\n",
       "6                AG    0.663688\n",
       "7   Kirchengemeinde    0.662947\n",
       "8    Tabellenspitze    0.656888\n",
       "9       Raumstation    0.655456\n",
       "10    Infektiologie    0.654944"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = \"Universit√§t\"\n",
    "most_similar = pd.DataFrame(model.wv.most_similar(search_term, topn=10), columns=[\"Word\", \"Similarity\"], index=range(1,11))\n",
    "most_similar.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a341e1f-fd38-41f6-9d3b-b0130b8c7107",
   "metadata": {},
   "source": [
    "Dies sieht schon viel besser aus! \n",
    "\n",
    "Wenn wir nicht blo√ü an den √§hnlichsten W√∂rtern zu einem bestimmten Wort interessiert sind, sondern daran, wie √§hnlich bestimmte Wortpaare zueinander sind, k√∂nnen wir folgenderma√üen vorgehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77835c17-9073-4cb0-8e19-6212a96778a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto und Fahrzeug     sind sich zu 86.07% √§hnlich.\n",
      "Auto und Fahrrad      sind sich zu 76.07% √§hnlich.\n",
      "Auto und Flugzeug     sind sich zu 61.23% √§hnlich.\n",
      "Auto und Haferflocken sind sich zu 28.95% √§hnlich.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Zahnb√ºrste' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m pairs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFahrzeug\u001b[39m\u001b[38;5;124m\"\u001b[39m),   \n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFahrrad\u001b[39m\u001b[38;5;124m\"\u001b[39m),   \n\u001b[1;32m      4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlugzeug\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m      5\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaferflocken\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZahnb√ºrste\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w1, w2 \u001b[38;5;129;01min\u001b[39;00m pairs:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m und \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m12\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sind sich zu \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39msimilarity(w1, w2)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% √§hnlich.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py:1154\u001b[0m, in \u001b[0;36mKeyedVectors.similarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, w1, w2):\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute cosine similarity between two keys.\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \n\u001b[1;32m   1141\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dot(matutils\u001b[38;5;241m.\u001b[39munitvec(\u001b[38;5;28mself\u001b[39m[w1]), matutils\u001b[38;5;241m.\u001b[39munitvec(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mw2\u001b[49m\u001b[43m]\u001b[49m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py:395\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py:438\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m \n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py:412\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Zahnb√ºrste' not present\""
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    (\"Auto\", \"Fahrzeug\"),   \n",
    "    (\"Auto\", \"Fahrrad\"),   \n",
    "    (\"Auto\", \"Flugzeug\"), \n",
    "    (\"Auto\", \"Haferflocken\"), \n",
    "    (\"Auto\", \"Zahnb√ºrste\")]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    print(f\"{w1} und {w2:12} sind sich zu {model.wv.similarity(w1, w2)*100:.2f}% √§hnlich.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807b1c4-abcf-4f5c-a11c-08188c1384a0",
   "metadata": {},
   "source": [
    "Auch interessant ist es, herauszufinden, welches Wort am wenigsten zu anderen gegebenen W√∂rtern passt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15da4eab-522d-494c-b07f-95c0e856b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dresden\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match([\"Berlin\", \"Hamburg\", \"Z√ºrich\", \"Dresden\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ecf5a8-f633-4a6b-bca1-95600049d21a",
   "metadata": {},
   "source": [
    "Experimentiere bei s√§mtlichen \"√Ñhnlichkeitsmethoden\" mit eigenen Begriffen herum, um herauszufinden, wie gut sich der Algorithmus Deinem Sprachverst√§ndnis nach Wortbedeutungen aneignen konnte.\n",
    "\n",
    "Wir k√∂nnen uns den Vektorraum auch visualisieren lassen, zwar nicht in all seinen 200 Dimensionen (jedes Feature entspricht einer Koordinate in einer Dimension), da das f√ºr Menschen schlicht nicht vorstellbar ist, aber reduziert auf drei Dimensionen. Wir benutzen dazu den Embedding Projector von TensorFlow. [Hier]() findest Du eine Visualisierung des mit 1 Million S√§tze trainierten Modells. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
